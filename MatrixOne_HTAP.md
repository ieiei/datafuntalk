我叫张潇，我们公司叫做矩阵起源。今天我讲的主题是MatrixOne，我们公司目前开发的一个开源的分布式数据库，他从NewSQL到现在新的HTAP架构的整个架构的引进，可能相对来说更稍微抽象一点。
接下来我先介绍一下我自己，我在2011年到2021年这十年的时间里一直是做全职的DBA。在金融在商业地产还有教育行业都有过相关的从业经历，2021年我入职到矩阵起源做产品架构师，至今已经快两年了。

我之前做的数据库可能更多的像Oracle、MySQL、SQLServer，还有像PG卡三折这种关系型数据库更多一些
跟现在的工作既有一些交叉，同时又有很多之前没有涉及到的东西。
今天我的整个目录是：

一、我们来测算早期的这个架构和遇到了什么样的难题
二、我们架构的升级之路。可能会给大家讲一讲，架构升级遇到了哪些东西
三、架构升级过程当中我们有什么样的困难还有收获
四、我还对整我们现在的架构以及升级做一个最后的总结



第一部分 MatrixOne早期的架构和难题

我们早期的架构，如果大家在2022年的上半年之前有了解过我们产品的话，会发现我们当时的架构和我们现在
大家拿到宣传页的架构有很大的区别，其实这就是当时我们做架构升级。
早期的架构如果让我去总结的话，我的总结是两个词：一个是NewSQL，一个是MPP。
NewSQL就是当年谷歌那几篇论文衍生出来的很多的这个分布式数据库的一套理论体系。
它其中第一个非常重要的就是分布式架构，
其实解决的是传统像不管Alcohol Server，还是单机版本SQL，它的高可用以及水平扩展的这种难题。

另外一个就是多引擎。其实在当时谷歌发布的很多论文当中提到过了。可能用不同的引擎来做不同的事情，这是当时的一个NewSQL的一个大家认为的比较典型的特点。
第二个就是MPP，或者叫大规模并行计算。
这个主要的用途就是通过分布式的方式将一些规模比较大的计算任务分布到不同的节点，并且在计算完成之后汇总。其实充分的利用了分布式架构的这个算力资源。
而我们早年的早期的架构确实也是这个样子，比如大家能看到我们上面可能有一个，负责分发负载均衡的proxy
下面就是我们MatrixOne Server，每一个Server下面有自己的存储。实际上是一个存算不分离的这么一个架构，
每个节点看似是对等。当然这里面还有自己的问题。我们再看一下整个这个组件的详解。最上面这一层我们叫SQL前端，SQL前端它干嘛呢，它是为了兼容MySQL的语法协议，
这个协议还有它的语法，就是我们用不管是用MySQL client，还是JDBC都可以直接连。
而计算层是在一个是传统的这个SQL Parser，不管我们是做语法树的这个解析
还是用来支持多方言的这种SQL。都有它自己的用途，而下面就是我们自己写的这个MPP SQL执行器。
我们当时整个研发的同事们当时对他的一个设想是第一针对这个基础引擎做一些向量化的加速；
第二是我们部分操作甚至是用汇编语言做了改写；第三个就是我们当时使用了一个比较独到的因子化加速的能力。
所以最早如果大家看我们2021年的性能测试报告的话能看到还是有一个相当不错的性能表现。
但是后面为什么会舍弃掉这个东西呢？待会我给大家去讲。

而大家看到的是一个分布式框架，这一层是我们叫做Matrix Cube，是我们当时分布式组的同事做的一个开源的项目。其实当时也做了好多年，他提供的是一个整个对于多台机器的分布式存储框架。像高可用多副本，这个负载均衡强力制这种基础能力都有。而且当时设想是要用它来为对这个Matrix计算，提供分布式事务的支持能力。





大家如果说做过这个分布式系统的开发的话，也会发现这个分布式事务的实现是一个非常让人头疼的事情。直到现在我们公司现在做记忆板的时候，在分布事务上，我们还在投入很多的研发人力和时间，争取想要把这个东西攻克。再一点就是在里面是有Raft协议的，而且中间会有一个调度器叫做profit Raft，本身大家也都知道它是一个有leader节点的一个分布式的协议。
而最下面呢，就是我们的计算层和存储层。存储层除了引擎接口之外，我们有三个引擎。
最中间的这个叫AOE、AOE是一个不支持事务，你可以往里写数据，但是对于这个这个事务、去重等等，它基本上是不支持的。最左边的我们叫TPE，TPE是用来保存原数据catalog，像大家能看到这个catalog，只是其实它是贯穿于从前端，到底下存储层，它是一个非常繁忙的引擎。
而最右边叫做TAE，TAE是一个典型的列式存储。能够提供完整的SID
同时我们希望它能够去支持比较大规模的OLAP能力。
所以早年我们看到的一个问题是三个存储运行
这也是后来我们为什么从架构层面整个进行重构的一个原因
这整个引擎详解的早起架构早期架构呢我给大家解释完了。

那么接下来我来讲一讲这个引擎他的问题在哪里，以及我们为什么要对他做整个升级重构。
首先就是扩展性，这是我们刚刚才我给大家看的是一个存算不分离的架构。
这个架构它在扩展性上有一个什么问题，每扩展一个单位的节点，就要去扩展相应的存和算两种资源，存算不分家。
而且因为我们数据是以3副本的形式保存，意味着我们只要一个节点加进去，他想要真正接管计算任务的时候要先把整个存储先完成同步。
那其实节点扩展它需要的时间就预热的时间非常长。
比如说我们大概有1TB的数据
那可能我们要先等着1TB数据的3副本完成在这个节点上同步，才能去开始提供所有的计算负载。
还有一个就是性能，因为Raft协议它一定是有一个leader就像刚才我说的，那么这里面就容易出现这个leader节点容易成为热点。很多的调度任务都从他这里走，在性能比较差的存储下，整体性能下降其实会超过预期。
这句话的意思是，比如我们用SSD做的这个benchmark测试的时候，我们预计它的性能比如说是10
那么我们最早预想HDD我们能跑到5或者6，但是实际情况是我们在HDD上可能跑出来的结果只有3或者4，这成为了一个性能上的瓶颈。再一个就是刚才给给大家看的三个存储引擎，TPE、 AOE、TE三个引擎，用途不同而且性能还不一样，有的时候经常会出现某一个业务场景三个引擎当中的一个成为了整个性能的桎梏。我没有办法继续往前推，最后这就是最短的这个板，成为了我们性能的瓶颈。

还有就是成本，我们当时最早是用3副本存储，那性能实际上这个节点规模越大，我们是线性提升的，这个成本负担就越重，到了公有云上，比如说有的公有云还提供了一个高可用的方案，那就成了壳子套壳子，就不是线型而是指数级增长了。可能我要用3个节点的时候，数据库里存了9副本。然后在公有云又可能做了一个3层冗余那就是27层。这就到了后面客户所承担的成本负担，实在是太重了。还有就是只有高配存储才能发挥出预期，这个就是我们大家所说的性能较差的存储架构发挥不出来我们想要给到的性能特性的时候，那只能不断的通过增加存储成本的方式来实现一个比较好的最佳实践。
面对这些问题，从去年的3月开始，整个就由我们CTO田丰博士作为牵头人，对整个架构做了一个重新的升级。
其实我们是从0.5开始做这个事情，0.1到0.4 大家更多的时候也是在一个探索的过程，我们到底该怎么怎么做，可能这是一个不断思考不断试错的一个过程。所以到了0.5的时候，我们终于意识到这个架构走不下去了。
所以怎么办呢，首先先分析一下为什么原有的架构走不下去了，实际上我们当时总结三座大山。
第一座大山就是分布式框架，这个分布式框架，多副本存储带来的是成本存储的飙升
第二座是leader选举，人为制造了热点
第三座大山，就是我们的引擎实在是太多了，三个存储引擎彼此之间的代码复用率非常低。比如说我在上面写一个新的功能，甚至代码的维护量可能差不多约等于3倍。还有就是我们所说的因子化算法有点过于激进，意味着除了我们那位主开就他能够对这个算法，包括这个计算引擎做出一些驾驭的话，其他的同事能参与度非常低，只能做一些辅助，加一个功能都会非常的困难。所以这个引擎当时也成为了我们不得不去剃掉的一个原因。

再一个就是资源分配，一开始我们说存算不分，那实际上我们再去做各种资源配比的时候，可能会出现的是不同业务场景，它的隔离性就非常差。
再一个就是这种share-nothing的架构，意味着扩展性非常差。必须同时扩展存和算两种资源
所以在我们总结了这3个根本的原因之后，开始做这个架构升级。
最左边这个大家可以看一下，一层套一层、一层叠一层，像个千层高一样。各层和各层之间又是强依赖的关系，所以第一步就要先把整个各层彻底的打散，做一个更加灵活解耦的这个整体的架构啊
最后我们从整理出来是一个是存储层单独做了一层，可以使用的是各种存储，不论对象存储FS、 HDFS。然后把所有的cache服务放在存储层来做，最上面就是计算层。所有的跟MPP以上的这些
，不管是前端也好Parser也好，还是说执行器也好，最后全都放到计算层。而中间我们又开发了一个新的层，叫做事务层。事务层可以看做为log service，其实大家熟悉的传统关系数据库中的Redo或者PG里的WAL日志，差不多跟我们log service有的功能上是非常类似的。
而我们还有一个事务节点DN，它是专门用来做事务服务，因为我们是一个分布式数据库。
就涉及到分布式的这个事务裁决，包括去重，还有这个落盘这些这些任务。
所以最终选择开发了一个单独的事务层来专门处理这些东西。
那么之前我们说的三个存储引擎肯定不可能全留，最后留哪一个呢？我们想了半天最后觉得还是TPE最合适，就是TPE它是一个行存，AOE它又是一个不能够去重，不能够支持事务的层级。
所以TPE它的好处是，第一它能够提供基于列存的TP引擎，而且它能够做到完整的这个事务的
原子性，一致性，隔离性和数据一致性等等，4就是以及这个OLTP能力。
它成为了我们认为最适合用于新架构的这样一个引擎，剩下两个引擎把一些可能有些功能想办法融到TE里，剩下两个就全都去掉。所以最终我们看到的是TPE，它是用列式的编码来存用column family，可以在行和列之间做一个切换，这个在column family那我会有一张图给大家看到。
这样做的好处是一个可以同时运行TP和AP的负载，因为这两个其实对于有的时候对于TP和AP我们经常说行存更适合TP，列存更适合AP，现在做完这样一个转换之后，行和列就都能够兼顾到了。
再一个就是所有的表都能够实现表级别的快照事务隔离，这是我们现在已经实现的快照隔离。
当然现在还在做的叫做2c，就是独立提交，大家平时在数据库当中打交到最多的那个会在我们下一个迭代的时候发布，即主键唯一键排序。外键索引也会在这个引擎里面一起实现。
就是之前UE和这个TP做不了的事情，我们在TAE上都会让它用一个引擎去完成。还有就是多副本和分片，因为之前我们说过，数据要保存三副本，而且每个副本它在数据库里面是以分片的形式保存，用的是数据库自带的操作系统自带来完成冷热数据的管理。
但是我们现在这个新架构就有一个新的要求，就是冷热数据尽量分离，读写请求分离，实现一个对存储的精细化管理。所以最后我们选择了亚马逊的S3对象存储，还有甚至私有化的话，跟S3的协议兼容的对象存储。热数据就保存在计算节点的存缓存cache上，所有的节点也都实现了无状态。并发能力可以通过比如说我们现在对于AP业务现在可能要求更多的会话的时候，那我们多启动几个计算节点。如果说AP我们需要更多的计算的话，那么我们可以尝试着将计算节点进行扩容。彼此之间三个层级不需要再过度的依赖，然后最后我们就是说从分布式存储完成之后，就是计算层。计算层的话之前是因子化算法构建执行计划，做复杂的加速查询，主要是针对对AP性能的提高。但是它的问题也是表达式和节点的抽象仪表述啊太复杂了，不管是增加功能还是说去修改什么东西难度非常的高，而且就像我所说的，多个引擎之间的代码复用率太低，就导致了不管我去做什么事情，我的工作量都是两三倍的增长。所以最后弄了一个新的我们叫MPP执行计划，基于DAG来构建执行计划。能够实现节点内和节点之间的调度，待会会介绍我们节点之间的这个调度是怎么做的。
然后同时能够满足并行和并发两种请求，因为我们都知道并行是AP，并发是TP。通常大家会这么去处理，而且SQL能力得到了完善，像之前可能只支持的非常不太好的像子查询、窗口函数、CTE，还有像内存溢出等等现在都具备了这个能力。而且现在除了spill内存溢出可能还在开发中，剩下的几个子查询、窗口函数、CTE现在都已经实现了。这样的好处是在未来的优化空间更大，就是不管是我们的主开还是说其他的计算组的成员都可以对他做优化加新功能。现在我们从0.5之后都有一个比较深刻的体会，我们不管是做什么其实都在这个代码优化上，比以前节省了很多的人力。

最后给大家来就是做完这么多的设计之后，我们来看一下现在总体的架构是什么样子。

首先最下面我们叫做这个叫做file service，
这是我们一个分布式的同事写的一个统一的文件读写的一个服务接口，它能够从S3对象存储去读数据，并且把这些数据推给像不管是日志，我们的计算节点或者是事务节点等等。而且事务节点和计算节点的日志又可以通过它去写S3。就是所有的节点只需要跟这个file service打交道，就能够完成对存储的读写。然后全量的存储都可以保存在S3上，尤其是公共云版的S3大家应该多少有点体会，就是它的成本非常的低。再一个公共云版的S3，我们可以认为它是无限伸缩。容量是只要你肯交费，要多少有多少。然后在上面的事务层，我们有两个DN节点这专门去负责管理日志服务和元数据，平时的他会在里面混存元数据。做一些事务裁决，并且会指挥log service的日志服务来落盘写数据。他自己就是通过三副本的方式来保证从日志级别上的高可用，而且这里面还有一个叫local的东西
（待会我给大家详细解释）。local和现在的落盘的数据之间是如何共同完成我们数据写的过程，
最上面就是我们所有severless的，我们叫CN节点计算节点。计算节点它是完完全无状态的，每个计算节点有自己的开始，好处是比如说我现在cn是最近的这个计算，计算的负载比较高那我就多起几个。如果说可能现在业务比较低的话，到了一些像大家过年了没有什么业务的时候，那干脆把节点全都宕机，节省一些成本。
而存储的话，存储层现在TAE我们是完全实现了列存级别的。大家可看到从数据库到表，再到segment，再往下是一列一列，而且我们列的单位是列级别的block，或者说块。比如一次读的时候，他会从一个列里面去读大概多少行作为一个block，推给上面的计算节点或者说日志节点。可能大家比较关心的就是我们如何在AP和TP之间找到一个平衡点，现在的如果说大家默认的去建一张表，那都是列，就大家能看到的是右边的这个。但是如果想某一些某一些表强化一下它的行存的性能怎么办，我建一个叫做column family，就对某一些行做一些特殊的优化，那么在对这一些可能需要频繁在上面做。



所以说是更新这些列，通过column family列的方式能够大幅的提升他的TP性能，最终就是只需要存一个副本。
但是我在有些表上做好一些优化之后，就可以实现行存和列存可以在各自性能上的这种优势。
当然这个康乐分类现在还正在开发当中，可能会在未来的一两个版本迭代之后，会有一个最初的版本给大家来用。
而计算的话，就像我说的，我们现在实现了节点之间的调度。
大家能看到，所有的计算节点之间都有个双箭头，这是什么意思呢，就是我们每个计算节点比如说我现在是从
最左边的计算节点进来，我需要去做一个数据的查询，但是我发现他的开始里面没有我想要的数据。怎么办？他会遍历所有的其他计算节点去找哪一个节点有自己想要的数据。有的话他直接在那个节点里面把计算任务完成，再把这个结果返回到最初的接受请求的节点。这样的好处是最大限度的利用了不同节点之间缓存，不同的热数据。对于一些常用的查询，可能是一个非常大的性能的提升。而且现在我们计算节点，除了缓存以外上面还有一个自己写的pipeline，将很多的sick请求拆解成为物理执行计划来进行执行。上面是目前正在开发的一个优化器，而最上面就是我们一直在上一个迭代的代码复用最多的MySQL级别的Parser。
能够对语法做一些解析，同时还能够去做一些方言上的支持。比如说我们现在也在做一些对PG的语法和方言的支持其实都是用MySQL Parser来做的。
那其实做了这么多之后，现在大家可能比较好奇的是整个过程当中我们遇到了哪些难题，又是怎么解决的。
前面的架构和后面的架构差异，不论是整个架构的差异，还是每个组件的功能都有非常大的变化。
首先面对的几个难题：第一个难题是如何寻找一个能够对高性能计算引擎匹配的存储，存储的两个核心的需求：一个是更少的冗余，一个是更低的使用成本。
当然这个更少的最后经过我们很多的论证之后发现，亚马逊的S3对象存储其实能够完美的匹配我们这两个核心的需求。
比如说我们现在整个单副本，在亚马逊基本上是一点几个副本。差不多就多了多了20%的冗余，其
实这个成本比起我们之前的三副本，他的整个冗余度是大幅下降的，成本上可能从之前的两三倍变成了一点几倍。
还能保证一定程度上的数据冗余.，而且就使用上来说，现在匹配S3的各种接口，各种的方式开发都已经慢慢的成熟起来，还有S3自带的这个冷热分离，导致我们在使用上，一方面将冷数据放在S3里面降低的成本，热数据放到这个计算节点上，就基本上完成了用更低的成本来实现冷热数据分离的事情。现在大家如果说去试用一下我们的东西，在有些冷数据作为S3放对象，存储热数据用块式的话，这个性能的差异其实是非常明显的。
第二点就是我们在事务层是如何做分工，因为分布式数据库的分布式事务始终是一个非常大的难点，一开始我们希望我们的CN，即计算节点只负责计算，所有的事务ID的生成事务裁决，还有一致性隔离性，包括数据的读写全都由DN来完成，就是我们的事务节点来完成。然后所有的这个冲突检测，还有约束完整性也都由DN来完成。但是我们做到后面就发现一个问题，就是DN会成为瓶颈。因为我们平时启动的事务节点的数量其实是远远少于计算节点的数量，如果说事务节点起的多了，在事务裁决上多个事物节点之间同步又会出现问题。所以当时DN成为了一个瓶颈。
于是我们做的第一件事情，引入一个概念叫做Locktale。因为大家如果平时对数据库的写流程有一定了解的朋友会知道我们怎么写，首先把数据这个操作写到日志里，然后再落盘去写。这样的好处是如果我们这时候在写的过程当中发生宕机了，我只需要回放日志，就可以保证数据最终还是可以落盘的。所以我们选择了引入一个叫做logotel的东西，我们先把数据写到日志里叫做logotel，然后logtel它会定期的把这部分数据写入到S3对象存储。就不需要频繁的去写，相当于攒一波往里推，是这么一个机制。这样的好处就是我们在写的时候，不再局限于整个DN的这个，这写的性能DN只需要一次，就是攒够一大批一起往里写一次。当DN不怎么忙的时候，他可以选择在自己不怎么忙的时候把他写进去。忙的话那当然可能全都混存在Locktale里，这样的话cn只需要负责所有的事物和事物逻辑，还有计算。DN既保留最近一段的数据，同时又负责日志服务，这样的话其实把DN一定程度上解放出来，使得写入动作的上限基本上是被打破了。

但是还面临一个问题，就是事务量非常大的时候怎么样保证写的性能。当时我们选择一个新的策略，如果说我们是批量写入一大批数据，是大概比如说一下子写入几百兆的数据的时候，我们不再通过日志，而是直接往对象S3里写。只是告诉日志服务我要通过什么样的操作，在哪个文件里写什么东西。而那些比较小的事务，可能比如只是更新一两行数据，或者插入一个新数据，仍然还是走原来的从计算节点到事务节点再到对象存储这么一个过程。
并且现在我们将约束完整性和冲突检测，都放在了cn来做。就一定程度上让我们的事务DN节点更加的灵活，就整个的负载更轻。这样的好处是写入性能比之前明显的提升了，比如说我们现在如果没有开批量写入直接插入S3的话，可能几百兆数据我们要写几十秒。但是现在的话可能大概一两秒就能写完整个的。这个提升大概提升了一个数量级，然后现在还面临一个问题，就是我们如何实现不同业务类型的负载工作。工作负载的隔离，大家可以看一下，我们按照现在的架构，如果说OLTP级别它是怎么做，首先计算节点先放先把数据推给事务节点，事务节点再通过日志写到S3里。如果是OLAP负载。那可能就是直接从S3里面去读数据来进行计算就可以了。
这样我们现在所选择的方式就是用不同的cn节点来跑不同的东西。比如说我们现在成立第一个CN组，他只跑TP业务，第二个CN组他只跑AP业务，实现计算节点之间的隔离。
当然如果你觉得我的系统比较重要，我的预算比较充裕，那可选择用机器做服务器级别的隔离。我用物理机来部署不同的计算节点，如果机器你想用低成本的方式跑，我们也提供了容器级别的隔离，容器级别可以实现数据和负载的完全隔离。所以最终我们通过这个问题，最后把它做成了一个什么样子呢。大家可以看这张图，我们现在给先做了一个新的概念叫先label，就是标签化。比如说我们现在看到这里面有三个是打了AP的标签，一个打了TP的标签。那我一个会话进来的时候，我先去看一个优化器，会先去判断他是一个AP请求还是TP请求。TP请求那就进TP的计算节点，AP的话那就进AP的计算节点。这样的好处就是不会出现两种业务上的资源公用。比如哪边现在业务更高的时候，我就选择对哪一边分配更多的资源。当然未来我们还会有个设想，就是希望能够实现自动的负载均衡。比如通过优化器，通过某一段时间的统计信息，我们来判断最近可能TP业务更多一些，那就自动扩容一些TP的计算节点。AP的更多的话就自动扩容一些AP的节点。
这就是我们目前公测的0.8版，主要提供给用户的是手动的通过配置标签的方式，让用户把自己的不同类型的负载打到不同的计算节点上来实现。就是这么的一个实现方式。
但是整个升级过程其实也是个很痛苦的阶段，所以那段时间我也跟我们很多的研发同事做了一些语言上的交流，还有像技术上的一些复盘。所以后来我就问他们，在整个升级过程当中，你们都有什么样的收获你们对现在的产品又有什么样的新的了解。我们很多同事，对整个执行计划做的重构，包括像这个语法的解析执行计划，还有现在甚至连SQL语言标准，大家都有了新的认知。从头到尾我们相当于将整个的一条SQL从客户端进入服务器再到完成执行
整个过程我们做了重构。很多同事之前对这些东西了解其实是还浮于表面，但是经过这次整个系统升级之后，他们对这种这些东西的理解加深了很多。现在我们大家再去讨论各种执行计划，讨论开销包括语法如意的事情的时候
大家可以聊的很深入。甚至SQL标准，当时我们公司还出钱，大概几千美元买了SQL标准的字典。我建议大家平时有空的时候一定要去翻看标准是怎么定义的，这样有助于你们更好的去理解每一条语句它背后所支撑的逻辑。还有就是这个事务和acid，因为之前我们是多引擎，有的引擎的同事开发的时候他就不需要考虑事物的acid。然后现在不一样了，现在每一条你都要考虑事务的四个特性。
那这个时候对于整个事务的理解，大家可能更多的是对不同的隔离级别怎么做，那锁服务要怎么搞，为什么会有今天这样的一个情况。然后在开发事务层的时候，就是cn和DN的适配，我们当时是从去年9月一直到11月，两个月的时间里面在解决这个东西。作为分布式事务到底该怎么分工，既能够保证完成事务的acid，同时又能够保证让系统的架构和系统的负载，不会出现明显的短板和弱点。所以在当时大家反复验证了将近两个月，最终的结果就是引入log tale。并且CN和DN一个只负责元数据，另外一个负责计算和逻辑以及驱虫。而log tale，最后我们还发现log tale还有个什么好处，因为它是放在DN里，它可以实现不同的计算节点对这一部分数据的共享，不需要再从对象存储里直接load。存储层大家积累了两样东西，一个是S3对象存储，积累了对S3对象存储的开发经验。
因为之前我们很多同事可能没有做过共有云的开发，但是在开发这段时间里面，对对象存储的使用有了相当大的进步，还有就是我们现在自己的file service文件服务基本上开发完以后，大家很多的时候在使用不同类型的存储的时候，不再需要考虑很多接口要怎么写。我的兼容性怎么样我的性能怎么样，统一交给file service去实现。

最后我来给大家做一个总结，就是我们整个系统架构升从2022年的4月一直到11月，差不多半年左右的时间大概
做了哪些东西是值得去给可能给大家去借鉴或者也许会给大家带来一些不一样的思考。
第一点：存算一体的分布式架构它有它自己的好处，但是它也有它自己的问题，容易制造热点。可能再加上成本等等这些问题。而我们完成了三层的解耦之后，每一个层级可以自行的进行扩缩容，不再依赖于其他层面。这种灵活解耦的架构，其实在不同的业务需求上，确实可以得到不同的最佳实践。
比如说有些业务，可能需要更多的计算资源，可以直接加计算资源。而像以前的不管是集中数据库还是存算不分的数据库，都面临只要扩容，存和算都要区分。

第二点：多引擎到单引擎，因为多引擎之前给大家说过，多引擎一个是要维护的代码量以及可能你要考虑每个引擎的特性之间怎么样去搭配。但是单引擎现在只需要考虑的是一个单引擎是怎么做这个事情。对于大家的工作量啊
包括整个HTAP的设计，其实都觉得比多引擎可能具备更节省人力的这么的一个用处。
因子化算法到DHG ，就是有些东西确确实实是很好。但是我们不仅仅要仰望天空更要脚踏实地，在当下的话在没有那么多人才储备的情况下或者说对于有些还不够成熟的东西，不足以支撑起我们未来的产品发展的情况之下，那选择一些更实际，更有效，让更多人能够参与的方式去构建执行计划，可能才是我们最终的归宿。

还有就是从多副本存储到对象存储vlog tale的引入，之前的多副本存储带来的成本问题可能在对于一些对成本比较敏感的客户那里是一个天坑。现在我们对象存储和logo tale引入之后，实际上存储的成本降到了原来的1/3左右，对于以后可能数据量越来越大的用户来说，他未来对于成本的焦虑会大幅的下滑。
再就是灵活调整节点带来的资源隔离，一方面就是我们所说的存算分离，并且就像水多加面，面多加水更加灵活的资源配比。再一个通过用标签的方式，将一些请求强制隔离到不同的节点上，避免出现不同业务类型对资源征用。

这是我们的一个企业服务号，平时我们有很多的文章。可能是我们内部同事写的一些干货，还有就是可能我们对外部的一些新的进展，还有某些客户给我们的反馈，都会在里面有。右面是我们的企业微信群，可以扫码进群。
最后我再介绍一下我们公司现在有一个叫做Beta program的用户体验计划。这是我们目前为一些即将有合作意向的客户提供的一个专属的计划。您参与的好处有，一个是新功能我们可以第一时间交给您去用；并且可能有些比较匹配您业务的场景我们会做一些定制；还有您甚至可以直接参与到我们整个产品的设计当中。

比如说您们当前有什么痛点，可以直接找到我们，我们会告诉你，这个东西可能怎么解决。如果当前我们没有这个功能，可能会优先以您的需求来作为蓝本去设计。然后再比如说您在使用我们的这个产品的时候遇到了各种问题。可能您可以直接找到我们研发团队，很资深的工程师。那他们会告诉你可能这些东西大概怎么用，可能会有一些指导。
现在我们是属于0.8版本叫做Beta program阶段，我们会在第三季度发布正式版。我们现在也提供了公有云版，公有云版现在也是在公开招募阶段。
大家可以看到，在我们展台有小册子。如果对我们的这公有云感兴趣，也可以申请使用。目前我们使用的是service计划，可以去在在上面跑一些像TBC是或者说TBCC这种比较常见的benchmark来看一看
如果你有一些基于像MySQL开发的一些应用程序，你可以在我们上面做一些测试。就只要扫一下大家右下角的这个二维码就会有用户体验计划参与。
当然如果你觉得还想再了解更多也可以到我们展台，跟我还有我们同事约，我们两个人都可以给大家做一些解答。
今天有关我们整个产品架构的升级分享，我就讲到这里了。



Q & A

Q.：后期有没有计划接入更多的存储引擎，我看现在目前咱们不是接了S3吗？是有比如说像minio，或者是HDFS之类的引擎。
A： 确实是有，我们现在私有化的场景就是以minio作为私有化部署的方案，当然整个对象存储也会越来越多。
如果说在minio比较成熟以后，我们也会选择更多的存储对象来支持。
现阶段的私有化，我们标准版本就是私有化是minio。公有云版本就是S3或者是阿里云的OSS。

Q：您这边就是如果说对企业级的用户会不会有那种定制化的支持,
比如从对于解决方案的设计，对于企业级的应用方案的设计，以及运用a的一些个设计等等。
A：会有，首先我们现在企业客户会有一个付费用户，会有一个单独预约工具。
这个预约工具对于我们整个就是不管是集群也好，还是私有号管控也好都会有更好的使用。
然后如果您对于，比如说自己行业内的应用程序，大概可能怎么在我们这做落地最佳实践。我们确实现在也有这方面的同事一直在做这个事情，比如说你想做一些定制化的开发，或者是一些应用程序的优化。我们也会有技术的同事去帮您去做这个事情。

Q：就是目前我们的这个合作的企业中，大概都是哪些行业的。
A：现在可能一个是以制造业为主的一些企业，比如像麦斯，麦斯客户他们会把我们的数据库，嵌入到他们的这个产品里面去做一些应用。还有一些就是BI类的比如说像翻软、红友，他们这些BI客户也在尝试着把我们的数据库作为MPP层去推给他们的用户来使用。再还有一些像公有云有一些中小企业客户，会直接把他们的应用程序布在公有云上，使用我们公有云版的数据库。
现在是这样一个情况，也还在不断的拓展。我估计等到1.0的时候，我们的客户的应该不论是行业还是说场景都会更多。

老师你好哎你好
就是问请教几个问题吧就是
第一个问题就是怎么跟他说那个
就是CN计算的时候
就是事故啊
DN那个CN这个事故的时候就是DN成为瓶颈
引入这个lock tire写着无上限，这个怎么理解。因为我看了下这个lock tire的话，好像是存储在DN本地的吧。
这怎么跟那个那个存储层的话都不不在一个

A：平时是这样，我们所有的DN，可能它平时是和写数据打交道是最多的。如果我们就是每条数据，比如说只要写一条就要直接落盘的话，其实这个落盘的频率非常高。DN它作为一个节点，可能这时候就会成一个高负载的节点。他要发起很多写入动作。引入Locktale之后，我们很多的数据，先由DN缓存到自己的Locktale上，等到他攒够了一批之后，他再统一写一次，其实对他整个的负载程度就没有那么高了。这是其中之一。
第二就是我们现在的设计上，可能是cn的数量是远远多于DN的数量。所以经常会出现高并发的情况之下，可能我会有好几个cn同时往一个DN里写。所以这个时候DN的带宽和处理能力有可能就会成为一个瓶颈。所以我们尽量想的是把DN的工作负载降到最低，来实现写入性能的不断提升。
比如说大量的CN去写同时写一个DN是吧，
然后就是说我为了
因为这个DN的话就是可能如果说你
没有个logo tire的话
就是我们一般的话都是先写日志是吧
对对对如果先写日志的话
传统的话就是说
先写日志的话没有本地
你不走本地盘直接落S3的话是不是
哦就是你直接写3这样的就是呃
如果说您的数事故量特别大的话
你可以把那个那个那个参数开关打开
打开的话比如超过一个预值
比如说10兆
那么他会日志里只会记录我在哪个
我在哪个数据文件里写了什么
写了哪些内容
然后呢呃
直接把直直接跳过了，先写日志再再落盘的过程
就是这也是给用户一个选择
就是您是想
您是想就是说提高性能
当然
有可能出现一定一个一定程度上
就是这个时候我掐写了一半了
然后档机可能会会
日志没法回放的情况
这也是给把选择权交给用户
而且觉得这个这个写的过程呢比较快
就比如说100兆的数据
我们可能一两秒就写完了
就是像这种顺序写吧是吧
嗯他们又更新是吧
日志对这个
嗯嗯其实我想问的就是这个
这里面的
问题就是为为什么我们设计成
设计成就是单独的存储
就是在直接在DN上面的话挂存储
就是locktail
而不放在那个
就是统一的那个存储材
哦local首先就是
呃local我们给它配的是比较好的存储
就它就直接在里面
缓存的时候
它往里写的性能其实是比S3要更好的
它相当于一个中转站
呃这这第一个好处
第二个就是Locktale它存在DN里面的话
我不同的cn只要这个数据它没有落盘
没有被创k到之前
所有的cn都可以共享
比如说我恰好就需要Locktale
数据的时候
我直接从
Locktale里读不需要再走入33
其实一定程度上
也就是对CN也做了一个加速
这个也是一个共享的是吧
对对对它是一个共享
DN节点之间共享的是吧
对对这个logo啊
是啊它是一个共享的存储
这个可能就是走走的是快存储
而不是对线存储是吧
是是它的性能更好一些啊
这是一个第二个问题是
第二个问题就是冷热与读写分离，咱们现在怎么体现冷热跟读写分离呢，冷热首先冷热数据一个是S3他自己提供了一个冷热数据分离的机制。他可能读取速度会比冷数据稍微快一些。然后其次我们在每一个节点上，不管是DN还是cn，也放了自己的缓冲区。就允许用户把一些就常用的数据以内存的方式放在自己内存里，如果自己还有一些除了内存以外，在每个节点上再配一块高性能盘，再缓存一些数据，其实也可以做。实际上冷热数据他实现了多机分离，比如说本身存储层他有这个机制是吧
其实就是开始是吧你说
对对就是开始
对对可以这么理解
但像这个的话
比如说我我们
用户你是控制不了这个啊
这个是个确就实际上是一个
就是用那个
那个由交给S3对象存储
我开始自己来做
可能用户没有办法
就目前为止吧
我觉得可能没有哪个数据库
能够精准的告诉大家
我开始让存某个精准的
精准的就实存某个数据
就是可能最近用的多的我放到内存里
稍微少一点的我放到本地盘
然后
再冷一些的那我就直接只从S3里读取
就相当于三个级别嗯
读写这个是冷热读写分离怎么实现读写分离的话就是就是现在是
比如说我在我在去写的时候
读的时候
可能只需要去从内存开始里面去读
我不需要涉及到去写
然后不同的计算节点
可能就是就刚才有一张图
我给你找一下
就是比如说现在有有一个请求
协读请求进来了
那么我可能直接
直接从这他的开始里面去找
找到对应的开始直接直接就可以读
这时候其实这个这一定程度上
呃然后我这个cn
继续可以负责只负载去写
这节点之间可以实现这个这个
自自适应调度
然后再就是这种这种标签
不同的标签我可以实现不同的打
基于不同的标签的情况之下
有的可能TP只负责写AP只负责读
就是相当于在
这部分资源是这部分资源是完成
隔离了
专款专用的这种这种这种使用方式
当然以后
等到我们优化器逐渐的强大之后
可能还会有在一个节点内部
我们画出某一部分区域只负责给读
一部分只给写
这个可能还需要一段时间
才能开发出来
这是读
写没问题啊
但是你读的话一致性怎么保证
读一啊您说读哪方面的一致性
比如说这咱们不是说吗
比如在cn一个一个cn上面CN1上面是
写了然后我读的时候我是先2上面读
哦就是我们所有写的数据
都会先推到Logto里
保证它是全局可以读到的
就cn只负责计算
只要cn计算完了就立刻把数据推给DN
是这样他查的时候还是要经过DN
对对是这样的就是呃
开始里有的话
如果必须这样我们现在叫一个Locktale
我们叫铺式模式
就只要Locktale里的任何数据
发生更改了
他立刻会把更改的数据
推到每推到对应cn的这个开始里
之前我们叫破模式他是要自己主动拉
现在是DN主动的去推
所以就保证了
DN里的开始的这个数据和
cn的开始数据是它是同步的
就是这个DN嗯应该是写了这个
比如说写了一个啊
嗯就是比如我是写啊
然后的话其他是读副本的吧
他都是同步是吧
对对他会就是就是
我给你看上面这这张图
就是现在S3嘛再到DN再到cn
如果说我现在写了一个数据进来的话
DN会立刻触发它的破世模式
它会把现在更改完的数据推到
就它会检测哪一个就是哪一个
cn里面它的缓存有有这份数据
然后立刻推过去更新
这样的保证了
每个CN的开始
都是最近刚刚更新过的数据
就不会出现不一致
嗯
然后第三个问题就是
咱们这个更少的容易啊
嗯更少的容易的话这个呃
因为
你这个分不是吗就是
一般还走的这个是
就是三副本
如果说是更少容易1.5副本
他总总保证这个数据高就是
啊就是您说存储级别的高可用是吧
对啊就是对象存储
它可能是一点几个级别的荣誉
然后在这个通过I3和mi i o
它可能自带了一些
这种这种检检测机制
比如说他发现某
某一部分数据可能坏了
不可用了
或者说怎么样
他一定会
用自己的这个方式去把这个这个
种鱼度再重新拉起来
这个是目前就公园上
我们做了一些测试
就是目前
呃还没有出现过公园上的种鱼
结果不咋样
我们这个说的这个泳鱼说的是
是他S3上面这种也还是说我我们这个
我们上面那个计算节点
这这啊计算节点首先计算节点
计算节点他是提供的种鱼
主要是就相当于我是李级别的种鱼
比如我一个计算节点宕机了，我还有其他计算节点可以接管这个计算任务。存储的话现在就是完全依赖S3对象存储自己提供的冗余机制。咱们DN节点没有提供这种机制是吧啊
DN节点
现在比如说我们启动多个DN的话
多个DN之间它log TL是可以共享的
通过这个方式实现荣誉
比如我一个log TL数据
我在多个DN上都存一份
然后其中一个DN档机了
我从另外一个节点上能够找到这个
这个其中的这部分荣誉
就是这个log TL它是有容仪是吧
对他是有提供荣誉的对对啊
就是将来可能我这是日志的荣誉
然后真正的数据的荣誉是
咱们本身引擎里面是不
就不去配置这个而是由S3自己
对对这个是对
目前交给对象存储来做这个事情
啊就是说其实我只关心就是我其实对
对于我DNA说的话嗯我就闻
真正的数据存储就是S3那那一份是吧
嗯对是这样
就是就是嗯
这因为我们数据存在LOGO service里，它是三副本的，所以相当于在logoto数据本身就存了三份。

Q：刚才说那个a C，因为咱们这个是，就是你选择的线跟negative是融合的是吧。
嗯对对就是你工作负载隔离
工作负载隔离跟他说的这个
容器级跟那个机器级的隔离
对就是咱们现在不是多个节点吗
哦比如说那个你就是
咱们这个隔离是
这个隔离是在所谓的节点
真的所有节点还是说不分节点
咱们先不是有
有计算层事务层跟重筑层吗对是吧
啊如果说
就是你想要比如机器级别的隔离
那我可以选择比如3个logo service
我放在3个不同的物理服务器上
然后DN可能放在一个一个节点上
然后每个cn
可能再放到不同的物理机上
这是一种隔离方式
还有一种容器级别呢
我可能就选择3台服务器
然后呢呃每个log service放到一起
放到各自其容器
放到三个不同的服务器上
其他的cn节点我也打散了
放以容器的方式
在不同的这个这个物理服务器上启动
是这样的一个一个一个
就是提供的这种种子
就是或者说
隔离的这样一个一个机制
对，其实这个也是跟那个有关的
因为就是现在都
那个你的数据都放在S3上面去了
嗯其实
咱们那个作为一个HAHTAP的这个数据库
其实性能是最关注的
S3的话它就对向重组
对咱们重组不是有其实有有三种类型：一个是快、一个是文件、一个是对象存储。对象存储其实是最慢的存储。那怎么去解决这个问题？
A：针对对象存储，现在我们也在开发有关对象存储我们叫prefect，就是一些块。比如说对象存储当中的一些块，
如果说开启了这不是肺数功能之后，他可能会预先的把一些块先缓存到cn计算节点的cache上。
这样的话在使用的时候，它会跟基基于以前液化器的统计信息，摆截常用的块包，甚至包括一些元数据或者表结构
执行计划，提前先load进来。这样在使用这部分的时候，其实一定程度上可以提前做一些加速功能。目前按照我们CTO的设想是以后，冷数据跟热数据之间的数据差会比我们想象的要小。比如说差一两秒，将来如果真的这个prefer做好了，会变成一个比较理想的状态。其实这还有一个问题，就是数据存储的格式，比如说table，待在倍数下面tiber table下面，可能表的存储格式不是自己设计的，对你没有用。那个hudi还有灭欧和那个三尺是对持一种存储服务。所有的存储已经贴意都是我们自己写
是吧对对贴一是完全自己写的
其实这格式完全是自己自己定义的
完全可以用快存储
为什么一定要
就是如果你就是你想
你不想用对象存储的话
我们也现在提供那个单机版
就直接给您用最快的这个
这个磁盘也可以写
就是现在是
只是对象分布式是用这种架构
就单节点
因为我们可能没有讲
今天主要讲分布式单节点
我们是允许大家用NFS或者是这个
这个本地盘非常快的那个
Nnvme其实也可以哦
好好