啊大家上午好
今天是我是整个上午这场的最后一位
分享的这个
公司的产品架构师
我叫张潇
我们公司叫做矩阵起源

呃我自己呢
之前是呃今天我讲的这个题目呢
是我们的MatrixOne
是我们公司目前
开发的一个开源的分布式数据库
我演讲的一程
主要是
讲他从NewSQL到现在新的HTAP架构的整个架构的引进
可能相对来说更稍微抽象一点啊
呃接下来呢我来介绍一下我自己
我在2011年到2021年这十年的时间里一直是做全职的DBA
可能在金融在商业地产还有教育行业都有过相关的从业经历
2021年我入职到现在的矩阵起源做产品架构师，至今已经快两年了。

我之前做的数据库可能更多的像Oracle、MySQL、SQLServer，还有像PG卡三折这种关系型数据库更多一些
跟现在的工作既有一些交叉，可能又有很多之前没有涉及到的东西
今天我的整个目录是：

第一个，我们来测算早期的这个架构和遇到了什么样的难题
第二个，我们架构的升级之路。可能会给大家讲一讲，架构升级遇到了哪些东西
第三个，架构升级过程当中我们有什么样的困难还有收获
最后，我还对整我们现在的架构以及升级做一个最后的总结

第一部分 MatrixOne早期的架构和难题
我们早期的架构，如果大家在2022年的上半年之前有了解过我们产品的话，会发现我们当时的架构和我们现在
大家拿到宣传页的架构有很大的区别，其实这就是当时我们做架构升级。
早期的架构如果让我去总结的话，我总结两个词：一个是NewSQL，一个是MPP。
NewSQL就是当年谷歌那几篇论文衍生出来的很多的这个分布式数据库的一套理论体系。
它其中第一个非常重要的就是分布式架构，
其实解决的是传统像不管Alcohol Server，还是单机版本SQL，它的高可用以及水平扩展的这种难题。

另外一个就是多引擎。其实在当时谷歌发布的很多论文当中提到过了。可能用不同的引擎来做不同的事情，这是当时的一个NewSQL的一个大家认为的比较典型的特点。
第二个就是MPP，或者叫大规模并行计算。
这个主要的用途就是通过分布式的方式将一些规模比较大的计算任务分布到不同的节点，并且在计算完成之后汇总。其实充分的利用了分布式架构的这个算力资源。
而我们早年的早期的架构确实也是这个样子，比如大家能看到我们上面可能有一个，负责分发负载均衡的proxy
下面就是我们MatrixOne Server，每一个Server下面有自己的存储。实际上是一个存算不分离的这么一个架构，
每个节点看似是对等。当然这里面还有自己的问题。我们再看一下整个这个组件的详解。最上面这一层我们叫SQL前端，SQL前端它干嘛呢，它是为了兼容MySQL的语法协议，
这个协议还有它的语法，就是我们用不管是用MySQL client，还是JDBC都可以直接连。
而计算层是在一个是传统的这个SQL Parser，不管我们是做语法树的这个解析
还是用来支持多方言的这种SQL。都有它自己的用途，而下面就是我们自己写的这个MPP SQL执行器。
我们当时整个研发的同事们当时对他的一个设想是第一针对这个基础引擎做一些向量化的加速；
第二是我们部分操作甚至是用汇编语言做了改写；第三个就是我们当时使用了一个比较独到的因子化加速的能力。
所以最早如果大家看我们2021年的性能测试报告的话能看到还是有一个相当不错的性能表现。
但是后面为什么会舍弃掉这个东西呢？待会我给大家去讲。

而大家看到的是一个分布式框架，这一层是我们叫做Matrix Cube，是我们当时分布式组的同事做的一个开源的项目。其实当时也做了好多年，他提供的是一个整个对于多台机器的分布式存储框架。像高可用多副本，这个负载均衡强力制这种基础能力都有。而且当时设想是要用它来为对这个Matrix计算，提供分布式事务的支持能力。
大家如果说做过这个分布式系统的开发的话，也会发现这个分布式事务的实现是一个非常让人头疼的事情。直到现在我们公司现在做记忆板的时候，在分布事务上，我们还在投入很多的研发人力和时间，争取想要把这个东西攻克。
再一个就是在里面是有Raft协议的，而且中间会有一个调度器叫做profit Raft
本身大家也都知道它是一个有leader节点的一个分布式的协议。
而最下面呢，就是我们的计算层和存储层。存储层除了引擎接口之外，我们有三个引擎。
最中间的这个叫AOE、AOE是一个不支持事务，你可以往里写数据，但是对于这个这个事务、去重等等，它基本上是不支持的。最左边的我们叫TPE，TPE是用来保存原数据catalog，像大家能看到这个catalog，只是其实它是贯穿于从前端，到底下存储层，它是一个非常繁忙的引擎。
而最右边叫做TAE，TAE是一个典型的列式存储。能够提供完整的SID
同时我们希望它能够去支持比较大规模的OLAP能力。
所以早年我们看到的一个问题是三个存储运行
这也是后来我们为什么从架构层面整个进行重构的一个原因
这整个引擎详解的早起架构早期架构呢我给大家解释完了。

那么接下来我来讲一讲这个引擎他的问题在哪里，以及我们为什么要对他做整个升级重构。
首先就是扩展性，这是我们刚刚才我给大家看的是一个存算不分离的架构。
这个架构它在扩展性上有一个什么问题，每扩展一个单位的节点，就要去扩展相应的存和算两种资源，存算不分家。
而且因为我们数据是以3副本的形式保存，意味着我们只要一个节点加进去，他想要真正接管计算任务的时候要先把整个存储先完成同步。
那其实节点扩展它需要的时间就预热的时间非常长。
比如说我们大概有1TB的数据
那可能我们要先等着1TB数据的3副本完成在这个节点上同步，才能去开始提供所有的计算负载。
还有一个就是性能，因为Raft协议它一定是有一个leader就像刚才我说的，那么这里面就容易出现这个leader节点容易成为热点。很多的调度任务都从他这里走，在性能比较差的存储下，整体性能下降其实会超过预期。
这句话的意思是，比如我们用SSD做的这个benchmark测试的时候，我们预计它的性能比如说是10
那么我们最早预想HDD我们能跑到5或者6，但是实际情况是我们在HDD上可能跑出来的结果只有3或者4，这成为了一个性能上的瓶颈。再一个就是刚才给给大家看的三个存储引擎，TPE、 AOE、TE三个引擎，用途不同而且性能还不一样，有的时候经常会出现某一个业务场景三个引擎当中的一个成为了整个性能的桎梏。我没有办法继续往前推，最后这就是最短的这个板，成为了我们性能的瓶颈。

还有就是成本，我们当时最早是用3副本存储，那性能实际上这个节点规模越大，我们是线性提升的，这个成本负担就越重，到了公有云上，比如说有的公有云还提供了一个高可用的方案，那就成了壳子套壳子，就不是线型而是指数级增长了。可能我要用3个节点的时候，数据库里存了9副本。然后在公有云又可能做了一个3层冗余那就是27层。这就到了后面客户所承担的成本负担，实在是太重了。还有就是只有高配存储才能发挥出预期，这个就是我们大家所说的性能较差的存储架构发挥不出来我们想要给到的性能特性的时候，那只能不断的通过增加存储成本的方式来实现一个比较好的最佳实践。
面对这些问题，从去年的3月开始，整个就由我们CTO田丰博士作为牵头人，对整个架构做了一个重新的升级。
其实我们是从0.5开始做这个事情，0.1到0.4 大家更多的时候也是在一个探索的过程，我们到底该怎么怎么做，可能这是一个不断思考不断试错的一个过程。所以到了0.5的时候，我们终于意识到这个架构走不下去了。
所以怎么办呢，首先先分析一下为什么原有的架构走不下去了，实际上我们当时总结三座大山。
第一座大山就是分布式框架，这个分布式框架，多副本存储带来的是成本存储的飙升
第二座是leader选举，人为制造了热点
第三座大山，就是我们的引擎实在是太多了，三个存储引擎彼此之间的代码复用率非常低。比如说我在上面写一个新的功能，甚至代码的维护量可能差不多约等于3倍。还有就是我们所说的因子化算法有点过于激进，意味着除了我们那位主开就他能够对这个算法，包括这个计算引擎做出一些驾驭的话，其他的同事能参与度非常低，只能做一些辅助，加一个功能都会非常的困难。所以这个引擎当时也成为了我们不得不去剃掉的一个原因。

再一个就是资源分配，一开始我们说存算不分，那实际上我们再去做各种资源配比的时候，可能会出现的是不同业务场景，它的隔离性就非常差。
再一个就是这种share-nothing的架构，意味着扩展性非常差。必须同时扩展存和算两种资源
所以在我们总结了这3个根本的原因之后，开始做这个架构升级。
最左边这个大家可以看一下，一层套一层、一层叠一层，像个千层高一样。各层和各层之间又是强依赖的关系，所以第一步就要先把整个各层彻底的打散，做一个更加灵活解耦的这个整体的架构啊
最后我们从整理出来是一个是存储层单独做了一层，可以使用的是各种存储，不论对象存储FS、 HDFS。然后把所有的cache服务放在存储层来做，最上面就是计算层。所有的跟MPP以上的这些
，不管是前端也好Parser也好，还是说执行器也好，最后全都放到计算层。而中间我们又开发了一个新的层，叫做事务层。事务层可以看做为log service，其实大家熟悉的传统关系数据库中的Redo或者PG里的WAL日志，差不多跟我们log service有的功能上是非常类似的。
而我们还有一个事务节点DN，它是专门用来做事务服务，因为我们是一个分布式数据库。
就涉及到分布式的这个事务裁决，包括去重，还有这个落盘这些这些任务。
所以最终选择开发了一个单独的事务层来专门处理这些东西。
那么之前我们说的三个存储引擎肯定不可能全留，最后留哪一个呢？我们想了半天最后觉得还是TPE最合适，就是TPE它是一个行存，AOE它又是一个不能够去重，不能够支持事务的层级。
所以TPE它的好处是，第一它能够提供基于列存的TP引擎，而且它能够做到完整的这个事务的
原子性，一致性，隔离性和数据一致性等等，4就是以及这个OLTP能力。
它成为了我们认为最适合用于新架构的这样一个引擎，剩下两个引擎把一些可能有些功能想办法融到TE里，剩下两个就全都去掉。所以最终我们看到的是TPE，它是用列式的编码来存用column family，可以在行和列之间做一个切换，这个在column family那我会有一张图给大家看到。
这样做的好处是一个可以同时运行TP和AP的负载，因为这两个其实对于有的时候对于TP和AP我们经常说行存更适合TP，列存更适合AP，现在做完这样一个转换之后，行和列就都能够兼顾到了。
再一个就是所有的表都能够实现表级别的快照事务隔离，这是我们现在已经实现的快照隔离。
当然现在还在做的叫做2c，就是独立提交，大家平时在数据库当中打交到最多的那个会在我们下一个迭代的时候发布，即主键唯一键排序。外键索引也会在这个引擎里面一起实现。
就是之前UE和这个TP做不了的事情，我们在TAE上都会让它用一个引擎去完成。还有就是多副本和分片，因为之前我们说过，数据要保存三副本，而且每个副本它在数据库里面是以分片的形式保存，用的是数据库自带的操作系统自带来完成冷热数据的管理。
但是呢我们如果说现在这个新架构
就有一个新的要求
就是冷热数据尽量分离
读写请求分离
要实现一个对存储的精细化管理
最后所以说选择的是什么
选择了亚马逊的这个S3对象存储
还有甚至私有化的话
就是S3的这种兼容
兼容协议的对象存储
热数据就保存在计算节点的这个这个
存缓存开始上
而所有的节点呢也都实现了无状态
呃并发能力
可以通过
比如说我们现在对于这个呃
AP业务现在可能要
APP业务现在可能要求
更多的绘画的时候
那我们多启动几个计算节点
如果说AP我们需要更多的计算的话
那么我们
可以尝试着将这个
这个计算机点进行扩容
彼此之间三个层级
不不需要再再过度的依赖
然后呢就是最后我们就是说从这个
到月从这个这个分布式和
存储完成之后呢呢就是计算层
计算层的话之前是因子化算法
构建这个执行计划
做复杂的这个
这个加速查询
主要是针对对AP性能的这个提高
但是呢
它的问题也是表达式和节点的这个
这个抽象仪表述啊太复杂了
不管是增加功能
还是说我去修改什么东西
难度非常的高
呃而且呢就像我所所说的
多个引擎之间的代码服用率太低
就导致了呃
这个对这个
不管我去做什么事情
我的工作量都是两三倍的增长
所以最后弄了一个新的
我们叫MPP执行计划
一个是它是基于这个DAG
来构建执行计划
能够实现节点内和节点之间的
这个调度
待会给大家讲
我们节点之间的这个调度是怎么做的
然后呢
同时能够满足并行和并发两种请求
因为我们都知道并行可能是AP
并发是TP
通常大家会会这么这么这么去处理
而且SQL能力得到了完善
像之前可能
只支持的非常不太好的像自查询啊
窗口函数啊CTE啊
还有像这个内存溢出等等等等
现在都具备了这个能力
而且现在除了spill
这个内存溢出可能还没有还还
在开发剩下的几个自查询窗口函数CTE
现在都已经实现了
呃这样的好处
而且是在未来的优化空间更大
就是不管是
这个这个我们的主开还是说
其他的计算组的成员
都可以对他做优化加新功能
这现在我们这个从0.5之后
都有一个比较深刻的体会
我们不管是做什么其实都在
呃整个的这个这个代码优化上比以前
节省了很多很多的人力
最后给大家来就是
做完这么多的这个这个设计之后
我们来看一下
现在一个总体的架构是什么样子
首先最下面呢
我们叫做这个叫做file service
这是我们一个呃分布式的同事
他写的一个
统一的文件读写的这么一个
一个服务接口
它能够从S3对向存储去读数据
并且呢把这些数据呢推给
像不管是日志啊
像这个这个
呃我们的计算节点
或者是事务节点等等等等
而且事务节点和计算节点
日志又可以通过它去写
S3就是就是所有的这个节点只需要跟
这个file service打交道
就能够完成对存储的读写
呃然后呢全量的存储呢
都可以保存在这个I3上
I3相就是尤其是公共云版的I3
大家应该
多少有点体会
就是一个是它的成本非常的低
再一个就是公共云版的这个I3呢
它我们有时候
我们可以认为它是无限伸缩啊
容量是只要只要你肯交费
要多少有多少
然后呢在上面的这个事务层呢
我们有两个DN节点这个就是
我们专门去负责管理
呃日式服务和这个这个
呃原数据的
平时的话他会在里面混存原数据
做一些事务裁决啊
并且呢会指挥这个这个
我们家log service的这个日制服务
来落盘写数据
呃而且呢他自己呢呃
就是通过三副本的方式
来保证从日志级别上的这个高可用
呃而且这里面还有一个叫local的东西
这个待会我给大家去详细解释
local和现在的这个落盘的数据之间
是如何共同
完成我们数据写的这个过程
最上面呢就是我们所有severalist的
我们叫CN节点计算节点
计算节点的话呃它是完完全无状态的
每个计算节点有自己的开始
彼此之间
比如说而且好处是比如说我现在cn
我是最近的这个计算
计算的负载比较高那我就多起几个
如果说可能现在业务量比较比较
比较低的话
到了一些像像这种
大家过年了没有什么
业务的时候
那干脆把节点全都当机
节省一些成本
而存储的话
呃存储藏
现在TAE
我们是完全实现了裂存级别的
这个这个管理
大家可看到从数据库到表
再到segment再往下是是是一列一列
而且我们的列的单位呢
是是列级别的block
或者说块
一次他会比如读的时候
他会从一个列里面去读大概多少行
作为一个block
推给上面的计算节点
或者说日志节点
而可能大家就是说这个
这个比较关心的就是
我们如何在AP和TP之间找
到一个平衡点
现在的如果说大家默认的去建一张表
那都是列
就大家能看到的是右边的这个这个
但是如果我想某一些某一些表
我想强化一下它的
行存的性能怎么办
我建一个叫做column family
就对某一些行做一些特殊的优化
那么在
这对对这一这些可能需要频繁去做
在上面做
所以或者说是更新的这些
这些列呢
通过Ctrl分泌列的方式
能够大幅的提升他的TP性能
那最终可能就是
呃我只需要存一个副本
但是我在有些表上做好一些
优化之后呢
就可以实现
这个行存和列存在
可能各自性能上的这种这种优势
当然
这个康乐分类现在还正在开发当中
可能会在未来的一两个迭代之后
会会有一个
最初的版本
版本给大家来用
而计算的话呢
呃就像我说的
我们现在实现了节点之间的调度
就是大家能看到
所有的每个计算节点之间
都有个双箭头
这是什么意思呢
就是我们每个计算节点
比如说我现在是从
最左边的计算节点进来
我需要去做一个数据的查询
但是我发现他的
开始里面没有我想要的这个数据
怎么办
他会便利所有的其他计算节点
去找
哪一个节点有自己想要的这个数据
有的话呢
他直接在那个节点里面
把计算任务完成
再把这个结果最后返回到最初的
接受请求的节点
这样的好处是最大限度的
利用了这个这个
不同节点之间缓存不同的热数据
其实就对于一些常用的察觉
可能是一个非常大的性能的提升
而且呢现在我们计算节点
除了这个缓存以外
上面还有一个自己写的pipeline
将这个这个很多的这个这个sick请求
拆解成为
物理执行计划来进行执行
上面是正目前正在开发的一个优化器
而最上面呢
就是我们一直在上一个迭代代码
服用最多的Myseco级别的
这个passer
能够对语法做一些
呃做一些这个解析啊
同时呢呃还能够去做一些这个
可能方言上的支持
比如说我们现在呃也在做一些
对PG的语法和方言的支持
其实都是这个Myseco passer来做的
那其实做了这么多之后呢
呃可能大家现在比较好奇的是
整个这过程当中我们遇到了哪些难题
又是怎么解决的
因为
前面的架构和和这个后面的架构差异
不不论是整个架构的差异
还是每个组件的这个
这个功能呢都有非常大的变化
首先就是其实面对的几个难题
第一个难题是
如何寻找一个
能够对高性能计算引擎匹配的存储
两个两个核心的需求
一个是更少的溶于
一个是更低的使用成本
当然这个更少的
最后我们经过很多的论证之后发现
就是亚马逊的
S3对象存储其实能够完美的匹配
我们这两个核心的需求
就是呃一个是比如说我们现在
整个的这个单单副本
亚马逊基本上他应该是一点几的
一点几个副本吧
差不多就是呃多了多了20%的荣誉其
实这个成本比起我们之前的三副本
他的整个荣誉度是大幅下降的
成本上可能从之前的两三倍的成本
变成了一点几倍的成本
还能保证一定程度上的数据荣誉
而且呢呃在这个这个使用上来说
S3现在匹配S3的各种各种
接口啊各种各样的这个方式开发
其实都已经慢慢的成熟起来
还有就是S3自带的这个冷热分离啊
其实呃导致使用在我们使用起来的话
一方面冷数据我们放在S3里面
降低的成本
然后呢热数据呢放到作为开始呢
放到这个计算节点上
就基本上也完成了
用更就更低的成本
来实现
冷热数据分离的这样一个一个事情
就现在
大家如果说去试用一下我们的东西
在这个有些有些冷数据和
冷数据可能作为这个S3放对象
存储热数据用快式的话
这个性能的差异其实是非常明显的
第二个就是
我们在事务层是如何做分工
因为分布式数据库
分布式事务始终是一个非常大的难点
一开始我们希望的是什么
是我们的CN
就是计算节点只负责计算
所有的什么事务ID的生成事务裁决
还有这个一致性隔离性
呃包括数据的读写全都由DN来完成
就是我们的事务节点来完成
然后呢所有的这个冲突检测
还有约束完整性也都由DN来完成
但是我们做到后面就发现一个问题
就是DN会成为瓶颈
为什么呢
因为我们平时启动的这个事物节点的
数量其实是远远少于
计算节点的数量
如果说事物节点起的多了
在事务裁决上
多个事物节点之间同步又会出现问题
所以这当时是DN成为了一个瓶颈
于是我们做的第一件事情
引入一个概念叫做Locktale
因为大家如果相信
平时大家去去对数据库
呃这个写的流程有有一定了解的朋友
会知道我们怎么写
首先先把数据这个操作写到日日里
然后再落盘去写
这样的好处是什么
如果我们这时候在写的过程当中
发生宕机了
我只需要回放日志
就可以保证数据最终还是可以落盘的
所以我们
选择了引入一个叫做logotel的东西
它是什么呢
它是我们先把数据
写到日志里叫做logotel
然后呢logtel呢
在它会定期的把这部分数据
写入到S3对象存储
就不需要频繁的去写
就相当于攒一波往里推
攒一波往里推
这个是是这样的一个一个
这么一个一个机制
这样的好处是什么呢
就是我们在写的时候
不再局限于整个DN的这个
这个这写的性能DN只需要一次
就是攒够这一大批一起往里写一次
平时当DN这个不怎么忙的时候
他他他
他可以选择在自己不怎么忙的时候
把他写进去
忙的话那当然可能可能全都
这个这个混存的Locktale里
这样的话呢
cn只需要负责所有的事物和事物逻辑
还有计算
DN保DN既保留最近一段的数据
同时又负责这个日制服务这
样的话其实把DN一定程度上解放出来
使得这个写入的这个动作呢
它的上限其实基本上是被打破了
但是还面临一个问题
就是事事务量非常大的时候
怎么样保证写的性能呢
当时我们选择一个新的策略
就是如果说我们是批量写入一大
就是大概比如说一下子写入个几
几百兆的这种数据的时候
我们不再通过日志
而是直接往对象S3里写
只是告诉日志服务我要我要通过
什么样的操作
在哪个文件里写什么东西
而那些比较小的事物呢
可能比如说只是更新一两行数据
或者插入一个新数据
仍然还是走原来的
从计算节点到事务节点再到对象存储
这么一个过程
并且呢现在我们将这个约束完整性
和这个冲突检测
都放在了cn来做
就一定程度上让
让我们的事物DNG点呢更加的灵活
就整个的它的负载更轻
这样的好处是
写入性能比之前明显的提升了
比如说我们现在
如果说我们没有开这个这个
批量切入
直接插入S3的话
可能几百兆数据我们要写几十秒
但是现在的话可能
几秒钟大概一两秒就能写完
整个的
这个提升呢是大概提升了一个数量级
然后呢呃
那么现在还面临一个问题
就是我们如何实现
不同业务类型的这个负载工作
工作负载的这个隔离
因为像大家可以看一下
我们现在按照我们现在的架构
如果说AOTP级别它是怎么做
首先计算节点先
放先把数据推给事务节点
事务节点呢再再通过日志写到3里
如果是这个OAP负载呢
那可能就是我直接从
直接从这个这个S3里面去读数据
来进行来进行计算就可以了
呃
这样呢
然后我们现在所选择的方式是什么呢
就是用不同的cn节点来跑不同的东西
比如说我们现在第成立第一个CN组
他只跑TP业务
第二个CN组他只跑AP业务
实现计算就计算节点之间的隔离
如果说当然
如果你觉得我的这个这个
系统比较重要
我的预算比较充裕
那你可选择用机器
就服务器级别的隔离
我用物理机
可能来部署不同的计算节点
如果这些机器你想只想
用这个这个低成本的方式跑呢
我们也提供了容器级别的隔离
容器级别
可以实现数据和这个负载的这个这个
完全隔离
所以最终我们通过这个这个问题
我们最后把它做成了一个什么样子呢
大家可以看这张图
就是我们现在给先打一个
先做了一个新的概念叫先label
就是标签化
比如说我们现在看到这里面
有三个是打了AP的标签
一个打了TP的标签
那我一个绘画进来的时候
我先去看这个是一个优化器
会先去判断他是一个AP请求
还是TP请求
TP请求那就进TP的计算节点
AP的话那就进AP的计算节点
这样的好处就是这个不会出现资源
就是两种业务上的资源中用
呃如果哪边
比如说哪边现在业务更更高的时候
那我就选择
对哪一边分配更多的资源
当然未来我们还会有个设想
就是希望能够实现自动的负载均衡
比如说通过优化器
通过某一段时间的统计信息
我们来判断
这个最近可能TP业务更多一些
那我就自动扩容一些TP的计算节点
AP的更多呢
我们自动扩容一些AP的节点
到现在目前这个就是我们目前
公测的这个0.8版呢
主要是呃
主要提供给用户的是手动
手动的通过配置标签的方式
让用户把
自己的这个这个不同类型的负载
打到不同的计算节点上来实现
就这样的一个一个一个一个实现方式
但是整个升级过程呢
呃其实也是个很痛苦的阶段吧
所以这段时
那段时间我也跟我们很多的研发同事
做了一些
大家一些语言上的交流
还有像技术上的一些一些复盘
所以我后来我就问他们
在整个升级过程当中
你们都有什么样的收获
你们对现在的这个这个产品
又有什么样的新的了解呢
一个就是
很多我们很多同事
对整个执行计划做的重构
包括像这个语法的解析执行计划
还有现在甚至连c口语言标准
大家都有了新的认知
就从头到尾我们相当于
呃将整个的从一条c口
从客户端进入服务器再到完成执行
整个过程我们做了成功
所以很多同事
之前对这些东西了解
其实是还浮于表面
但是经过这
次整个系统升级之后
他们对这种
这些东西的理解加深了很多
现在我们大家再去讨论各种执行计划
讨论这个开销包括语法
如意的事情的时候
我们经常大家是聊的是是
聊的可以很深入
甚至CQ标准
就是我们当时还公司当时还出钱
大概出了几千美元
买了这个CQ标准的那个
那个那个那个字典啊大家都去
就是我建议大家平时有空的时候
你们一定要去翻
看一看标准是怎么定义的
这样有助于你们更更好的去理解
每一条语句
它背后所所支撑的逻辑
还有就是这个事物和acid
因为之前我们是多引擎嘛
那多引擎有的同事开发的时候
他就不需要考虑事物的这个acid啊
然后现在不一样了
现在每一条你都要考虑
事物的四个特性
那这个时候对于整个事物的理解
大家可能更多
不同的隔离级别是怎么做的
那所服务要怎么怎么搞
为什么会
会有今天的这样的一个一个情况
呃然后呢在开发15层的时候
就是cn和DN的这个适配
我们当时是从去年9月一直到11月
两个月的时间里面
就是在解决这个东西
就是作为分布式事务到底该怎么分工
既能够保证完成事务的这个acid
同时又能够保证让系统的架构
和这个系统的负载
不会出现明显的短板和弱点
所以在当时
大家反复验证了将近两个月
就是这个这个
最终的结果就是引入log tale
并且呢CN
和DN一个只负责
这个这个原数据
另外一个负责计算和逻辑以及驱虫
嗯而log tale呢
其实还有
最后我们发现log tale还有个什么好处
因为它是放在DN里
它可以实现不同的计算节点
对这一部分数据的共享
不需要再从这个对象存储里直接low
存储层呢
大家积累了两样东西啊
一个是S3对象存储
就积累了这种SN对象存储的开发经验
因为之前我们很多同事可能
没有做过这个共有云的开发
但是在开发这段时间里面
对对象存储的使用有了相当大的这个
这个进步
还有就是我们现在自己的这个file
service文件
这个文件服务
基本上这个开发完以后
大家很多的时候
在使用
不同类型的这个这个存储的时候
不再需要考虑很多很多啊
不接口要怎么写
我的兼容性怎么样我的性能怎么样
统一交给file service去实现这个东西
呃
那么最后呢我来给大家做一个总结
就是我们整个系统架构升从2022年的
4月一直到11月
差不多半年左右的时间吧大概
呃做了就是哪些东西是值得
值得去给可能给给大家去借鉴或者说
呃
也许会给大家带来一些不一样的思考
就说一个是呃
存算一体的这样一个分布式架构呢
它有它自己的好处
但是它也有它自己的问题
就是这个容易制造热点
可能再加上成本等等等等这些问题
呃而我们完成了三层的结偶之后呢
每一个层级
可以自可以自行的进行扩缩容
不再依赖于其他层面
这种灵活结偶的架构
其实在呃不同的业务需求上
确实可以得到不同的这个
不同的这种最佳实践
比如说有些业务
可能我需要更多的计算资源
你可以直加计算资源
而像以前的不管是集中数据库还是说
存算不分的这种数据库
其实都面临只要扩容
存和算都要区分
第二个就是多引擎到单引擎
因为多引擎之前我给大家说过
多引擎一个是要维护的代码量
以及可能每
你要考虑每个引擎的特性
之间怎么样去搭配
但是单引擎我现在只需要考虑的是
是一个单引擎我是怎么做这个事情
其实对于大家的这个这个工作量啊
包括
呃对于这个这个整个HCAP的设计呢
其实都
都比我觉得比多引擎可能具备的更
更节省
更节省人力的这样的一个一个
一个用处
呃因子化算法到DHG
就是有些东西确确实实是很好
但是我们不仅仅要讲
不仅仅要仰望天空更要脚踏实地
就是在当下的话在没有那么多
人才储备的情况下
或者说对于有些还不够成熟的东西
呃不足以支撑
起我们未来的产品发展的情况之下
那选择一些更实际的方式
更有效的方式
让更多的人能够参与的方式
去构建执行计划
可能这是我们最终的归宿
还有就是从多副本存储到对象存储
vlog tale的引入
就之前的多副本存储带来的成本问题
可能在在对于一
些对成本比较敏感的客户那里
这是一个
这是一个天坑
那现在我们对象存储
和logo tale引入之后
实际上存储的成本降到了原来的
1/3左右
对于对于这个这个
以后可能数据量越来越大的用户来说
他未来对于成本的这种焦虑
会大幅的下滑
再就是灵活调整节点的来
就是带来的这个
这个资源隔离分配带来的
这种资源隔离
一方面就是我们所说的存算分离
并且就是像水多加面
面多加水更加灵活的资源配比
再一个就是通过用标签的方式
呃就是将这个一些请求啊
强制隔离到不同的节点上
避免出现不同业务类型
对资源上的这种
征征用啊
是这样
然后呢这个两这是一个是我们
呃企业服务号
平时我们有很多的这个呃文章
可能是我们内部
内部同事写的一些干货
还有就是可能我们在外部的一些呃
新的进展啊
还有某些客户的这个新鲜
客户给我们的反馈啊
都会在里面有
右面呢是我们的企业微信群
如果单这个这个这个码如果扫完
加不进去也没关系
我们那边有展台
可以扫那个那个那个码也可以进群啊
然后呢我最后再介绍一下
我们公司现在有一个叫做
呃Beta program就用户体验计划
这个是我们目前为一些
呃有即将有合作意向的
这个这个客户呢提供的一个
一个专属
一方面呃如果您参与有什么好处呢
一个是
新功能我们可以第一时间交给您去用
啊并且呢可
能有些有些比较匹配的
这个比较匹配您业务的场景呢
我们会做一些
定制还有呢
就是您可以甚至直接
参与到我们整个产品的设计当中
比如说您觉得这个这个功能
你们当前有什么痛点
还可以那可以直接直接找到我们
我们会告诉你
这个东西可能怎么怎么解决
如果当前我没有这个功能
可能会优先以您的需求来作为
蓝本去设计
呃然后再就是呃
像比如说有一些
您在使用我们的这个产品的时候
遇到了各种问题啊
可能我们可以直
您可以直接找到我们研发团队
很资深的工程师
那他们会告诉你可能这些东西啊
大概怎么用
可能会会有一些指导
现在呢
我们是属于到0.8叫做Beta program阶段
那么我们您会在第三季度发布
我们就接就是正式版
然后我们现在也提供了公务员版
公务员版呢现在也是在
呃这个公开招募阶段
大家可以就我们展台有那小
就是小册子
大家可以看到
如果对我们的这公务员感兴趣
也可以申请使用
目前我们使用的是service计划
呃可以可以去在
在上面跑一些像TBC是或者说TBCC这种
比较常见的奔驰Mac来看一看
以及如果你有一些
基于像Misco这种开发的
这样的一些应用程序呢
你可以在我们上面做一些测试啊
就只要扫一下大家右下角的这个
二维码就会就会有
就会有这个用户体验计划参与
当然如果你觉得就是还有还想再了
解更多也可以到我们展台
跟我还有我们同事约
我们两个人都可以给你
给大家做一些解答
呃今天有关
呃我们整个产品架构的升级分享呢
我就讲到这里了
那接下来我们还是到提问环节
有问题的话可以现在举手提问
啊老师你好
我就想问一下
这个后期有没有计划
接入更多的存储引擎
我看现在目前咱们不是接了S3吗
嗯是有
比如说像Mio
或者是HDFS之类的这些个引擎
呃确实是有
我们现在私有化的场景呢
就是以mi i o作为私有化部署的方案
呃当然
现在就是整个对象存储越来越多嘛
呃如果说
在这个mi i o比较成熟以后
我们也会选择更多的存储对象来支持
呃现阶段的话
私有化
我们标准版本就是私有化是mi i o
工友云版本就是S3或者是阿里云的OSS
好第二个问题就是您这边就是
呃如果说对企业级的用户
会不会有那种
定制化的支持
比如说呃从
对于解决方案的设计
对于企业级
的那个应用方案的设计
以及运用a的一些个设计等等
呃会有
就首先我们现在企业客户会有一个
就是呃付费用户
会有一个单独预约工具
这个预约工具呢
对于我们整个就是不管是集群也好
还是私有号管控也好
都会有更好的这个使用
然后呢如果您对于
比如说自己行业内的这个应用程序
大概可能怎么在我们这做
落地最佳实践
我们确实现在也有这方面的
这个这个同事一直在做这个事情
比如说你想做一些定制化的这个开发
或者是一些应用程序的优化
我们也会有人
我们也会有这个技术
技术上的同时会去帮帮
帮您去做这个事情
啊我最后一个问题
还是就是
目前我们的这个合作的企业中
大概都是哪些行业的嗯
呃现在呃
可能一个是一些
呃以制造业为主的一些企业
比如说像麦斯
麦斯客户他们会有一些呃
他们会把我们的数据库
嵌入到他们的这个产品里面去做一些
应用
还有一些就是BI类的比如说呃像像
翻软啊红友啊他们这些BI客户呢
也在尝试着把我们的数据库作为
MPP层呃
去去推给他们的用户来使用
再还有一些像公有云上
也有一些中小企业客户呢
会直接把
他们的应用程序布在公有云上
使用我们公有云版的数据库
现在是这样一个情况
呃也也还在不断的拓展
我估计等到1.0的时候
我们应该客户的这个不
不论是行业还是说场景都会更多
好那个还有没有其他的观众想提问
好
哎
老师你好哎你好
就是问请教几个问题吧就是
第一个问题就是怎么跟他说那个
就是吸烟计算的时候
就是事故啊
吸烟跟那个
低烟
这个事故的时候就是低烟成为平静
引入这个lock tire写着无上限
这个怎么理解这个
啊
就你这引引入这
因为我我看了一下你这个lock tire的话
好像你
这是本地是DN本地的存储是吧
对对是
这就跟那个怎么那个
那个重储层的话都不不在一个
哦呃平时是这样
我们所有的这个DN
可能它是和平时
和这个写数据打交道是最多的
如果我们就是每条数据
比如说
只要只要是写一条就要直接落盘的话
其实这个落盘的频率非常高嘛
DN它作为一个一个节点
它可能这时候就会成一
个高负载的节点
他要发起很多写入动作
引入Locktale之后呢
我们很多的数据
先由DN缓存到自己的Locktale上
那么等到他可能攒够了一批之后
他再统一写一次
其实对他
整个他的负载程度就没有那么高了
呃这是其中之一
第二就是呃
我们现在的设计上
可能是cn的数量是远远多于DN的数量
所以经常会出现
呃高并发的情况之下
可能我会有好几个cn
同时往一个DN里写
所以这个时候DN它的带宽
它的处理能力
这个时候有可能就会成为一个瓶颈
所以尽量我们想的是
把DN的工作负载降到最低
来实现写入写入性能的不断提升
嗯就是说这个比如说
大量的吸烟去写同时写一个DN是吧
然后就是说我为了
因为这个DN的话就是可能如果说你
没有个logo tire的话
就是我们一般的话都是先写日志是吧
对对对如果先写日志的话
传统的话就是说
先写日志的话没有本地
你不走本地盘直接罗S3的话是不是
哦就是你直接写3这样的就是呃
如果说您的数事故量特别大的话
你可以把那个那个那个参数开关打开
打开的话比如超过一个预值
比如说10兆
那么他会日志里只会记录我在哪个
我在哪个数据文件里写了什么
写了哪些内容
然后呢呃
直接把直直接跳过了
先写日志再再落盘的过程
就是这也是给用户一个选择
就是您是想
您是想就是说提高性能
当然
有可能出现一定一个一定程度上
就是这个时候我掐写了一半了
然后档机可能会会
日志没法回放的情况
这也是给把选择权交给用户
而且觉得这个这个写的过程呢比较快
就比如说100兆的数据
我们可能一两秒就写完了
就是像这种顺序写吧是吧
嗯他们又更新是吧
日志对这个
嗯嗯其实我想问的就是这个
这里面的
问题就是为为什么我们设计成
设计成就是单独的存储
就是在直接在DN上面的话挂存储
就是locktail
而不放在那个
就是统一的那个存储材
哦local首先就是
呃local我们给它配的是比较好的存储
就它就直接在里面
缓存的时候
它往里写的性能其实是比S3要更好的
它相当于一个中转站
呃这这第一个好处
第二个就是Locktale它存在DN里面的话
我不同的cn只要这个数据它没有落盘
没有被创k到之前
所有的cn都可以共享
比如说我恰好就需要Locktale
数据的时候
我直接从
Locktale里读不需要再走入33
其实一定程度上
也就是对CN也做了一个加速
这个也是一个共享的是吧
对对对它是一个共享
DN节点之间共享的是吧
对对这个logo啊
是啊它是一个共享的存储
这个可能就是走走的是快存储
而不是对线存储是吧
是是它的性能更好一些啊
这是一个第二个问题是
第二个问题是怎么说的
这个就是冷热
冷热与那个毒血分离啊
其实咱们现在怎么怎么体现这个冷热
跟毒血分离啊
呃冷热首先冷热数据一个是S3
他自己提供了一个冷
热数据分离的机制
就他常用数据
他可能读取速度会比
呃这个冷数据稍微快一些
然后其次呢
我们在每一个节点上
不管是DN还是cn上
也放了自己的缓冲区
就会允许用户把一些就常用的数据
以这个内存的方式
放在自己内存里
然后自己还会有一些如果
当然如果你想除了内存以外
在每个节点上再配一块高性能盘
再缓存一些数据
其实也可以做
就是实际上冷热数据
他实现了多机分离
呃就是就是
比如说
嗯本身重组残他有这个机制是吧
其实就是开始是吧你说
对对就是开始
对对可以这么理解
但像这个的话
比如说我我们
用户你是控制不了这个啊
这个是个确就实际上是一个
就是用那个
那个由交给S3对象存储
我开始自己来做
可能用户没有办法
就目前为止吧
我觉得可能没有哪个数据库
能够精准的告诉大家
我开始让存某个精准的
精准的就实存某个数据
就是可能最近用的多的我放到内存里
稍微少一点的我放到本地盘
然后
再冷一些的那我就直接只从S3里读取
就相当于三个级别嗯
读写这个是冷热读写分离怎么
读读写分离的话就是就是现在是
比如说我在我在去写的时候
读的时候
可能我我
我只需要去从内存开始里面去读
我不需要涉及到去写
然后不同的计算节点
可能就是就刚才有一张图
我给你找一下
就是比如说现在有有一个请求
协读请求进来了
那么我可能直接
直接从这他的开始里面去找
找到对应的开始直接直接就可以读
这时候其实这个这一定程度上
呃然后我这个cn
继续可以负责只负载去写
这节点之间可以实现这个这个
自自适应调度
然后再就是这种这种标签
不同的标签我可以实现不同的打
基于不同的标签的情况之下
有的可能TP只负责写AP只负责读
就是相当于在
这部分资源是这部分资源是完成
隔离了
专款专用的这种这种这种使用方式
当然以后
等到我们优化器逐渐的强大之后
可能还会有在一个节点内部
我们画出某一部分区域只负责给读
一部分只给写
这个可能还需要一段时间
才能开发出来
这是读
写没问题啊
但是你读的话一致性怎么保证
读一啊您说读哪方面的一致性
比如说这咱们不是说吗
比如在cn一个一个cn上面CN1上面是
写了然后我读的时候我是先2上面读
哦就是我们所有写的数据
都会先推到Logto里
保证它是全局可以读到的
就cn只负责计算
只要cn计算完了就立刻把数据推给DN
是这样他查的时候还是要经过DN
对对是这样的就是呃
开始里有的话
如果必须这样我们现在叫一个Locktale
我们叫铺式模式
就只要Locktale里的任何数据
发生更改了
他立刻会把更改的数据
推到每推到对应cn的这个开始里
之前我们叫破模式他是要自己主动拉
现在是DN主动的去推
所以就保证了
DN里的开始的这个数据和
cn的开始数据是它是同步的
就是这个DN嗯应该是写了这个
比如说写了一个啊
嗯就是比如我是写啊
然后的话其他是读副本的吧
他都是同步是吧
对对他会就是就是
我给你看上面这这张图
就是现在S3嘛再到DN再到cn
如果说我现在写了一个数据进来的话
DN会立刻触发它的破世模式
它会把现在更改完的数据推到
就它会检测哪一个就是哪一个
cn里面它的缓存有有这份数据
然后立刻推过去更新
这样的保证了
每个CN的开始
都是最近刚刚更新过的数据
就不会出现不一致
嗯
然后第三个问题就是
咱们这个更少的容易啊
嗯更少的容易的话这个呃
因为
你这个分不是吗就是
一般还走的这个是
就是三副本
如果说是更少容易1.5副本
他总总保证这个数据高就是
啊就是您说存储级别的高可用是吧
对啊就是对象存储
它可能是一点几个级别的荣誉
然后在这个通过I3和mi i o
它可能自带了一些
这种这种检检测机制
比如说他发现某
某一部分数据可能坏了
不可用了
或者说怎么样
他一定会
用自己的这个方式去把这个这个
种鱼度再重新拉起来
这个是目前就公园上
我们做了一些测试
就是目前
呃还没有出现过公园上的种鱼
结果不咋样
我们这个说的这个泳鱼说的是
是他S3上面这种也还是说我我们这个
我们上面那个计算节点
这这啊计算节点首先计算节点
计算节点他是提供的种鱼
主要是就相当于我实力级别的种鱼
比如我一个计算节点宕机了
我还有其他计算节点可以接管这个
这个计算
计算的任务
嗯存储的话现在就是完全依赖S3
对象存储自己提供的荣誉机制
就咱们DN节点没有没有
提供这种仪这种机制是吧啊
DN节点
现在比如说我们启动多个DN的话
多个DN之间它log TL是可以共享的
通过这个方式实现荣誉
比如我一个log TL数据
我在多个DN上都存一份
然后其中一个DN档机了
我从另外一个节点上能够找到这个
这个其中的这部分荣誉
就是这个log TL它是有容仪是吧
对他是有提供荣誉的对对啊
就是将来可能我这是日志的荣誉
然后真正的数据的荣誉是
咱们本身引擎里面是不
就不去配置这个而是由S3自己
对对这个是对
目前交给对象存储来做这个事情
啊就是说其实我只关心就是我其实对
对于我DNA说的话嗯我就闻
真正的数据存储就是S3那那一份是吧
嗯对是这样
就是就是嗯
这因为我们LOGO service
这个数据存在LOGO service里嘛
它是三副本的
所以相当于这个
在logoto数据本身就存了三份
哦
然后最后一个问题就是那个
刚才说那个a C
因为咱们这个是
就是你选择的线跟
negative是融合的是吧
嗯对对就是你工作负载隔离
工作负载隔离跟他说的这个
容器级跟那个机器级的隔离
对就是咱们现在不是多个节点吗
哦比如说那个你就是
咱们这个隔离是
这个隔离是在所谓的节点
真的所有节点还是说不分节点
咱们先不是有
有计算层事务层跟重筑层吗对是吧
啊如果说
就是你想要比如机器级别的隔离
那我可以选择比如说呃
3个logo 3个logo service
我放在3个不同的物理服务器上
然后DN可能放在一个一个节点上
然后每个cn
可能再放到不同的物理机上
这是一种隔离方式
还有一种容器级别呢
我可能就选择3台服务器
然后呢呃每个log service放到一起
放到各自其容器
放到三个不同的服务器上
其他的cn节点我也打散了
放以容器的方式
在不同的这个这个物理服务器上启动
是这样的一个一个一个
就是提供的这种种子
就是或者说
隔离的这样一个一个机制
对
哎其实这个也是跟那个有关的
因为就是现在都
那个你的数据都放在S3上面去了
嗯其实
咱们那个作为一个HAHTAP的这个数据库
其实性能是最关注的
S3的话它就对向重组
对咱们重组不是有
其实有有三种
三种类型嘛一个快一个是文件
一个是那个对象存储
对象存储其实是最慢的存储
对是这样
那咋怎么怎么去解决这个问题
对象存储的话现在这个
这个这个现在我们也在开发
就是有关对象存储我们叫prefect
就是一些
呃一些一些块吧
比如说对象存储当中一些块
如果说开了
开启了这不是肺数功能之后
他会
可能会预先的把一些块先缓存到cn
就是计算节点的开始上
这样的话在使用的时候
呃
它会跟基基于以前液化器的统计信息
摆截常用的块包
甚至包括一些原数据或者表结构
执行计划
提前先漏的进来
那这样的在使用的这部分的时候
其实一定程度上可以提前
提前做一些加速功能
呃目前按照我们CTO的设想是以后
冷数据跟热数据之间的这个数据差
会比会会比我们想象的要小
比如说差差个一两秒
会将来如果真的这个prefer做好了
会变成这样
像一个比较理想的状态
其实这个这个还
有一个问题
就是说你这个数据重储的格式
比如说table
嗯待在倍数下面tiber table下面
可能我
表的重储格式是是不是自己设计的
对你没有用
那个a蝴蝶还有什么
啊对对啊
灭欧和那个三尺是对持一种存储服务
所有的那个存储
已经贴意是我们自己写的
都是自己写
是吧对对贴一是完全自己写的
其实这格式完全是自己自己定义的
完全可以用快存储
为什么一定要
就是如果你就是你想
你不想用对象存储的话
我们也现在提供那个单机版
就直接给您用最快的这个
这个磁盘也可以写
就是现在是
只是对象分布式是用这种架构
就单节点
因为我们可能没有讲
今天主要讲分布式单节点
我们是允许大家用NFS或者是这个
这个本地盘非常快的那个
Nnvme其实也可以哦
好好