大家好，今天很荣幸代表我们团队做这么一个分享

今天主要是分享一下我们HTAP数据库在OLAP方面的设计和实现。

先介绍一下我们公司，矩阵起源是一家数据库的创业公司，我们的产品叫MatrixOne。MatrixOne是一个云原生的HTAP数据库，使用go语言开发，每一行代码我们都是从头开始写的。MatrixOne可以同时支持OLTP、 OLAP、streaming等不同的工作负载。用户可以在公有云或者私有云，或者自建的比如HDFS上面存储自己的数据，可以无缝的在这上面部署和运行。

我今天的分享大概分为3个部分：

1. 第一部分是MatrixOne的整体的架构
2. 第二部分是MatrixOne里面OLAP引擎的设计
3. 第三部分是测试的结果

大概在一年前，我们对MatrixOne整体的架构做了比较大的调整。之所以做这样的调整，是因为旧有的架构有一些痛点无法解决。

我先介绍一下我们早期的架构。MatrixOne早期的架构是一个比较典型的share nothing架构，数据是存放在一个Multi Raft集群上面，数据的每一个切片存在一个Raft上面，不同的Raft Group之间的数据互相是完全没有重叠的。

这么一个架构大概分成这么三个部分：

1. 最下面存储层，在存储层我们之前是打算要做几个不同的存储引擎。一个是AOE，AOE只能支持AP查询，他没有事务的能力。AOE他只能是往后面追加的添加写数据，包括删除也不支持。之后为了支持TP查询，我们之前打算要新加一个存储引擎TAE，说它会带有完整的事务来。
2. 在存储层之上的是分布式框架。分布式框架是我们自己做的一个multi raft的一个架构，用来做存储节点的调度。
3. 再上面就是计算层，计算层是一个比较经典的MPP的执行的架构。

这早期架构存在有一些问题和痛点是我们一直无法解决：

1. 一个是扩展性，因为他是share nothing架构，每扩展一个节点，就需要同时扩展存算的资源，因为计算和存储并没有完全的分开。而且每扩展一个节点，需要大量的数据迁移工作。另外因为raft协议，每一份数据的都要保持 3个副本，从扩展阶段到完成的时间会非常的久，每扩展一个节点可能要搬迁大部分的数据。
2. 在性能上，我们知道Raft协议在写操作需要复制到所有节点上面去，但是读操作，对于每个Raft group，都只能在leader节点上面去做读取，这样容易造成leader节点成为一个热点，成为性能的瓶颈。
3. 因为我们之前是打算设计多种各种不同的引擎，然后这些引擎的性能也是非常不一样的，无法有效的应对HTAP的场景，比如说用户既需要有AP的查询，又需要做TP的查询，这种扩展性也是非常不好
4. 成本的问题，还是因为raft本身的问题，因为数据要保存三副本，实际上那个数据冗余是非常大的。用户数据是放在用户自己的私有云或者是HDFS上，随着节点规模上去，成本就不断的攀升。而且由于存算不分离，存储也是放在用户自己的机房里面，需要让用户使用非常好的SSD，才能很好的发挥数据库预期的性能。

这几个痛点，我们之前思考很久也没法解决。所以在大概一年前的时候推出了一个整体的新架构。

新的架构从整体上看，会分为三个部分：

1. 最上面是计算层，计算层里面每一个单位叫做cn，就是computation note计算节点。

2. 在计算层下面有数据节点datanode。
3. 再下面是file service。file service的话就是支持各种不同的文件系统。

下面再对每个部分分别详细的介绍一下：

- 先从最底下说起FIle service，FIle service会支持各种不同的文件系统，比如说你用户自己的本地的实盘
  也可以，NFS、HDFS都也好，或者是对象存储，比如现在各种公有云的对象存储，量大又非常便宜。file service对上层只提供一个统一的接口，对用户来说他并不需要关心最底下的那个文件系统数据本身是存在什么样的介质上面。这边还会有个log service，因为我们知道在file service插入数据的时候，我们肯定是只能一整块一整块的往往下面写，特别像S3它一个object就是非常大，我们写数据时候不可能是一行一行都往那个file service里面写。我们一般是会积累到一定的量再往里面写一个整块，那么这些还没形成整块的部分的话，会放在一个单独log service里面去存放，log service的它还是一个Multi  Raft的集群。就是已经形成整块的部分数的据完整性和一致性的话是用S3或者HDFS他们自己的功能来保证。然后还没有形成单个block的数据的一致性完整性是通过Multi Raft group来保证。

- 再上面的话是存储节点，存储节点里只存放元数据信息，比如说每一个表分成大的叫segment，每个segment会分成很多小的block。比如用S3来存放数据话，元数据里会存放每个block存放具体对应的具体S3 object。
  还有其他元数据信息比如说row map，或者是次级索引bloom filter这些信息也会存在dn上，这里可以看到有多个dn，这多个DN它是怎么分布呢？比如说有一张表它有一个主键，那么我们可以根据主键来做这么一个分区，我们目前是暂时只说了按hash来做这么一个分区，也可以按range来做都是没有问题的。那如果用户的数据量很小的话，对大多说小客户来说存放元数据其实只需要一个DN就足够了。

- 再上面的话就是cn节点，cn节点就是具体的执行计算任务的节点。CN实际上是可以分成各种不同类型，
  比如说专门做TP查询用的，专门做AP查询用的计算节点，还有专门做streaming用的，还有一些对用户不可见的数据库自己后台的一些任务。
- 还有一个组件的话是叫HA keeper，跟zookeeper功能类似，在节点之间互相通知上线下线这些信息，维护整个集群的可用信息。

这就是MatrixOne在大概一年前迁移到的一个新的整体架构。

我们从之前的share nothing架构，迁移到share storage架构的优势包括：

1. DN节点的话它不保存任何的数据，它只保存元数据信息。这样的话就不会让单个DN成为一个瓶颈，如果你要做弹性扩缩容的话，比如说DN要新增一个节点或者是要删除一个节点。做数据迁移的话只需要交换一些元数据的信息，不需要把所有数据都做搬迁的操作，这大大简化并加快扩缩容的效率。
2. 我们之前的架构，对于数据的一致性，完全是通过Raft协议即来三副本的架构来保证。现在的话我们的数据的完整一致性主要是通过S3来保证，S3它本身就具备这个功能，并且从成本上来看的话也是非常的低，实际上比用户自己去搭建个Multi Raft集群的成本会低很多。
3. 然后计算的任务现在也可以很好的把它给分离开，比如说TP和AP的计算，可以放在不同的CN节点上去做。
   因为现在数据的话已经不跟DN绑定在一起，所以计算的节点也可以完全的解耦。

我们看一下一个典型的查询里面的读数据的操作是怎么样的。数据查询从一个CN节点开始，他会首先去访问DN的信息，读取元信息，判断某张表在哪些block，甚至会先通过过滤条件用row map来做过滤，剩下的实际上需要去读的那些block信息，拿到之后直接去那个file service下面去读取数据

在这里DN节点实际只提供了元数据信息，基本上不会成为一个性能瓶颈。因为更多的数据是CN节点去直接向file service读取的，CN节点上面还会维护一个metadata cache。假设这个数据新鲜度还没有过期的话，可能根本就不需要访问DN，直接去file service下面拉取数据。

上面就是MatrixOne现在更新后一个整体架构。

先介绍一下现在的存储引擎，TAE，T和A分别是指TP和AP，T和A表示它既有事务的能力也可以很好的处理分析性查询，用同一套引擎同时支持AP和TP。

那么它的结构是什么样呢？我们实际上还是可以把它看成是基于列存，首先不同的列之间，它数据仍然是单独分开存放，然后每一列的话我们会按照8192号给它分成一个小的block。为什么选择8192行的话，因为大多数的数字类型的一个block可以在L1 cache里面直接装下，在后面的批量计算的时候会对计算引擎比较友好
然后有很多个block会组成一个segment，这个segment它会有什么作用呢？
就是假设我们这个表它有主键或者排序键的情况下，一个segment内部它是会通过排序键和主键去做排序，这样数据存储的话在每一个segment内部是保持有序的，但segment之间的话那可能是会有重叠。这跟LSM的存储会有一些相似的地方。在不同的segment之间数据是有重叠的。

但如果我们之后做了partition功能之后的话，可能会把一个partition的所有的segment也会去做这么一个compaction操作。就是说把它们重新拿出来，做一个归变排序再放进去。
我们现在存储架构的话就是大概就是这么一个样子。


下面会介绍一下MatrixOne的OLAP的计算引擎

我们的计算引擎分为四个部分：
1. 第一个是parser：把一个sql语句解析成一个AST树。
2. 第二个是planner： 把AST树转化成一个逻辑计划。
3. 第三个是optimizer： 把逻辑计划通过各种优化器规则或者是通过一些基于代价估算的方式把它转化成更好的那个逻辑计划。
4. 最后是execution：把具体的逻辑计划转化成可执行的pipeline，然后去具体CPU上面去执行。

我先介绍下我们的parser，parser的话对于各大开源数据库大多数来说都不会去手写一个parser。至少是用mysql或者PG的parser。比如duckDB就是直接使用postgres的parser代码。
即使我们不直接照搬运，也可以用一些YACC的工具去生成这么一个parser，在我们测试之后发现用YACC生成的一个parser的话，他并不成为一个性能瓶颈，因为他耗时非常少，所以我们没有必要去手写一个parser。但是像clickhouse的parser是手写的。
parser生成AST树之后的话，就会通过逻辑计划器，把AST树转换成一个可以执行的逻辑计划。

逻辑计划器主要是包含两个部分：

- 一个是Bind。如果我们的计算引擎想要支持子查询，因为我们并不支持像SQL Server一样将子查询转换成apply join，或者像mysql一样完全从父查询里面拿出一行，然后再带入子查询里面，把子查询完完整的执行一次，这样的子查询是完整不停的的执行。我们考虑到在AP查询的场景，这样的一个执行计划是不可忍受的。就干脆完全不支持这种apply jon的方式，所以我们在planner这一步，就把子查询的消除给做掉。

然后是优化器的话，通常来说会有一个RBO基于规则的优化，基于规则的话在对大部分查询来说已经是够用。因为优化器通常来说分为两种，一种是减少数据IO的，它确实会实际的减少从磁盘从文件系统读取的数据量。
还有一种是在计算的过程中减少计算的代价，那么减少磁盘IO的这种的话通常来说就是RBO。对于CPU来说，一般是减少这个实际计算的代价。

后面我也会具体举一些例子来说明来说明我们这个MatrixOne是怎样设计这一部分。

那之后的话是从逻辑计划到实际可执行的PIPELINE这一部分，执行器的好坏对OLAP系统的执行效率影响是非常大，后面会详细介绍一下。

先介绍执行器的部分，众所周知，执行器有一个经典的火山模型。对于每一行它是一个典型的pull模型，从最上层的那个计算的那个operator开始，每次去调用一个NEXT函数去从下面的节点去拿一行新的数据出来，做完计算之后，再等待更上层的那个计算节点去调用next从它这里取走。

那么这火山模型它还会有什么问题呢？首先它是并没有做批量处理并没有做并行化。它是一行一行的处理，而且每一行的不同层级之间做这么一个调用的话，实际上会产生虚函数的开销。因为next在不同编程语言肯定是主要是这是要做这种函数重载，就算是虚函数开销也很有可能是比你实际计算的是开销还大，即使不大的话也是会占相当的比例。

而且这个对于缓存来也是非常不友好，因为他是一行数据他会跑多个不同的operation，但是可能取下一行的时候，原来的缓存已经被清洗掉了。
MatrixOne的执行器是基于一个push模型，基于push模型的话它可能某几个连续的operator可以组成一个流水线。组成流水线的话就是说，而且这个流水线里面流动的数据，它并不是一行一行的数据。而是我们刚才说到那个TAE存储引擎里面的一个block，一个block的话就是8,192行，对一般的数字类型是可以直接放进L1 Cache里面
对缓存是非常友好的。

就是每一个operator他每一次要处理完这8192行才会喂给下一个operator，再加上调度的话是，从最下面的那个实际读取每一张表的那个table scan 那个节点开始，从那个节点开始往上面push。

对于Push模型，它是以数据为中心而不是以operator为中心，他的生成过程是对我们上一步那个planner和optimizer生成的逻辑计划，作一个后续遍历，后续遍历之后就可以得到一个基础的pipeline结构，这个基础的pipeline结构的还没有带上比如每台机器有多少个CPU和这样的信息或者说你要需要在多少个cn节点上去执行的信息。在后面实际执行的时候，再动态的根据这些基础信息去做这么一个扩展。

举一个简单例子，假设有一个简单的查询，有R S T三张表做join，假设R表是最大的一个表，然后S表和T表相对比较小，并且每个表都有过滤条件，这样的一个典型的hash join。我们会把S和T这两张表去构建hash表，然后R表在这两个hash表上面去依次去做探测（Probe）操作，得到join之后的数据。

这么一个逻辑计划至少需要插上这么三个pipeline，S这张表的数据读取，做完过滤之后再建完hash表就在这里终止，T表也是在建完hash表之后就在join算子上面终结掉。但是最大的表R表它始终是要做probe的，这张表的pipeline就可以往上走很多步。比如说他先做完滤然再跟S表join之后，他输出的仍然是一个批量。然后这个批会继续往下走，在下一个join的话，仍然在同一个pipeline的下一个operator里面，再跟T表做一个join。所以这么一个3表join通常会拆成这么3个pipeline出来。

右图还包括了数据并行的信息，比如说S表可能会使用go语言里的三个协程并行的去读数据。然后再做合并操作，合并之后构建hash表，T表也是会用三个协程去并行的读，读完之后，然后送到这里的构建hash表。
R表因为比较大，Pipeline会展开出更多的实际的pipeline出来。我们可以看到就是R表这个pipeline是不会被被阻断的，他通过hash operator之后，会继续进到下一个join节点。

如果pipeline中间出现一些异常的情况。比如说S表读数据发生错误，出现了异常。那么怎么样能够正常的把整个pipeline计算任务一起给结束掉。我们使用了go语言里自带的类似context的一个的数据结构。R表的pipeline在join地方会从一个channel里面等待hash表的数据，他在等hash数据的同时也会去监听另一个context，这个context是和S表的pipeline共享。如果S表发生异常退出，这个context也会cancel掉，然后R表的pipeline也会接收到这个信息，知道他也可以退出，就可以提前退出。同理，当R表退出之后的话，他同时也会把这个退出的信息通过另外一个context传递到T表上面，这样T表也就不用再读，整个计算任务就可以直接退出。

我们的pipeline做了这些算子，比较典型的有聚合，分组和各种join操作。然后是像构建hash表他这边有很多merge，merge和connector dispatch的话这个把它颜色标识的不一样，它们和其他operator的区别是其他的算子都是只能在一个pipeline的中间，接受的数据是从上一个算子传过来，他发送的数据的话就直接会发送给下一个算子去做后续的计算。
那么这个标成灰色这一部分的话他是在这么一个pipeline的数据的source或者sink就是入口或者出口这个地方
像merge的话它会去其他的pipeline去接收数据，就是说把不同pipeline的数据合并成一个。
比如说我们最终输出接口的那个Pipeline的它一定会有个MERGE操作。
就是会把所有的那个Pipeline计算的结果给合并成一个，返回给用户。像group by或者order by算子，也会执行merge操作。
然后发送这边的话也会有两类：

1. 一个connector的话就是一对一的发送；
2. 一个dispatch算子的话是，是会是一对多的发送。dispatch的话会有很多不同的模式，一种是广播的模式比如说比如说像那个hash表构建这里会有一个广播的模式。就是说假设S表是一个很小的表，他构建完hash表之后，会把hash表广播到不同的pipeline出来去做计算；那有一种dispatch是做shuffle，假如S表和R表都比较大的话，因为要做shuffle join，那么直接会通过一个shuffle dispatch算子把数据给发送到那个不同的对应的一个pipeline上面去。

对于OLAP系统，从语义上来说通常是跟sql本身是没什么关系。但是我们现在的OLAP系统的话，它必须要具备sql查询的能力。

OLAP的话就说他分析性查询一般来说是比较复杂的这种这计算任务
就说你有一些sql能力是必须具备的，比如说像多表join这样子查询像窗口函数，还有CTE 和Recursive CTE，或者是用户自定义函数这些不同的sql能力，然后目前的话MatrixOne已经都具备这些能力

下面会简单介绍一下MatrixOne的优化器规则。

优化器规则一般来说是分成这么两个部分：

- 第一部分是减少IO的：
  1. 减少IO的话就是有列裁剪，列裁剪的话就是说，我实际读一张表，比如说他这个表有很多列，但我实际上数据我只用到其中一列，其他的列是不用读取的。这是一个很好理解的规则，
  2. 还有谓词下推，就是说下推的话比如我们会把一些过滤条件给直接推到读取数据这一部分，这样可以尽量少的读取数据
  3. 还有一个叫做谓词推断，谓词推断主要会影响这个TPC-H里面的Q7和Q19，这在后面会在举例详细介绍一下。
  4. 然后一个是runtime filter ，runtime filter就是说在刚才做join的操作，比如大表和小表做join，但是小表构建完hash表之后，可能他的hash表的计数非常小，这样的话我们可以直接通过hash表里面不同的词去大表里面通过runtime或者说元数据信息去进行过滤，这样的话在运行时就把要需要读取的大表的block数量给大大的减小，这也是减少IO的一个优化规则。

- 第二部分是减少计算的。即使他并不能减少实际从磁盘里面读取的数据，但是他会在计算的过程中大大的减少计算量，或者减少中间结果的一个数据量：
  1. 其中一个就是说join order的一个join定序
     那join定序的话就说在就，通常我们做那个OLAP 的benchmark的话，用那个TPCH的话，它会影响很多的不同的查询，如果join order做的不好的话，这些查询都可能会以数量级的变慢。
  2. 还有一个就是说一个聚合函数下推和上拉的操作。假设我们聚合函数在一个join的上面
     我们先做一个join再再去做聚合，然后再在join这里的话，可能数据会膨胀的非常多。
     但是如果可以假设把聚合函数推到join下面去做，就是说在做join之前的话那个数据就已经减少很多的话，这也是可以大大的减少这个计算。

简单介绍谓词推断，谓词推断就是说可能我们之前说谓词下推的话，那个是已经显式的可以下推的一个位置。但谓词推断呢可能是用户需要去做一些逻辑上的变化之后，才能得到一些新的谓词，这些新的谓词也可以下推下去。

比如说TPC Q19的那个过滤条件是三个很长的谓词用or来连接起来，通过观察实际上这三个or里面其实有很多共同的部分，咱们可以把共同的部分提取出来，变成右图的样子。
可以观察到，首先part这张表上面有一个可以下推的谓词，lineitem这张表上面也有两个可以下推的谓词。这样的话这两个谓词可以先下推到每个表的table scan上面去。
然后还多出来一个谓词，part和lineitem上面那个用主键做连接的一个谓词。
如果原来这个执行计划你不做优化的话直接去执行，他可能会先做一个笛卡尔积，再去做这么过滤这个，这个效率非常低的。
那现在的话，我们可以把变成一个join操作

再简单介绍一下我们MatrixOne使用的join order的算法。在各大开源数据库上，join order在算法实现包括贪心法和动态规划。其中动态规划有很多有几种不同方法，也有很多论文可以参考。但是动态规划他有一个问题是当表的数量稍微多一点的话，这个状态搜索空间就会以指数的形式膨胀急剧的膨胀。比如StarRocks的文档里面提到10个表以上的话就没办法使用动态规划来算，就只能使用贪心法来算。

我们在MatrixOne里对这个问题做了思考，在大于10张表时，可以先用贪心的方法来做一个剪枝操作，让搜索空间大大的减小，在贪心法之后再做动态规划。

贪心法大概会分这么三步：

1. 第一步就是说确定事实表和维度表
   因为一般来说OLAP查询的数据通常会把表分成事实表和维度表
   无论是新型的那个框架还是雪花框架，他都是会分事实表和维度表。
   那事实表维度表之间的话使用维度表的主键去做join

2. 拿到一个查询之后，我们可以把事实表和维度表给找出来，下一步的话是说事实表先有维度表先join成子树
   因为事实表维度表之间始终是通过维度表的主键去做join
   所以说做这样的join的话他的
   他的结果的函数始终是不大于他本来输入的函数
   所以做这么一个join的话他的
   也就是并不会造成很多OLAP开销，不会造成这种数据膨胀的这么一个操作
   所以说实话，这样的join先做是没有问题的。在做这个事实表维度表join的过程中的话，我们会先考虑事实表会先与过滤性好的维度表做join。就优化器的话，越早的减少这个数据量是越好
   所以说先会以过滤性好的维度表先做join

3. 那最后一步的是在子树之间，像TPC DS的表，会有多个维度表，这维度表互相之间都不是以主键来做join。
   那么在子树之间，我们再使用经典的join order的算法，比如说像动态规划这些。

这样的话我们把这个Join order的算法就是说假设之前是你只能10表以下做这个动态规划，我们现在可以把它扩展到10个事实表之下都可以用动态规划来做

举个例子，TPS Q5的会有customer、 orders、lineitem、supplier、nation、region这么六张表，它是一个比较典型的一个星型的结构。

然后这个orders和region这两个表是标红的，因为它上面是有过滤条件，可以需要单独考虑。
然后的话是把他本身的join条件
包括给他标了一些线
像这有箭头的
这个实现的话就是说从事实表到维度表
他用维度表的主键做join，这么一个条件
大家可以看到，一共有5个条件都是用主键来做join
那么其中还有比较特殊的一个条件
用画的虚线就是他两边互相是都不是用主键来做的join。
嗯那么这我们这个join算法下一步的话
就是还会有一个联合优化，就是说最后就可以跟谓词推断联合起来使用的话会有一个新的优化。
因为我们可以看到supplier和customer这张表他会有一个join条件
然后supplier跟nation他也有一个join条件
用的都是同一个类型T来做在决定

我们可以推探出来一个nation和customer之间以类型key做join的一个条件
因为我用黄色的选项表示
那么最后实际
生成的最优的join顺序的话

实际上是从region先跟nation join,再跟customer join，再跟orders join，再和lineitem join，最后跟supplier join。这是我们新推断出来的条件。

那么为什么是走这么一个路线，而不是说先把上面这一条路join完成后再join下面这边的表
因为我们考虑到orders，region这两张表他都有过滤条件
那么两个都有过滤条件的表的话，他放在一起的过滤的效果会就会更加的好
这样会让lineitem这张表他的行数减少的那个速率更加的快。



MatrixOne从去年开始整个架构重构一直到现在差不多接近一年的时间。
可以看给大家看一下TPC-H 100G的性能测试的结果。
我们用的是亚马逊EC2上面的机器做的测试环境，用的是比较一般的机器。
一共用了四台机器，每个机器16核和64G的内存啊
有三个计算节点CN，一个数据节点DN。

大家可以看一下这边的这个benchmark结果
前两列的话是在Snowflake上面做测试的
就是Snowflake的一个呃
就价格跟我们这个配置差不多的一个配置

就是跑第一遍的那个时间和跑第二遍的时间

那后面是MatrixOne的跑第一遍和跑第二遍的时间
可以看到我们现在差不多能达到snowflake 60%的性能。
为什么要跟snowflake比呢，是因为snowflake和我们的架构是差不多的，都是云原生，数据完全存算分离。
然后数据的话是存储远端，不会跟实际的计算节点存在一起

这个OLAP数据库，大概做了这么一年时间吧
现在这个结果的话我觉得应该还是很不错，性能上达到了snowflake的60%的水平。


