# 数据湖查询引擎架构和实践

导读：今天简单的分享金山云在数据湖查询引擎上的实践，以及实践当中遇到的问题和相应的解决方案。

总体分四个部分来分享：

1. 查询引擎架构简单介绍
2. 针对查询引擎做的功能增加和安全方面增强的实践
3. 各种查询引擎带来查询结构的差异，分享一些具体案例
4. 针对查询性能提升方面的探索



## 01 数据湖查询引擎架构

查询引擎架构：根据客户具体使用场景，集成了Spark和Trino两种查询引擎。底层对接了两种数据湖和各种数据仓库，另外还集成了市面上常用的数据库，底层存储方面有常用的hdfs，以及市面上常用COS/S3等对象存储，包括hive和关系型数据库，中间使用alluxio做数据缓存来提升查询引擎效率。

如果只是简单的开源组件的集成并不能体现出整个查询引擎的优势。

因此我们还做了很多集成上的事情，在最顶上提供sql gateway。数据服务API类似的高速查询， 做ETL工作，或者数据分析场景，有的SQL运行时间很短，有的运行时间很长。这终场景下，对外统一提供查询引擎，对内根据各SQL特性选择更适合的引擎来提升查询任务。在此基础上，Gateway一方面做数据分析任务，另一方面做负载均衡相关的事情。从而整个查询引擎对外表现出来是整个无感透明的状态。 

上面提到整个架构是支持了flink和datax批量写入数据湖，

同时支持Spark和Trino两种查询引擎

也支持两种湖（hudi/iceberg）的数据结构，同时接入alluxio做数据缓存，提升热点数据查询效率。

同时对接入的数据源，还接入普通的数据源如JDBC，es，mongo的数据源；

集群运行在两种模式下，yarn环境和k8s环境。

## 02 查询引擎功能增强

如果只是简单的把这些模块简单的堆砌，是无法满足用户场景，对此我们对开源组件做了改进，是查询引擎功能有所增强

#### 1. 动态加载catalog

当前社区的trino版本是无法支持动态加载catalog

集群需要重启，实现动态加载。增加了部署和维护的成本，因此我们在开源的trino版本实现

用户侧通过http的请求来动态添加/删除catalog请求，trino commit 分发到各个worker节点，worker节点会实时响应请求，同时catalog信息存入数据库或者配置文件中。

当前开源的版本也在动态解决catalog的问题，开源版本还没有正式公开出来，只支持稳健的动态加载，不支持

我们是都加强，都可以存在， 以此实现动态源的加载



#### 2. 用户自定义函数方面

Trino已经支持用户自定义UDF功能，但是如果直接开发功能给用户使用，代码规范 代码是否存在内存泄漏等各种问题，如果这些问题没有被提前暴露出来，直接提交到trino集群内，在运行时由于某个UDF不符合规划，代码不理想，会造成整个集群性能下降，甚至集群奔溃。 我们采取把执行UDF功能从trino的架构中抽离出来，把执行的部署在一个或者多个的远程服务当中，当用户注册UDF，在trino侧只是注册了用户的路径和参数，具体执行是通过高速的RPC协议来实现高速通讯，在远端执行具体UDF计算的功能。 通过这样的架构，可以使UDF计算资源和trino集群自己的资源之间做很好的隔离，当用户提交的UDF存在某种缺陷时，可能只是这一次UDF的调用，或者UDF server集群内一个节点产生问题，其他集群不会有问题。这样可以很好的避免一个UDF问题造成整个trino集群资源的问题。

另外兼容spark UDF和hive UDF，这样通过一套机制同时执行Trino的UDF和Spark的UDF。

#### 3.多维度确保服务稳定

前面说的都是运行时的确保，但是在实际情况中， 当然可以在事后会看执行计划来判断是否有改善空间

- 事前检测

  如果我们能够提供一个对于SQL一个静态解析的方法，就可以在SQL提交之前，动态的检查SQL是否是一个满足规则的SQL，这样可以在另一个维度保证服务的稳定性，提供稳定的响应。  这是通过SQL的静态解析来实现， 解析过程中需要使用元数据，元数据除了字段列的信息定义，也包括统计信息（如limit条数，唯一值，最大值等等）。以及性能提示都有关。 语法只能提示包括是否会查询很多条数据，是否使用正确的分区字段， 开发了SQL预判的模型，另外对可能耗费大量资源的查询，比如大表join等比较危险和耗时的资源的SQL提示；

  另外支持SQL审计

通过事前检测，绝大多数SQL都可以提供解释给用户，用来判断是否需要的SQL做一些优化。 在SQL执行过程中

- 事中检测：（包括CPU，内存，扫描的数据）

  在执行中发现CPU内存达到集群上限阀值，是否全表扫描。提供是否终止SQL的方案，对最终SQL的优化做一些提示

- 资源监控

  对于集群的监控是达到集群每个节点的具体状态，包括CPU使用效率，内存，网络传输数据流带宽等指标做监控

通过这三点手段在SQL执行的开始和中间来解决SQL性能安全等问题。

#### 4. 数据访问权限控制

最后数数据安全相关的方向。对查询结果做一些敏感的脱敏

通过应用层面和物理层面两个维度来解决，这样两层限制更好的叠加在一起，做更好的保护：

1. 在应用层面限制：通过SQL的静态解析，从SQL当中提取出用户需要的源库，出现在查询结果，还是出现在filter， join 查询条件的位置， 通过在SQL中间出现位置的不同， 可以在业务层面限定，判断是否有查询的权限和是否需要做脱敏
2. 在物理层面限制：主要结合Trino，以及Trino提供的相应插件的能力，实现了底层的权限鉴权。另外对于Trino的场景，我们hook JDBC获取元数据的方面，在元数据层面就可以跟用户权限挂钩。在元数据获取侧看不见，也能防止权限的泄漏。

## 03 不同查询引擎查询结果的差异

我们在具体应用方面因为集成了Spark/Trino两种查询引擎，来满足用户需求，但是同时也带来一个很严重的问题。即不同的查询引擎对于相同的数据，包括一些特别的字段类型的数据，查询结果会有些差异。 这样会给用户带来一些困扰，比如去查同一张数据表，由于SQl写法不同，会造成查询结果或多或少的差异。 对于这样情况，简单分享一下我们遇到的相关的问题：

#### Trino读取不到Hudi mor表的数据问题

- 问题：对Hudi的MOR表先做了insert的动作后，然后对其中一些数据做update，但是最终更新后数据在spark可以读到，但是在trino无法读到更新后的数据，只能读到一开始insert后的那部分数据。
- 原因：因为Hudi自己的特性有ro和rt两种表，目前trino的connector还不能够读取rt表中的日志文件，从而也不能够读到更新后的数据， 相当于trino当前比较好的是读取ro的数据，如果我们需要读取Hudi更新后的数据，就需要  执行compaction的动作，让日志数据落到盘上， 或者换成Spark引擎来读取
- 解决：在架构上我们提供了无数据的兼容，识别表最近几次SQL执行的类型（insert or update），通过动态切换查询的计算引擎使用哪个来避免这些问题。

#### Trino读Hudi的MOR原表问题：

- 问题：对于MOR的分区表，内部是存在3种表的数据结构（源表，ro表，rt表）。我们发现对于分区表，Spark创建的表，但是Trino无法读到Spark创建的原表的信息的。
- 原因：Hudi源表的分区信息没有同步到hive的metastore里，Trino是通过hive的HMS来获取元数据，没有获取到元数据就无法读取分区信息
- 解决：正确的使用ro/rt表，同时我们发现了一个很有趣的问题，如果我们在建表的时候在原始表的后面带上\_ro或者\_rt，    建议大家尽量不要用到ro/rt的结尾

#### Hudi的Timestamp类型读取问题

- 问题：在实践中发现，对于带时区的数据类型，如Timestamp，Trino读取的结果和Spark或者Flink的结果会不一样，比如北京时间下执行结果就会少8小时。
- 原因：Trino源码在做市区适配中，没有做市区转化
- 解决：在Trino源码上做适配，对于带时区的时间类型，读取

#### Iceberg表时间精度和现实问题：（是个很细的点，看上图）

- 背景：时间精度也是不一样的，
- 问题：Trino查询iceberg的timestamp with timezone类型时，结果会达到纳秒单位
- 原因：
- 解决：在

#### Hudi驱动包版本问题： （我们发现的一个有趣的现象）

 当我们开启kerberos认证的情况下，Hudi不同版本表现也是不一样

- 问题1:  在Hudi的0.10.2版本，开启kerberos认证下
- 问题2:
- 原因：Hudi故官方包

## 04 查询性能提升探索

上面是我们在具体应用中发现的一些细节的问题，最后分享我们在查询性能提升上的探索

#### 数仓-关系数据库优化

在上面架构图中有提到我们应用alluxio缓存作为查询性能提升的方法

在源码角度，提出提升性能的解决方案

- 一方面查询关系型数据库，如果关系型数据库的统计信息很完整的话，使用数据库的统计信息来优化数据库的执行效率。用tpcds的SQL来做测试，发现有元数据信息后，性能提升在19%左右。

- 另一方面，在Oracle数据库中，性能提升能达到50%， 可见元数据的统计信息对SQL查询有相当大的帮助。

这也带来了一个问题，当前统计信息不全，除非我们对表主动做analyze处理之后， 它统计信息才会被收集   。 我们知道analyze是相当耗费资源的，当我们表的数据千万级以上或者上亿，执行analyze对于底层资源占用非常高，会影响使用，资源占用尽的情况，执行引擎无法提供服务

对此我们探索了如何通过底层HDFS获取统计信息的方法

- 根据HDFS inotify监听写入和变更Event（过滤、合并Event)。 很快定位到对应数据文件

- 对于Hive/Hudi表， 包括CBO/RBO的优化

通过这个方法，可以做到周期性定时统计HDFS底层库的信息，包括计算出来小文件的数量，再通过小文件数量做阀值的报警，规则的定义，当小文件数量超过一定数量后，定时做小文件的合并。提升HDFS本身的效率。 通过监听数据文件的变更。 通过定时文件合并和监控两则来完善统计信息。

#### z-order支持：（功能还在探索过程中）

最后还在探索中的方案：如果用户数据在存入底层文件时候，如果数据时按某种条件排序，在查询侧天生的支持返回的结果，通过实现数据写入后，自动的对数据进行z-order排序的功能，通过这样的手段，把落到盘上的数据文件的顺序做了优化，优化之后，运用到查询过程中。 当我们进行随机写入，orderBy的写入，通过z-order写入，相同的SQL查询性能提升的效率很明显。



## Q&A

Q: UDF server能否支持横向扩展？

A: 我们现在设计的方案UDF server跟Trino集群是完全剥离开的，是支持横向扩展甚至是支持云上部署的。只要网络是通的，因为这里考虑到Trino是个高速的计算集群，每秒上万次或者几百万次的请求。网络协议中， 横向扩展支持住网络请求。  不仅支持横向扩展，用户还可以自己定义分类，更好的支持资源

Q: Trino能否在真正执行SQL查询前，预估出代价？

A: 这不是Trino本身的功能的，Trino计算的代价是需要元数据统计信息来制定执行计划。代码，SQL还没提交到Trino之前，通过静态解析工具来实现SQL计算代价的预估，也会引入到元数据统计信息，以及经验累积是事前规定。这里需要对历史执行计划的保存和汇总，还可以通过历史上对相同表执行SQL的结果以及当前执行计划的汇总，通过对历史结果的事后分析，创建模型动态揣测，所带来的执行上的问题，做一些模型上的数据，对以后的执行检查做数据结果的支撑

Q：SQL的gateway是基于什么条件来做方法选择，做执行引擎的选择

A：客户写的SQL很可能是用一种特定的查询引擎，这里有两种情况：

	1. 特定SQL， 比如hiveSQL，sparkSQL，flinkSQL，都有特定的SQL定义文件，通过这个文件来直白的分辨使用引擎。
 	2. 通用的SQL，类似数据服务的高速查询查询，还是想执行ETL和加工，通过这样的解析和规则命中，来选去具体的查询引擎。

