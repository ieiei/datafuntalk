![lakesoul-1](./images/lakesoul/1.jpg)

背景：这次分享的主题是数元灵开源一年多的数据湖产品Lakesoul。数元灵同时还开源了另外一个产品，一站式机器学习平台MetaSpore，在上面可以做很多搜索，给大家在业务上带来一些非常方便的体验。

Lakesoul在我们公司主要的定位是我们希望构建成一个端到端的开源实时湖仓的框架。

![lakesoul-2](./images/lakesoul/2.jpg)

该框架如何实现端到端的实时湖仓呢？
这里涉及到一些设计理念的问题，Lakesoul是作为现有国产唯一的开源框架完全是由我们自研的，并不是基于其他的框架之上开发的。从底层设计到上层应用，包括一个生态系统的建设，整个链路都是我们自己实现。
今天在这里详细分析一下我们是怎么设计的、为什么这么做、做好之后会带来什么好处、
有什么样的应用价值以及未来怎么做。本次分享的目录：
- LakeSoul 设计理念解析
- LakeSoul 技术解析
- LakeSoul Benchmarks
- LakeSoul 应用案例
- LakeSoul Future Roadmaps

![lakesoul-3](./images/lakesoul/3.jpg)

因为湖仓一体框架概念比较新，大家不太了解，可能会思考为什么要使用湖仓一体的框架，所以在正式讲解技术主题之前，由我先介绍一下湖仓一体的技术背景。

提起湖仓一体的技术背景就离不开云原生，云原生给我们带来很多便捷的体验，比如随时拉一个镜像，搭建几台集群，基本上通过一件事或者几行命令就能把一个集群或者一个应用部署上去，这极大减少了使用和维护成本。并且云原生在底层，都是以对象存储为基础的，对象存储带来的最大的好处就是特别廉价，同时它的性能也不差，存储数据的格式多样化，包括结构化、非结构化、还有半结构化等等。

对于数据分析来说，这样的一个廉价的存储就能存储更多的数据，也会带来更多意想不到的收获。首先是带来数据链路模式的转变，有一段时间特别兴起的一个名词：”数据入湖“，同时相应的出现了一个新的工种： “入湖工程师”。针对数据，先入湖，而不是像传统模式，数据结合使用方先做处理。我们归结这种处理方式的转变是从传统的ETL转成到现在的ELT。

ELT的含义可以理解为这样的一种模式：

- E: 是从源头拉数据过来
- L: 是先加载在我我这个数据湖仓里面，就是先加载数据然后再做处理
- T: transform，就是说再做处理，比如做count，或者做字段这抽取，或者做其他处理等

传统ETL模式是：

- E: 新数据抽取不变
- T: 做转换，还是做count的distinct
- L:转换之后的load，在传统框架里面具有很多不同的这种实践方式，比如会分流和批：
  - 对于批的数据，一般会用spark去做处理，处理T+1的数据，处理完之后存在数仓里面，典型的就是Hive。
  - 对于流数据，一般就用flink处理，也没法直接存在hive里，需要找一个更强劲的数仓引擎，典型的比如clickhouse或者Doris。
  - 如果还需要日志型的数据话，就需要存在Elasticsearch引擎里面.

传统的ETL这里面会造成了一些困扰：

1. 链路多样：包括一个流链路和一个批链路，导致流批不一致

2. 存储不一致：比如离线数据存储T+1的数据，日志数据存入elasticsearch，实时数据存入clickhouse。这会导致整体成本都很高：
   a. 维护成本高：可能需要一个团队或者几个团队来维护这一套产品。对于数据分析师，如果对某个表提出了一个新的处理方式，比如新增几列或者修改几列，需要及时同步数据工程师对相应的任务进行调整，对任务进行不断的修正。在传统模式下，先把T任务按照数据分析师的要求重新转换，再把T+1的数据重新从源端拉取回来。但是源端的数据可能存在保质期（比如7天或者一个月），如果需要拉取更久的时间，数据可能就存在丢失，数据的价值也就体现不出来。
   
   b. 开发成本高：对于上层的AI/BI开发者，也造成了一定的开发困扰。比如说对于AI开发者希望用的数据越广泛越好，包括存在hive的离线数据和存在clickhouse里面的在线的数据，还包括存在elasticsearch的日志数据等。尤其是类似最近比较火的chatGPT这种大模型，需要大数据量才能完成神经网络的模型训练。大数据量意味着数据多特征的特性，对于AI工程师开发就比较痛苦，既要集成Hive，开发Hive的接口，又要开发集成clickhouse的接口，还要集成elasticsearch的接口。存储的不统一，导致开发成本巨高，也造成了AI和BI工程师之间开发不统一的一个现象。

所以说传统的模式存在现象比较多的问题，因为历史原因，它也是发展必经的一个过程，但是现在是云原生时代，我们希望实现一个大杂烩。

![lakesoul-4](./images/lakesoul/4.jpg)

数元灵的设计理念是认为现在的ELT模式是一个通用的比较流行的模式，即数据先入湖仓，再在上面做处理，并且处理的链路要统一，不能像以前那种spark和flink两条链路。

只有湖仓数据已经统一了，在上面做计算才能更方便。但流批统一可能还有很多其他的一些实现细节，这在后面会有提到。另外流批统一之后，自然而然的AI/BI，都可以在上面做计算，也做到了统一。

结合上面的背景总结，我们对lakesoul的定位是在云原生之上存储海量数据的数据湖技术，首先云原生是我们一个很重要的特性，提供海量数据的数据湖技术。我们提供输入仓库，这和数据仓库不一样。数据仓库是有自己独特的计算引擎和存储引擎。

Lakesoul的使用模式是不一样：

- 第一点：直接在数据湖上面去勾选数据仓库
- 第二点：在ELT模式下需要将所有数据进入到这个湖仓框架里面，那必须要提供一个端到端的这种数据转入能力，而且实时和批量的数据转入能力都需要支持，不同的异构数据都能进入到湖仓里面，而且必须是简单易用。因为之前大家可能已经用flink写了很多任务，如果要做额外转化的话，开发成本和入门成本都会很高。现在要提到端到端的话，基本上用户不需要很多东西，直接配置连接信息，剩下的数据都要进入到这个湖仓里面，不需要去做很多操作。在这之上，我们提供了很重要的流批一体的能力。这种流批一体最重要的点在于并发写的时候，要提供一个高吞吐的能力，可能会在很多模式都需要高吞吐，比如upsert模式，还有copy on write模式是都需要提供。另外在不断的写的时候，还能提供timestamp的能力，这在后面会再详细展开。
最后所有能力都统一之后，我的AI和bi框架都能用这个数据去做进一步的分析，lakesoul不是说简单的只支持sql，同时也支持如Pandas这类AI的引擎。

![lakesoul-5](./images/lakesoul/5.jpg)

从整体架构上来看，我们是做了一些分层的处理。
上层是一个元数据的分布式的服务层，这个元数据服务层是做一个schema的这种管理啊，同时提供acid的并发控制。不同于以前那种分裂的存储，只有hive有元数据信息，而clickhouse里没有，也没有一个总体的生命周期。我们的元数据层对数据能够提供上帝的视图，就是在这一个服务层能够将数据湖里面所有的文件都展示出来，AI和BI分析人员都能够从中获取文件的所有信息。
在计算引擎层，我们对接了一些开放的引擎比如说flink、spark、hive，未来还会对接presto，对引擎对接都是开放的。
在底层，我们对接很多这种对象存储，包括HDFS、S3、MINIO、OSS等都是有能力去支撑。另外数据写入格式是用的一些开源的格式，如parquet/arrow等开源格式，而不像某些数据仓库，使用自定义的格式，对于使用其他数据仓库就显得不够通用。
最后这一套整体架构，不管是流还是批数据都能进入到湖仓框架里面，流批数据统一，高并发高吞吐，对于上层AI/BI的分析师，可以直接从中取出全部数据。

![lakesoul-6](./images/lakesoul/6.jpg)

通过概述整体框架之后，给大家总结一下我们的几个特色的技术能力：
1. 我们提供的acid这种高并发高吞吐的这种能力

当你想实时写入的时候，通过ACID能控制写时不冲突，遇到冲突的时候会做一些冲突检测。
另外就是说我们提供这种行列级别的upsert
这种的话就比表级别就肯定是
在吞吐量上表现的更细腻度
肯定就是并发上就会高一点


在元数据这一层，我们提供是个分布式元数据，对比现有的现有的一些同业产品
其实那个小数据文件的元数据文件会比较多
甚至会造成一些困扰
2. 我们在对接的计存储引擎上面
比如说我们现在对接云原生这种对象存储上面
我们是做了大量的这种加速优化的
后面会详细讲我们是怎么做的
但是这个这里面加速优化
我们的性能会有一个几倍是十几倍的这种提升



![lakesoul-7](./images/lakesoul/7.jpg)

深入细讲介绍一下我们的元数据层是如何设计的。我们设计元数据层的主要目标是，希望能提供高并发的实时更新和批量更新的能力。

在以前传统的数据仓库上面，比如在同一个路径下面同时写数据可能会造成覆盖，你也无法知道读的是否是最新的数据。如果事先引入元数据层的话，在高并发读写的时候就能够实现读写分离，在读的数据时能读到比较一致的数据。另外在高并发写的时候，有很多元数据，会产生很多元数据的小文件。比如在hudi/iceberg里面，会产生很多delete file。这种元数据的文件规模一旦变大，会造成一个很高的性能瓶颈。虽然有各种各样的优化方法，但是我们实现的方式是直接使用postgresql数据库来做元数据管理，我们可以管理表、管理分区、管理文件，在每一个管理表里设立主键，可以认为这就是它的索引。另外我们通过数据库的事务机制来控制着并发写的时候产生的冲突，同时我们实现了快照读的方式，快照读指你只能读某个历史的版本，或者读最新的一个版本。而正在写还没有提交的数据，是不会读出来的，这就是通过快照读实现了读写分离。
另外PG数据库的性能高，单机模式上面可以支持千万级别的一级分区的文件。众所周知HDFS上面的小文件问题是一个比较头疼的问题，因为它不能做增量更新，你只能采取一些其他的处理方式。
如果在云上面就可以使用PG数据库的分布式数据库协议来做一个横向扩展，满足更大规模的数据要求，能达到几十亿级别。另外在元数据并发写的时候，为什么我们能实现并发控制呢。是因为我们对这个任务在提交的时候
做的任何的数据类型
他提到类型做了很多这种细分
比如说我们支持是我们把任务进行
比如说你是append的操作，还是update操作
还是MERGE操作还是compaction操作
细分很多不同任务类型
任务类型这样的好处就是说
我们在解提交的时候
会对不同类型做一个判断是不是允许并发，如果产生冲突就会回退
这也是基于MVCC的技术实现的快照图。因为MVCC是数据库上常见的实现方式，我们做了很多借鉴。
另外再是说在元数据的这种
就是提交的时候
是不是大家之前不是说了读写分离吗
读写分离其实我们直接
在写的时候提供了一个两阶段提交协议，在数据写入之后，再提交的时候需要确保数据是exactly once。



![lakesoul-8](./images/lakesoul/8.jpg)



接下来着重讲的点是upsert和mor的能力。mor(merge on read)，对比之前提到的copy on write，在lakesoul里面merge on read是天生支持的。对比copy on write 和mor的两种场景：

copy on write一般就是大家可以认为就是面对读多写少的场景会比较好，因为copy on write的实现方式上面是这样的。在写入一批数据时，会把之前的这些数据先读出来，做一次合并，然后再写数据。所以说在写数据时，会产生写增大的性能问题。面对读多写少的情况会比较好。

但是在写多读少的场景下是需要有upsert的能力，即update and insert的能力。我们引进upsert，在写多读少这种场景下提供一种高并发这种写入的机制，每次upsert就产生一个新版本，在读的时候都会做一次merge on read，对已写的数据做一次合并。我们的实现方式是在upsert的时候必须提供主键，然后按照主键进行一个hash分桶，单个文件再按主键做一次排序，可以理解在每一个文件里面数据也是有序的。另外我们还支持多级range分区，可以运用到一些分区查询中。在读取的时候，因为我们单独文件是有序的，在多版本文件产生后，把几个有序的文件做一次归并排序，实现merge操作。这样的归并操作效率会比较高一点。另外如果在归并的时候，需要做一些其他的操作，比如对于同一个主键的不同版本数据，拿最新的数据求和，可以使用我们提供的operator的功能。这个operator的功能能够控制主键的行为，目前默认的行为就是用最新版本的文件去覆盖以前版本的文件。又比如需要对同一个主键，做一次求和操作，但是这个主键里面有null值，希望用之前版本的非null值来做，这个也是支持的。
在upsert的能力上，我们可以扩展出很多新颖的东西。因为多流并发写的功能，多流并发写是指不同的流同时往一个表里写，不同流需要满足一个条件，就是主键必须相同，但是非主键列可以不同。因为有acid，可以相互往这个表里写，即使发生冲突了也不要紧，保证数据的一致性。所以在写的时候，都可以并发的往里写，提高吞吐率。
所以在最后读的时候，是如何去做合并的呢？其实读的时候主键是不变的，但是非主键列如果有就取出来，即不同流里面的主键都能取出来，这叫做schema引进。最后就可以像一个个join操作，自动去扩展成大宽表，这是一个很特色的功能。
另外在写的时候内存占用率是极低的，而且是与引擎无关。其实大家在使用hudi或iceberg的时候，比较关注它们的性能，但是没有关注他的资源消耗，比如内存消耗等。会发现它们的内存消耗是很高的。同比lakesoul的内存消耗是非常低的，这是因为我们是用的rust去做了底层的这种处理方式。





![lakesoul-9](./images/lakesoul/9.jpg)

![lakesoul-10](./images/lakesoul/10.jpg)

正如上面说的，我们在io层上最近实现了很大的突破。比如传统的hudi或者iceberg的框架，对不同的spark版本，或不同的flink版本，可能都要实现一套merge on read或者是copy on write，开发成本会比较高。所以我们专门独立出一个io层的概念，该io层与计算引擎无关，只提供简单的读和写的接口，上层引擎只要调用读和写接口就能
使用server 的upsert的的功能、merge on  read功能、copy on write功能，以及引入其他operator的功能。

针对计算引擎，我们只提供最简单的writer和reader的接口。因为开放计算引擎接口，对接flink、spark或者AI引擎。AI引擎接口不是简单的SQL就能搞定，还要用Pandas接口，在分布式场景下，还有tensorflow或者pytorch的接口，因此IO层得支持多语言。我们底层是用rust实现，提供c的接口，Java的接口，Python的接口都没问题。另外我们的底层是对云原生也是天然支持。底层利用rust做了大量的存储优化，而且严格控制内存消耗，内存使用非常低。我们结合参考了arrow-rs 还有arrow-datafusion的一些实现的方式，写了很多rust的代码，实现了异步式的writer和reader，这个对读写加速非常重要，对性能有很大的提升。

我们现在主要实现的是parquet的格式，后面会接其他格式。另外在writer的时候，按照主键排序实现了异步的multipart upload的这种机制。另外在reader的时候，我们尝试rowgroup预读的方式，在读这个rowgroup的时候，下一个rowgroup已经出来了，整个效率也会非常高。最后我们也实现了 merge on read按照主键进行有序的归并。

我们通过JNI提供Java接口，通过ctypes提供Python的接口，后续大家可以在我们的github上面去看一下我们的最新的接口文档。

这里简单介绍一下，我们用这个io层到底有多大的加速。通过使用spark去读aws  s3的数据，把spark里面的底层的原生的parquet mr替换成我们的native io，进行对比实验。发现在读方面，我们会有一个3.6倍的提升，在写的时候也会有一个1.44倍的提升。

大家可以根据上面贴的链接去做尝试对比。我们的代码都是公开评测，数据也都是开放的，我们就希望在国产这一块，可以做到一个很好的领先水平。现在大家比较讨论国产化，在这个背景下，作为国内现在目前唯一的开源湖仓框架，也是有我们很多独特的设计和优势，这也比较符合当前的政治理念。



![lakesoul-11](./images/lakesoul/11.jpg)

![lakesoul-12](./images/lakesoul/12.jpg)

上面讲了很多这种细化的功能，现在从整体生态来讨论
在生态系统上面，就像之前我们的定位一样，首先对于数据，我们希望任何异构的数据都能同步过来。同步进来之后，做增量计算或者流批一体的计算都在这个系统这里做，包括ods dds odl这种数据分层的设计也在这个系统里实现。上层提供面向用户的接口支持，不同的用户可能需要不同的接口，比如SQL，AI公司的append接口等其他接口。
另外最重要的功能是在很多细化工作上，数据不断进来之后都会带版本，可以用于快照读，也可以实现回归，做增量操作。我们把我们的湖仓一体平台搭建好，建好之后只需要解决上端的数据源问题，直白点就是如何把数据同步迁移到湖仓里面。

大家在考虑使用湖仓框架，会有很多问题：

- 第一个问题是在现有技术框架下，比如hive中已经有很多数据，需要做数据迁移。如何解决数据迁入到湖仓的问题。
- 第二个问题面对不同的数据源，需要自己做很多支持。比如当前已经有Flink CDC，可能同时还有很多Flink SQL，需要先去mysql里面拉取数据，再在后排起一个表，往这个表目的表里面塞，会产生很多开发成本。我们在这方面做了考虑，基本上只需要用户做一些简单的配置，填连接信息，就能享受到我们湖仓一体带来的优势，在实时数据的同步中出现数据failover的时候，也提供保证exactly once的机制。包括数据源的动态调整，在底层对用户都做了屏蔽。
- 第三点：数据在解决上层数据入湖之后就需要对接下游数据，如何去给用户使用。这里是有很多不同的方式的。比如只是做一些简单的bi报表分析，就可以直接提供JDBC接口或者restful接口这种简单形式，直接拿我数据就可以做一个简单的报表分析，这个在我们未来规划中，可能会有一个开源的产品给大家去看。另外对于bi分析师来说，都是希望希望数据更多更广。正好湖仓里面的数据是又多又广，可以拿来做各种各样的数据的加工处理。先前提到的我们支持流式upsert，大宽表的拼接能力，可以认为这是无限扩大的特征能力，在后面会再进一步介绍。

然后在生态里面，我们现在已有提供的能力啊
是说这flink CDC整库入湖
自动ddl同步这种
这种多源入湖的这个能力
怎么说呢
就说以前大家那个
比如说举个例子mysql上面
大家做的时候
如果mysql上面新建的一张表
或者表里面自动做了更新啊
做了比如说做了一个新增或者删除，是无法原生感知的

然后你需要把flink作业可能停了
停了之后你再改一下他的sql语句啊
sql语句之后你再起
然后你才能把新的新的数据能同步进来
就是新的上面地ddl同步进来
然后
这样的话就是他其实也比较困难吧
就是说你不断改不断停
那么要是任务多了也挺麻烦的对吧
就是说我们现在这核心就是说
我们就是说你就配置一个库
我这库的用户名密码是啥
后面就说这库里面你就不用管了
剩下就是说你剩下
新增那张表表示新增的数据
数据表里面改了数据
我全部给你自动去做处理
这里是我们改了很多内核的源码

做出了这这种类似于商业产品化的这种能力啊
另外就是说我们在kafka上面
我们也可以做自动这种topic感知
你topic一进来之后自动去帮你去
后台的lakesoul里面去建你的表
然后数据会自动去同步，只需要配置kafka的连接信息即可
然后在这
数据入湖的时候就说上面读的时候
我们还提供一个增量读的一个能力啊
因为增量读什么快照读什么一些什么读我们都支持
因为增量读是一个比较有挑战的一个
你所以说我们在增量读上面
我们能够就说不断的去读取
增量数据有很大的这种商业意义
就是说你
以前的读的数据
就每次都要读全部的全部的数据
那意思那就是说我每同步一批
我就我去拿这一批的数据
我下次同步一批我再拿下一批数据
这样的话就有很多就有很多这种
有很多这种这种
上面可以做很多这种这种计算
你比如说做一些新增用户了
什么感知什么自动就能帮你去呃帮你去做这个事

但是做这个事可能会有一些阻碍
就是说我可能用户不太喜欢写SQL
或写一些比较复杂的这种处理流程
我们也为你们考虑的其实

lakesoul里面也做了一些考虑，比如简单算子、比如过滤算子、比如group by能力、比如left join能力、再比如distinct。我们把所有的常用算子用yaml做成一个文件表现形式，在yaml文件里定义操作。
举个例子，比如我想对这张表做group by，之后想做sum统计求和，只需要填一下yaml文件，或者
在前端点两下，就能把整个pipeline构建起来。在下面你一个flink任务，任务的状态都不用关心，lakesoul底层自动会去做这个事，而且还能保证意外情况，因为我们做了大量的failover的保证机制。



![lakesoul-13](./images/lakesoul/13.jpg)



上面这张图可能好多人会比较关心，我们就直接拿数据，进行一对一比较。

这个benchmark的数据是通过我们和CCF举办的数据湖仓比赛产生的。这个比赛公开竞争，我们提供赛题，实现方式大家自己定，任何框架都可以用。但是严格控制机器资源使用，比如在4C16G下运行，每个参赛者都最大限度用这个资源，不能超过。然后计算引擎的版本也都定死。具体的数据内容如下：
模拟了一个真实的业务场景，总共11个文件，其中一个base文件，数据量1000万，每次会有一个增量数据，共10个，代表10个版本。比如从版本1到版本10，代表今天的数据、明天的数据，后天到第10天的数据。每批增量数据里有200万，其中有100万数据的主键会重复，不是指文件内部主键重复，而是和前面的比如base0或者base1的一些主键是有重复，这衡量了merge的能力。我首先把这11个文件写到一个对象存储里面，比如S3存储桶里，之后还要做一些读逻辑的计算，对这11个版本的文件有两列要做特殊计算，一列全是long型的做sum求和，另一string类型的，把新版本里有null值的去掉，用之前非null值的数据来替代，比如我第10个版本里面有null值，
在第1个版本里面对应这个主键是非null值，必须要用第9个版本的非null值作为最新的数据，而不是第10个版本的null值。对于计算框架可以任意选择，lakesoul、iceberg、hudi都行。

最终形成上图的参考结果，我们分不同模式都做了对比：

- 对于写入时候，lakesoul在copy on write和merge on read的模式，都能比其他快几倍不等。
- 对于读取时候，lakesoul在copy on write和merge on read的模式，更其他差的不是很大。仔细比较merge on read的结果，其实还是有点差异，lakesoul还是能够更快。还有很重要一点，lakesoul的copy on write, merge on read的这两个模式的读取时间差不多，这得益于我们底层的IO设计，做了很多这种异步加速。

PS，如果大家对这个benchmark感兴趣的话，我们代码和数据都是公开的，都在上方的链接里。大家可以在我们的社区尽情的交流和讨论。大家可能会发现，在hudi或iceberg提需求的时候，可能社区满足这个需求会比较慢。我们作为国内的社区，可能这个需求评估通过之后，就会在第二天或第三天上到社区里，这也是大家我们的优势。



![lakesoul-14](./images/lakesoul/14.jpg)

在应用案例上，简单的给大家介绍一些，主要是指我们提供了一个对外异构实时入湖的能力。
你可以任意建湖仓，需要支持数据kafka的自动同步。可以通过flink CDC来做自动同步。还可以自动发现新表，以及新表里面的Schema变更，都是可以支持到。

需要强调一下，lakesoul是必须严格支持一个exactly once，因为如果不支持exactly once的话，可能会导致很多数据不一致。

![lakesoul-15](./images/lakesoul/15.jpg)

另外一个案例就是大宽表，这是比较有意思的一个点。因为大家知道大宽表过去的时候都是通过join实现，join的坏处就是效率比较低，内存消耗比较大，而且数据一清洗非常容易产生oom。另外在join的时候肯定会有一个shuffle过程，shuffle过程是最杂乱无章的，效率比较低的一个过程，过程本身是比较耗时。但是如果采用lakesoul的话，是怎么实现这个过程的？lakesoul需要先定义一个基表，然后比如有3个流过来A、B、C，只要保证主键一致，剩下的非主键列不需要一致，通过底层有acid控制，就可以并发往里写，也不用产生shuffle过程。最终这些写入数据怎么产生大宽表呢？只需要直接读一下，因为读的过程会产生merge on read，merge on read会把自动一些非主键列的自动扩充，就表现成一个大宽表的形式。这在性能上会有几倍几十倍的提高。

![lakesoul-16](./images/lakesoul/16.jpg)


另外在机器学习里面，比如个性化推荐或者营销行动，都会有一个比较普遍的用户特征、物品特征和用户交互反馈这么几张特征表。这些表很重要，要把它们做join，合并成一个大宽表，这是非常贴近实际的应用场景。可以尝试通过lakesoul的多流合并大宽表来替代原来的join方式。

在商业案例上，我们非常强调lakesoul里面flink CDC的能力比较强，是因为有一客户跟我们提了这个需求，他有一个大量的在线数据库，一个库里面好几千张表，这几千张表每天都会有变更，或者会新增表，人工维护这些变更会比较麻烦。希望能够引进这么一个湖仓框架来解决这个问题，数据最终提供给BI做分析。lakesoul正好比较符合这个需求，就对这个需求做了大量的修改，达到满足商业化的成果。

大家如果非常感兴趣的话，可以去我们官网上，把这个CDC的案例好好看一下，这个案例是非常具备商业能力，而且和其他竞品是不一样的。



![lakesoul-17](./images/lakesoul/17.jpg)

最后介绍一下我们的整体生态建设，诚然我们的roadmaps落下了很多。
这里面简要提一下我们接下来要实现的目标，主要是分为如下几个方向：

1. 因为在io层上面我们还要进一步的优化，我们还看到很多优化空间
   同时compaction还没有做进io层，因为之前我们是与引擎绑定的compaction，现在我们希望把它做成与引擎无绑定的compaction。
2. 实现native merge into的能力，其实我们已经支持merge into的语句，但是只是针对主键表，后面我会做非主键表，主键表实现merge into的表达能力。
3. 在生态上面我们还需要扩大，现在已经提供了Python接口和c接口，后续还会对接presto，doris、 clickhouse等。因为我们接口已经是非常简单丰富，大家只要把开源的source和sink接口对接上去就没问题了。在机器学习上面的pandas和arrow都已对接，但这肯定不会走其他那些SQL的方式了，得使用Python的接口或者走c的接口这种方式。
4. Lakesoul当前还缺失了数据湖的一个重要东西，就是日志。我们考虑接入日志格式，跟别的接入方式类似，让用户尽可能的配置简单，只需简单配置一下而不用关心中间的一些细节
5. 另外就是说我们还有要提供一个服务层，这个服务层提供compaction和清理数据clean up的功能。我们通过参数做自动控制
   就是自动让我发现那个
   那个非常非常耗资源
   就说一旦你compaction和
   你在数据写入发生冲突的时候
   你的compaction没完的时候
   你通过这种自动这种做法
   你会非常的耗资源
   而且阻碍你的数据写入的这种效率
   所以说我们希望把它拎出来之后，做一个单独的服务层，这样的话单独用一套资源就是专门做这些东西，对线上的业务不产生影响。因为在hudi或者iceberg会遇到很多类似案例，在自动compaction之后，影响了数据的写入和读出这种能力。
6. 当前我们已经分封装了七八个算子，未来达到一定的这种商业价值之后，我们也会把它分享出来
   或者是直接就给大家一起过来去弄这个事也没问题
7. 另外在上层应用上面其实还会做很多事，包括物化视图和增量更新等，就是说物化视图上面就是说
   是我们后面着重要要弄的一个事啊
8. 最后在最近，我们正在将项目捐献到linux foundation AI&data开源组织，我们想要通过这种引入中立社区运作的方式来扩大lakesoul的影响力，也扩大国产湖仓的影响力。



![lakesoul-18](./images/lakesoul/18.jpg)

如果大家还有比较关心的问题，可以去关注我们的公众号，或者在我们这个群里面做各种各样的这种技术讨论，源码讨论，以及提一些大家的诉求。今天我的分享嗯就到这，谢谢。









今天的主题是自研国产湖仓一体框架的分享
我们邀请到的嘉宾是来自数元灵大数据部的总经理孙茂森先生
孙老师是是硕士毕业于中科院计算所
有10年的大数据内核的研究经验
先后在工商银行和民生银行深耕于金融业务
大数据架构与数据治理、智能运维、私有云等领域
有多篇的发明专利和国际顶会的论文
大家在收看直播的同时也可以把直播间分享给您的同事或者是朋友
然后可以领取到我们制作的数据湖仓的相关的资料一份
那下面呢就有请孙茂森先生为大家进行分享

首先自我介绍一下
我叫孙梦森，来自数元灵科技，现在主要负责Lakesoul这块的开发



































































Q & A
Q：Lakesoul相比于iceberg/hudi的优势有哪些？
A：通过benchmark对比我们的优势有：
首先在源数据层面上，我们不同于iceberg/hudi，对小文件的处理是利用文件系统的方式，如果小文件多了之后就影响使用效率，做compaction会导致OOM的问题。Lakesoul在这块是引进PG，千万级、亿级的分区文件都不会有问题。
另一个优势是flink CDC，目前我们是直接把商业化的能力给开源出来。你可以试一下配一个1000张表的库，只需要配置，库的用户名和密码，cdc就可以把表所有的东西全部给同步过来，后台你就看到1000张表的全部数据在里面，做的任何一个变更，新增一张表都是没问题，能够自动感知，不需要启停flink作业。
另外就是我们要我们要着重强调我们
我们这其实就是对我们这个这种
就是国产刚兴起的这个
这个新兴来上
其实我们与他们
他们是说他们走了四五年的路
我们现在是走了一年多
但是我们在这个IO设计上面
也是比较独特的一个设计方式
就是我们想要走最小的人做一个最简单的方式啊
就是io上面我们是用rust实现的
rust实现这个大家都如果了解rust大家就知道
就说这个这个底层从内存这种安全性
包括这种内存的使用率
包括这种他的这种加速机制
比如说大家现在都要用这种
有向量化引擎什么我们都是
IO层全部给你实现了
这种非常强悍的这种能力

大家可以去做那benchmark
看到benchmark你也知道
我们这个IO加速能力
还有这个那个
就是你读和写的能力都有巨大的提升
你随便拿一个数据可能测试
可能就是说就是就是没问题的
对然后还有还有很多优势
可能就是说的有点多了现在
我给大家说的可能就是这3点非常那个独特
另外就是说merge on read的时候其实我们提供operator功能吗
operator这个功能其实也算是我们比较独特的一个一种方式
可能大家可能说operator
可能hudi可能用的payload也能实现
但是你你可以看看你hudi payload
你写的代码得写多少
我们这operator你就写一行就行了
就是说这有很多
就是
有很多人有比较注重的是用户体验的层次

就说我们
本来是按理说应该商业化考虑这个事
但是我们在
我们在做这个这个开源社区的时候
我们就希望这个一步到位
就说底层框架我们都要
我们该做一些前线技术
该做做上层的上
游的系统对接和下游用户的使用
我们都要考虑
就是说用户上面不需要
太多的这种这种这种配置
大家那个那大家如果flink里边的痛苦
大家都是清楚的其实



Q. schema变更思路

A. 啊另外这个schema那个变更思路，大家比较关心这个是如何实现的。基本上底层都是全部被我们修改了，没有以前的影子了。
这是这样的就是说呃
底层刚开始就得
去拿他那个souce的时候
你比如说举个例子mysql source 的时候还是用的是Flink CC source
那个没有去他的
比如说你该用他的
增亮读还是它的全亮读那块没变
但是拿完之后后面就变了
变了之后什么做了哪些操作呢
就是说schema里面我们分两个流，一个ddl流和一个dml流。
ddl指所有做的数据变更都能从这个流拿出，包括数据的insert、update、delete等
dml指拿到数据所有的schema变更，同步到lakesoul表去做相应的变更。在dml的变更时，我们底层是用binary raw data去实现。以前的flink可能都用raw data，效率会比较低，binary raw data实现的效率极高。更细节一点，dml raw data之后是能够感知到是具体哪张表的变更。即一个dml里面配置整库1000张表都能同步，同时也可以指定哪些表需要同步。但是如果配置的是整库，会把一个库全部写进一个流里，这个时候就需要区分是具体哪张表，再去做分区写，做hash排序。并且我们底层使用native io，效率会更高。总结就是我们实现了分流，并且在底层做了加速优化，提升吞吐量。




Q: 是否支持Oracle？

A: 我们现在支持Oracle，可以看我们源码里面Oracle的实现。
同时我们现在所有东西都是开源免费的，可以随便使用。如果有遇到问题随时找我们，我们都会第一时间就协助解决问题，非常欢迎讨论。


Q：Oracle DDL同步的问题
A：Oracle DDL同步是有一点问题，是受限于版本的问题。


Q：compaction的性能如何
A：
哦我们compaction性能也是不错的
因为compaction其实走的就是说它的原理是把所有历史的这个增量文件
给合并成一个嘛
所以说
你增量文件以前如果是那种小文件的话你你你如果是在
hudi或者iceberg里面走那种文件系统
你去拿那些合并会比较慢吗
我们直接就是通过这种表
直接就把所有的文件给拿出来
拿出来之后非常快也是
而且都走的主键索引
所以那种
是单纯拿原数据层我们是没问题的
但是合并的时候这
这你也看到了我们的写入能力
我们读出能力
和他们那几个框架相比
我们都是有几倍提高的嘛
所以这一块其实你可以测一下
绝对是没问题的



Q：非主键merge into是怎么做的？
A：
主键merge into我大家也
嗯就是说是这么做的
是说
呃直接把它转换成一个lakesoul的功能
非主线merge into
因为你说
里面可以有各种各样的问问条件啊
这个东西是怎么转换
我们是我们现在
现在就是说要把它给做一层转换
就转换成下层的lakesoul的语义
这块其实具体做法没有想好
但是大体思路是有的
只能是现在是这么说然后
然后就是说将来肯定既然你关心了
将来肯定就是说嗯
不出很长时间
这个东西你可能就能看到
非主键merge into是怎么做的
因为
我觉得这个大家提这个需求是非常有必要的
实践中是非常常见的一个问题

Q：DDL变更和DML的Schema变更的数据是怎么处理的？
A：
嗯这这个问题这个问题还
挺好的呃就是说我们在其实在
我们
我们其实在在做这个ddl的时候
我们其实
你你在写那个
因为我们不是
呃如果是晚的话那么ddl晚的话其实
嗯首先是这种情况比较少
另外就是说他肯定会有一个窗口抖动
就是抖动的时候
就说网络抖动引起
他可能会延迟啊延迟之后
其实对我们来说
其实那个数据已经来了
数据如果是新来的时候
其实会有一部分的
就是说他原先他拿到的那个数据的一个表
数据那个表还是之前那个表表结构
这种表结构其实这一块在写入的时候
可能会造成一点点困扰
但是如果一旦但是我们这块会做了
我们不是有Schema的扩增的功能吗
但如果你是扩了一个Schema
其实对我来说就没问题的其实
但如果你小了这个
只要你同步过来之后
最后在上层读的时候会把
自动做一次过滤的
就说你的schema晚了之后你最后再读的时候
我会我们会按照schema最后那个schema那个模式去读数据
我从不同从不同文件里面去读去摘取
就是最后会会数据可能会
多一点或少一点
呃不是说是多写少写啊
是多一列可能少一列这问题
但是就是说
最后不影响最终的一个结果

Q：整库写入会有压缩吗？
A：我们在写入parquet的文件中时，都是有做压缩处理的。

Q：这个时间线回述是必须支持吗？
A：这是特有功能，时间线回述是指快照按照指定时间戳，想要同时支持快照和增量的话是要必须有。

