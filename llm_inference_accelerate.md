主要跟大家分享一下大语言模型推理加速这块的一些工作
当然就是说这个前部分的
可能会比较偏大圆模型的推理
然后最后的话
会结合现在咱们最新的一些
包括Gr的呃推荐的
Gr的模型和大圆模型的
在推理上面的一些异同点的一些地方
然后包括我们的可能Gr这些模型啊
在推理上面会有些什么样的一些挑战
然后来讲这块的一个工作
而且呃就说这里可能前情提要一下
其实说大圆模型推理这一块的话
可能我的呃讲的重心
不会将一些之前深度学习
呃系统推理相关的一些优化
比如说像呃炼化呀
然后呃算子的优化呀
这种类型的东西
就可能基本上就不会太提
然后更多会集中在这个大语言
模型推理
他所特殊的一些地方
然后来进行一些呃讲解
OK
对当然现在就是大圆模型
就是我们得先自己直接了解一下
它的基本的结构
那就是咱们传统的Transformer的结构
而整个推理的过程
我们一个头可以就是或者叫一个字
我们直接呢
看到一个字的生成的过程
其实大致上我们可以认为分成两步
第一步其实就是根据已有的信息
也就是说我们的in put已知的这些信息
然后去估计下一个TOKEN的概率分布
第二步其实就是根据一些采样的策略
从这个概率分布里面
挑出我们认为最有可能的一个
下一个头啃
但这个过程有可能是
以概率最大的
然后就偏Grady的方式来做
有可能
其实是要考虑到后期生成的一些头
啃的一些呃概率
然后从总体上去做一个采样
对这个地方
其实就大致上这是跟呃
我们传统的呃
深度学习的推理其实不太一样的地方
就是它的这个两步
然后然后最后是一个循环的过程
当我生成了下一个头捆之后
这个下一个头捆会进到下一步
就是F1里面去
然后去走到啊再生成再下一个头盔
哎这是一个基本的逻辑
那我们就要看
其实这里面就会引出一个
我们经常会听到的一个东西
叫KVK到底它是个什么样的东西
到底它为什么有
有KVK这个东西
刚才我们提到
其实就是我们在step一的时候
它其实是根据已有的信息
这里已有信息其实包含两个意思
一个是你的原来最原始的输入
然后还有就是你之后的生成的TOKEN
如果我们把这个一个生成的过程
这样去拆开
比如这个呃前面部分
这是最原始的输入
然后第一个TOKEN生成的是a
那我会第二步
其实从这个概念分布的逻辑上来说
我其实是要把前面的部分再加上a
然后去估计下一个TOKEN
然后依次循环
这样就会导致一个问题
整个的这个计算量
其实是在不停的增长的
而且就它会直接跟前面的这个生成
已生成的部分
和已经铺头部分形成一个正比
这样的话
可以想象的这个这样一个逻辑
他一定是越跑越慢的
然后所以在Transformer这个结构里面
他有一个计算的特性
其实就是他当前偷困的结果
其实只与前面的偷困有关所以说
他就会把前面TOKEN的计算结果
进行缓存
比如说他就会
我们经常常听到的两个阶段
就会产生了
那就是说是Pre filling的阶段
就说我把我的输入输进去
然后他就会产走一遍全部的过程
这是全量的一个走这个模型的过程
走完之后
然后我就会产生一些中间结果
这些中间结果被缓存起来
放入我下一步结构
这里标红的这些下一步的过程中
然后这个KV开启
会在进入啊腾讯之前
然后跟现有的
这个新生成的那个TOKEN的结果
做一个concat
然后再做计算
然后之后又是只有一个TOKEN的过程
也就是说通过kvkat的这样一个优化
那直接的结果就是
我的在decoding的整个阶段的时候
我的计算量
和我前面的投梗数就变得无关了
就说这里是其实是一个近似的无关
因为在主其他主要的部分都是无关的
但是在额腾讯的计算这个地方
是被恢复成了一个全长的
然后进行
而腾讯的这
这是其实现在的整个KV catch
为什么它会存在
就是为什么会有KV catch这样一个东西
它其实是要让的口顶阶段的这个计算
去尽量复用以前已经计算过的结果
这样的话来达到
就是我们整个对前部分没有
就前部分这个数量没有依赖
然后让我们
的整个推理的速度变提高
但是其实这个样子
我们就会发现有一个问题
就是说我第一把进去的时候
其实我是有很多的input的
然后之后我每次只会输入一个
比如说我们可以简单的认为
如果这里输进去的是一本书
那可能就是成百上千万的一个输入
然后最后之后的计算全部都是一
他的数全部都是一
所以说在推理的时候
就会发现出现这样的一种现象
比如说当我这是一个
我们按能够一个A100的卡
他能够就是并行推理的头肯数的一个
跟他的GPU的Tiffrox课题的一个呃关系
那我们可以看到
就是我当我的Cocon数然后增长的时候
那我
其实我的GPU被更充分的利用起来
让到达的一个限度
到达一个预值之后
也说我的
可能跟这个跟我们算值的组合有关
跟算值本身的时限有关
然后到达之后
然后基本上就是这个GPU卡
能够提供的最大的一个t Flops
但是我们就发现我们的其实
在profile的过程
其实我们是运转在这个区间
我们是可以把这个GPU给压满
但是因为后期
我的那个我病情
每一次只只预测了一个头壳
所以说我最后的病型
头壳数就非常的小
我也就说我大
在生存的大量过程中
我的这个呃
GPU都处于一种不饱和的工作状态
那不饱和
那其实最简单的问题不就是做batch嘛
就是把batchler加大
那我这个地方是不是可以往这个
方向移动
那这里面就有一个问题
我就是怎么才能把batchler加大
由于KV catch这个东西的存在
而且由于在大模型的情况下
它的bitch size就是那个KV catch
其实占显存间占的很厉害
那实际上我们发现这个bitch size
我的
我们的目标其实是想想把batch size搞大
但是BAT size其实受到显存的限制
那我们就需要去看一看
这个显存到底是被怎么消耗掉的
那我们去看
我们的一个正常的一个执行的过程
刚才提到啊
有两个过程
一个是perfume
一个是decoding
那我们正常一个模型加载之后
它会占用一部分线圈
那之后当我第一把去执行
我的input TOKEN的时候
我们的显存有个快速的一个消耗
之后随着TOKEN的逐步的生成
我这个消耗在慢慢慢慢的变大
所以说它其实跟咱们深
就是常规的深度学习的推理
非常不一样
因为它有这样一个深层的perfume
然后以及后面深层的过程
比如说它的这个消耗过程
其实第一跟这个长度有关系
然后当然我们在另外
我们家的输入有多大
这个也有关
也有关系
然后我们当我们掰size扩大的时候
那这个地方肯定就会成倍的往上涨
所以说我们就需要把
从显存这个角度去看这个事情的时候
那我们就把这个那个很简单的公式嘛
首先是模型占用了多少参数
然后在模型的推理过程中
我有很多的中间变量其实也会占用
一部分参数
然后另外我有多少TOKEN
有多少这个TOKEN的cable catch缓存
它也会占一部分
嗯
那么最后我肯定是要小于我的内存
呃显存的一个大小
那就正常而言
我的嗯一个模型
比如说我们规定了
上线的就是一个拉玛13B的模型
这种模型一上去
那这个显存基本就固定了
当然就我们这先可能我不会讨论
比如说呃电话呀那种类型的
因为这个这个
其实就到时候对应的去改改这个公司
就好了那不是一个大约模型推理
就说特特殊的部分
那我我们不会去讨论那一块
然后
这这个地方
我们其实我们想优化的是
把白塞子做大
那就可以优化这个白塔
看这个显存能不能搞得更小一点
那这个这个
其实哦哦
这个是白塔
对白塔这个搞的更小
其实接下来有一些技术
就是比如说是KV
看起来量化的一些技术
那这个地方我我们也属于是
这个也算是模型的
就是量化部分
我们其实今天也不会去设计
然后那么还有一个就是
这有占用了KV cat的这些头壳的数量
然后那这个东西跟什么东西有关呢
其实我们可以看他
其实可以这样去理一个公式
实际上是我的半企内
平均的头根的数量
比如说我现在已经输入了多少头根
以及我生成了多少头根
总的一个总数
然后当然乘以by size
然后这里还有一个伽马的系数
其实就是by起之中不同的请求之间
它的TOKEN可复用的QVT
简单的来说就是
我如果是相我的头部
比如说简单的讲
大家输入是我都是什么什么什么什么
然后这样一个头部
所有的请求都是这个头部
那它所对应的KV
看起其实内容是一样的
那我们就可以把这个内容
他的QQ开启
就可以复用
简单的讲这个道理就是这样
好啊我们刚才提到了那个意念的LM
那其实这简单的介绍一下
意念这个框架本身的一个由来吧
其实它取名是自一念三千
就是其实因为我们现在的大圆模型
实际上就是在
呃一个模型
转瞬之间
生成了很多很多的
各种各样的不同的结果
那而这里面
其实一念本身
在佛教里面是一个一刹那的意思
正好他其实对应的这个单位
大概就是我们一个正常的13B的模型
生成一个头壳的时间
所以说嗯
这一念3,000
它的意义其实也就相当于用一念l m
我们的目标其实是像大模型
在一瞬之间生成世间万象
这是一个呃名字的由来
所以说我们其实这里面就会
我们第一对nature say
对so put这两个方向
就会去重点的去做相应的优化
然后
这里是呃一念LM的基本的一个框架
咱们从上往下去
挨着挨着去说一下
其实最上层
其实就是咱们经常实现的这些模型
拿马的百呃百传的千问的这些模型
这与咱们常规的机深度学习的模型
和常规的推理方案不一样
我们采用的是
手写模型
比如说我们抛弃掉了计算图
有个计算图以前都是用角度算值
然后去拼成这个模型
那这个最大的好处是有很多的灵活性
然后可以便于算法人员去实现
但实际上
这个就会带来另外一个问题
就是他的深度优化比较难做
以至于最后
其实最后大家就会去走向一个方向
就是我去做融合算子
然后把这个融算子放到图里面去
然后他如果符合了这个图的这个pattern
然后就用这个融合算子去把它剃掉
而其实Transformer结构
坦白讲非常非常的结构简单
而且以现在的情况
结构基本都开始完开始收敛
然后大家都不再去卷太太
去卷模型结构这件事情
更多的去卷模型效果
就是说
这一块我们需要去实现的模型的类型
其实是比较少
然后所以
这就让手写模型和手写算子这件事情
变得有利可图了
在呃手写算子这件事情
就尤其是像各个大厂
像英美达或者像呃Facebook
其实大家去观察就发现
他们的那个算子都最后为大圆模型
这个场景写的算子都非常的大
甚至就是把像这样tension
现在基本就是一个标准的
用Flash Tension这些
然后做一个大顺子
然后来做
然后为什么这个地方还要用我们
还还就是不管用大份走
为什么要彻底去丢掉呃救援图
这个事情呢
因为其实我们还希望的是
去优化
整个呃模型推理过程中的那个显存
就说把只有你一步步的全部把已经规
定好了这个模型了
那那我就可以盯着这个模型
然后仔仔细细的把里面的显存的使用
全部去调好
让他的整个显存使用最小化
省出来的显存
我全部都可以拿去干其他事
去干干QQ开启的这些的事情
啊然后另外一个
其实就是高效的调度去提高吞吐
呃稍后我会去讲到
具体这这里面
业界有些什么样的一些技术
然后去做这样的一些事情
然后当然下面就是算子择优
就说我们的底层的这些算子
其实很多时候会是开元的封状
然后
因为其实当模型结构相对固定之后
这些硬件厂商会专门针对这些算呃
这些大的算子
然后进行优化
然后提高性能
然后从而到最终
他们的目的是卖掉他们的卡
就说这一块
其实现在在大圆模型这一块
这个是每个厂商都在干的事情
甚至为了不同的就可以
大家常说的不同的tensor的大小
然后去写不同的算子
这种事情都有人在干
因为只要因为因为这样的话
他其实是可以拿到一些收益的
因为这个模型
是大家都现在都在试着去用
所以说他这个收益
呃投入产出比还是不错的
然后那到下面
其实就是一个多硬件的支持
其实现在业界的主流的框架
基本上都是英维达偏多
然后当然也有支持英特尔的CPU的
但其实呃
像同时支持GPU和华为升腾啊
就是国产的这些卡的
整个呃
生态其实是并不有那么的完善的
我们其实现在从
国内的厂商来说都会面临一个问题
就是卡GPU卡
高性能的GPU卡进不来的一个问题吧
所以说从一完全的角度
我们也是必须要去支持不同的硬件
那其实有很简单有个道理
就是为什么不是英伟达用一个框架
然后华为用华为的框架
这样就这样不就挺好吗
对吧两边的都能用到一个好的收益
但是实际这种情况就是
当你在做这一层优化的时候
你会发现你就会面临不同的框架
然后去要重复的做事情的问题
然后
当然另外其实还有就是这些框架本身
各个框架
对你可能后期的业务的
更上层的一些逻辑的适配
本身也会存在一个表达
我们通过上面统一一个框架
然后下面支持多种硬件
这样就相当于做到调度
这层一次优化在所有硬件上都可以用
就像去去实现这样一个目标
这样对于整个平台本身的可运营性
也是有很大的好处的
OK刚才提到
就后面可能会重点的讲调度的部分
就是第一个问题
其实可能是大家或多或少
都应该了解的一个
其实就是computer smashing和page tension
其实从我们最原始的那个公式来看
它其实就是在优化有效的bet size
怎么去看这个问题呢
其实就是
我们正常会把不同的请求打成一个bet
然后去做GPU的推理
然后这个过程往往是一个buy进去
然后要等到这个buy
里面的请求全部处理完
然后我们才整个才退出这
个
其实从GPU的调度层任务调度上来说
是最简单的使用方式
但这个这种使用使用方式
最大的一个问题在于
它的有效办起会越来越低
因为每个请求的输入长度不一样
输出长度不一样
简单一点输入
我有可能第一个请求
是一个摘要的任务
比如说我丢了一个1,000字的文章给你
让你把它用一句话总结出来
也有可能第二个请求
是一个扩写的任务
就是给了你一一小段话
然后让他让你把它扩写成一边长
稳也就说他的输入输出的不匹配
就会导致有些请求很快就结束了
有些请求还要跑很久
这样的话
你在整个
如果你要等到所有的请求都完成
然后你这个BAT才退出
这就导致一个问题
就是你的那个有效的BAT
到后面越来越小
越来越小
当然这个过程就会导致另外一个问题
就是你的后面很多请求虽然结束了
但是你你这个消耗就嗯GPU就没用
那一个很直接的一种思考方式
那就是我能不能当一个请求完了之后
我就把另外的新的请求给他塞进去
对吧这就是custom的一个想法
其实这个想法在22年就已经被提出来
也就在微软就已经做了
但是这个其实为什么后来一直没爆发
就这么好的一个想法
为什么就一直都没爆发
其实里面有一个问题是
它的KV catch
这块的显存的操作成本比较高
其实就是说比较麻烦
那到去年的时候
由伯克利这边提出的配
件腾讯就把这个问题给解了
然后于是整个这个computer版型加配
件腾讯的这个机制
就迎来了他的一个爆发期
现在成为了
大圆模型推理框架的一个标配功能
那简单讲他为他怎么做的呢
那就是如果整块操作成本高
那那就把它切小呗
对他
其实他就是用了
咱们常规的操作系统里面的
一个分页机制
内存分页的机制来管理显存
那就是把这个KV cat切成不同的块
切成不同的块
然后让你用个指针再过去
然后这样去引用的方式
然后来进行这个计算
这样的话
他就显著的降低了这个呃
显存操作的这个力度
这样的话
我的整个的
对controls banking这个调度而言
它的操作效率就上来了
整个的收益就一下就上来
刚才提到了computers Banch
如果我们按这张图去看这件事情
其实就是最早的通用方案
那就是一个Banch一个Banch的往下走
那这就是可以明显看到的
这这就是咱我们刚才提到的有效Banch
他在不断的缩小
那controls般性解决的问题
其实就是把整个GPU填的更满
那这里面其实还遗留了一个问题
就是我们的input
它在请求与请求之间的共享问题
也就说这个问题
其实以前在深度学习的推理里面
很少去关注到
其实但是在在呃推理里面
实际上以前是比较早的时候
也也就已经做过类似的一个事情
那就是我的多个请求
比如说我一个用户的多个item的
rack操作
然后这个时候我
我用户因为都是一样的
所以说我这一部分的推理
我只做推一次
然后不同的item我的推理多推多次
然后我再把它融合到一起去
再把它拼到一起去
然后再做后面的一些推理工作
这其实在呃推荐领域
前两年就已经干过的一些事情
但在半月模型这个推理里面
其实这个事情一直就这样没有干
但他有什么特点呢
其实有类似
就是我的请求input有共性
直接请求的input是有共性的
我这些共性的请求
我是其实可以只算一次
然后把它的因为KV果
我刚才提到
kvkat其实是对计算结果的缓存
那我就可以把这个kvkat存起来
等到下一个请求
这些我我处理的前面的
我就只只留后面
我就只处理后面的
这样的话
我半起之内
我就可以不同的请求之间
这个部分就只算了一次
然后如果还可以有半起和半起之间的
那我仍然可以继续服用
这样我的整个的
后期的这些请求的推理
响应就可以一下提上去
就刚才提到
其实这里面还又又会有
又会扯到另外一个问题
就是培训是开启
这个机制看起来非常的美好
但是它是有成本的
因为KV catch会占用显存
而且一旦我们缓存的大
然后就会面临catch患入患出
还有catch的就是命中这些问题
就是我如果我我
比如我放两份在在显存里面
那那我这就得占两份的显存
就是我两个
两个前序
那所以说
最终
在具体的执行节点上面的这个命中率
就决定了我们最终这个机制的收益
所以我们其实更多的craft catching
其实是刚句话
刚才我们那个公司里面这个橄榄
其实我们在系统的前置
比如说服务节点的前置
其实是加了一些个FS TOKEN的
一个路由器的
呃这里面其实就是我们这个路由
要平衡的是两个事情
一个就是命中率
还有一个就是传统路由的这些问题
就是负载平衡啊
熔灾啊这些问题
因为简单来讲就是
如果你的前序都是一样的
我只最简单的方法
我就直接打到一个节点上去嘛
他一定命中对吧
但是实际上这样的话
你就会面临你的负债
平衡和熔灾的这些事情
怎么解决的问题
所以说
我们其实是在构建了这样一个路由表
比如说简单
我们呃以经常咱们看到的
一些角色扮演的这种服务为主
就是比如说我们会咱们聊天
比如说正在跟宋江聊对吧
宋江聊
你
其实会需要给魔性输入什么东西呢
第一你要告诉魔性
你现在正在扮演的是宋江
宋江是一个什么样的人
然后你们之前他有什么样的生平
他有什么样的呃力啊
一些重要的事件
然后他有性格
什么样的人
这种各样
就跟他自己的用户简历一样的东西
然后再说你跟宋江前之前聊过什么
好下面请生成你准备回作为中奖
你要回复给用户的信息
比如说这个过程里面有大量的信息
其实是跟用
户profile一样
它是一样的
还有那那就说比如这另外一种
它就是a闪这个角色对吧
或者牛顿这个角色
对这些不同的角色
然后进到这个模型的之前的
那我们就就会把前面的这一段
做一个practice TOKEN
就相类似
就相同的这一段吧
然后指定
然后给他指定一个具体的路由表
在我的有限的这一堆机器里面
去选中一个机器集合
这样的话
其实就相当于是我对宋江这个角色
他就会陆游到这些节点上去
然后另外我们刚才也提到一个问题
就是我们需要解决的问题是不能
有太多份的catch
就开启份数多了
最后最后你发现你显存的全是catch
然后都没没有开启去
没有显存
去计算你当前要执行的那个请求了
所以我们通过这个维度的管理
就说对于单一的节点
我的所属的就相当于star
set其实是有限的
从这张图大家可以简单感觉一下
其实就是最后每个节点它只有两个set
只命中了两个set
然后从而达到就说对单个节点来说
我的catch分数是相对可控
相对可控的
然后又能够在这个set的维度
完成负载平衡和熔灾的这个事情
好
然后最后我们再讲一下呃
CPU跟GPU的混合推理
其实就是优化
简单讲就是优化m
呃刚才我之前我们已经提到
就是其实有其他的
比如说把把模型从FP16变成INT8或者INT4
这种其实是量化
它是效果有损的这
个从框架层面叫支持
但是业务用不用
那得业务去评估
那我们这讲的其实是无损的一些方式
这这里面
我们其实前面的同学
已经很多都讲到过一件事情
就是一个计算密度的问题
计算计算密度这个东西
我们可以看到在大圆模型里面
我们一般讲的大约模型是计算强
密度很高的
那其实往往是讲的是
这部分是它的Transformer结构部分
但实际上它还有一个Hokken 1000部分
这个部分
实际上就是个差别
就是
咱们经常在推荐里面的一个SPA操作
但往往这个部分呢
又是放在GPU上干的
这个表一旦大了就它就会吃显存
那而且就说咱们看到的
一般的我们在业务应用的时候
很多时候都会括TOKEN的里面
也就说会括这个表
因为因为原始的模型
它里面的那些词
简单讲它那里面的一些词
其实跟我们业务里面的词
有时候并不是完全全覆盖的
咱们有可能业务里面会有一些特殊的
自己业务场景的这些词
所以说往往最后我们看到开源的模型
可能也就是像现在标准的呃
那么13B这是2的时候是3.2万的磁表
但实际上业务器用用的时候
然后就慢慢就扩磁标了
就我们这以在拿马13B来看齿表
这个小原声的齿表
可能占整个参数
可能也就只有百分之一
但是如果我们把它扩到
百分三十万的尺表的时候
那其实它对整个的模型参数量的占比
就会到11.8%
那就是说这是11.8的
显存啊
而它又是个SPA操作
那很直接的一个想法
那就是把它丢到CPU上去改
就是我们在自己测试
这种30万的尺表的这种
那么13B干这样一件事情
就能够有10%的性能提升
但这个10%并不是一定能拿到的
收益跟实表大小有有直接相关
因为这牵涉到一个
CPU跟GPU的联合推理的问题
也就意味着CPU执行完的结果
要拷贝到GPU去
当这是相当于多了一个成本的
多了这个成本
如果说你节省的险存
不足以去cover这个成本
那你的收益可能是负的
所以这个这个
其实会根据实际的业务所用的模型
然后去调整是否选用这样一个呃
这样一个机制来做
OK好吧
今天咱们是个推荐系统的论坛
所以我不能一直都讲大圆模型对吧
而且
对于现在推荐系统的算法同学来说
也非常的焦虑
就是大圆模型这趟车
能不能上得去的问题
所以大家都在考虑
大于原模型的这些Transformer
这些结构
生成式的结构
能不能用在推荐系统里面去
就大于
其实推荐系统的推理一直有一个问题
就是他因为他的量非常的大
耗时要求非常的高
然后他的成本
其实真的是比常规的AI推理
是要高很多的
因为模型动不动就是GB TB级
然后一个用户来了之后
然后可能要给他推上千个item
然后要耗时要求还低
然后业务请求量又大
对吧就是这就这都是成本
那当我们把常规的模型结构
变成了这个
推一个大圆模型的这种结构
然后用一个长序列的输入
然后去做的时候
我们这里面可能我们的计算量
是以上千
近万倍的量
然后再来估计的
那我还能不能以原来的这个成本
去干这件事情
就变得很重要
好我们再回过头来看这个
就说这个长
就深层式这个推荐
它的基本的
这个优化是一个什么样的任务
其实就是我输入一个历史序列的预测
然后以及对候选
然后去历史序列
去预测候选的item的action
就说我我猜我下一个item
我已经知道
我之前干过这个item干过什么事
那个item干过什么事
然后现在我给你新的一个item
然后你猜你下面用户会干什么事
是点击还是收藏还是什么样子
对吧
那其实这里面就隐含了一个事情
就是单个用户他的大量
大量有大量的item要预测
因为我们的rack
往往是以千这种级别来算
然后它的历史序
对于同一用户的历史序列
其实是一样的
那就其实是很好的
符合了这个purse catch的一个场景
我就可以对你这个用户的
所有的这些item
预测请求
做一次face catch
然后之后
就只做你item部分的一个推理
干这样一件事情就可以实现
原来我们的整个的计算量
是我的profit TOKEN
然后加上一个待遇测的item的你
然后这样一个要推你的头啃量
然后乘以item的数量
然后以每一个每一个头啃的计算量
这个其实才是
就因为这里放一是个成的关系
所以会导致我们的计算量
成千上万倍的这个增长
因为我们可能这个
这个投款可能是上万的
有可能
可能好歹大家至少要搞到上千去吧
那通过这个功能
其实就可以实现item变成了item的数量
然后加上
而这个TOKEN数量
也就说把乘变成了加
这样的话
最后这个计算量
就是跟item数量线性相关
那就跟现在咱们的正常的推荐的推理
是相似的
但这里面
其实只是解决了一个计算量的问题
其实还有一个问题
就是later say的问题
如果说我是以万的TOKEN去输入
你要想最后控制在10毫秒以下
也是非常非常的困难
哪怕业务能够接受更长时间的雷神c
那个也不是
呃可能大家从预支从10放松到50毫秒
这样的一个状态
可能要放松到秒级去
那这个地方就可以发现一个事情
item的预测
也就说这一段的预测
和这一段预测是可以分开的
我们可以提前就说
我在没有不知道it m的情况下
我已经知道了用户的序列
那我就已经可以提前计算这个东西了
也就是说
我在用户刚刚过来请求的时候
我就已经可以把这个训练发给用户了
然后等到我把item做了召回
做了出牌之后
我再去执行这部分
这部分就只有只有一个了
就只有一个item的TOKEN了
比如说这个时候才这个耗时
才是我们传统意义上讲的这个呃
一个rack的请求耗时
这个这样的话
这个house就可以做到
只跟这个头啃这个
这个item
最后这个item有关
而跟前面的这个头啃的数量无关
对就这样的话
我们就可以把整个系统完成对
跟现有推荐呃
推理系统的一个基本对齐
对那说技术内容可能就这些
然后未来的方向的话
其实也就是围绕之前的呃
整个架构的几层吧
一个是咱们常呃哎不好意思
这里有一个打印错误
一个常用的大圆模型
然后还有就是咱们业务的
Gr的推荐模型的一个支持
当然就是调度的优化层面的话
计算显存的这些流水线啊
然后包括现在
其次还有大家嗯
说的比较多的投机解码呀
这些业界的先进技术
我们都会随时去跟
然后另外就是硬件的支持
这里其实不只是在这个硬件上跑起来
更重要的
其实是在那个硬件上的定制化的
这些算子的开发
就这种加速硬件档案
就是像华为
然后咱们现在业
国内也有一些其他的公司
也在做这种
就是加速类的硬件
然后这为什么会单另一个CPU呢
其实呃英特尔他没赶上这一波
他现在也很慌
现在最新的英特尔芯片
其实已经在他的CPU盒里面
加进了矩阵计算的硬件单元
所以说这个情况下
他的机CPU
其实是可以去承担一定程度的
这种高暖高密度的几吨几几吨计算
这种类型的事情
他不会像以
前传统的CPU的时候
嗯一条一条线走到黑的
它会性能会比这个现有的状况
会好很多
行呃
哎这块就没有其他的内容了
哎好的好的
非常感谢袁老师的这个分享好
非常清晰
呃然后那个我们来问
帮问一下那个评论区一些问题哈
然后
虽然刚刚那个于老师已经讲到很多
这个LN推理和CTR推理的区别了
但是同学还是很关心
就是说呃
因为
其实如果说之前做CTR的推理的话
那可能之前做了很多
像这种显存的分配啊
动态batch啊
多流并行啊
克罗浪去啊等等这样的一些工作
那如果说从未来来看的话
那之前在CTR推理的这样一些能力
或者说经验
哪些是可以借借鉴到LM推理上的
就是这两块
CTR和LM
他们之间的这个区别和优化侧重点
有一些什么样的共性和不同
哦还是请老师再稍微再再小结下
哈哈
嗯我其实我就这里没有讲
就是说
传统的深度学习推理里面的那些优化
呃准确的说是那些优化全部都有效
只是在大圆模型这个推理场景下面
他有他的
因为常训练这件事情
就是他有他有训练输入
然后以投捆方式
去做并行的这样一个特殊性
然后他引入了一些特殊的优化方法
就说呃你包括动态batch
其实你很大程度上
呃
也会是跟我们刚才的康利斯batch这些
呃结构会
其实是有一些像
然后但实际上就是那个更更细化
就是Ctrl在
一起会比那个更细化
多牛的变形
你这些其实也都可以
在我们就是单个请求单个办起
然后去做
去做这个前向推理的这个过程
的油画里面也可以用
因为其实我刚才一直没有讲这部分
因为因为这部分
就相当于你已经办起一定了
你要生成一个TOKEN
你就是要过这个图
过这个图
这个过程
所有的以前用的优化都可以用
所有的优化的东西都要继续用下去
那只是说可能有一部分呢
就是包括像如果像我们现在这种方案
是手写算子
像图优化这件事情
因为没有图了
所以说图优化这件事情没了
但就是如果说是简单来说
用CTR模型
然后刚才提到了
因为我们目前的这个Gr模型
它其实并不是一个连续生成的模型
它不是个连续生成的
它其实对KV can起
对连续生成的这一点上的依赖
没那么重
就像这个点
嗯
就像这个
其实在咱们现有的体系下面
你说你用计算图去实现
这个逻辑也是没有太大问题的
就是这个
然后走一次前项嘛
这个走一次前项
这个再去走一次前项
只不过多了一个输入
就是那个TV cat的输入
然后图有些有些变化
其他的部分
你用计算图的
传统计算图的一些优化方式去实现
也都是没问题的
但是就是说
这个理论
就可能会牵涉到一个极致优化的问题
因为手写算子极致优化这件事情
本身就是以消呃
相当于损失应用性来拿到的
因为
因为这张图其实跟那图上会应用嘛
咱们算法同学搞完然后就上限了
但如果说是这个算这个呃
到大约模型这个场景下面
其实就是你如果有一个新模型
那就相当于你又得去支持他
重新从工程层面上来说
你就得重新把从训练
到到推理的这一套
全部的给他重新弄一遍
但这个东西
其实就还是取决于
这个模型结构本身的稳定性的问题
因为在大圆模型这个场景
这个结构现在已经真的非常的稳定
他跟推荐这个场景比起来简单太多了
推荐咱们会搞很多很多的那种SPA啊
然后小算子
然后来回拼来拼去的那些事情
在大约模型这种情况下全都没有了
只有大算子
然后就那么几个
然后在那拼
所以这个地方
其实呃我理解就是你像car
car launch啊这种类型的
还有显同拷贝这些优化的
这些东西
其实是在现有的
你换到大圆模型场景下去
全部都可以用
继续用然后只是嗯看不懂
框架嘛现在的基本上主流的呃
大圆模型推理框架
基本上都不是用我
我可以认为他可以说他不是用图
就喷上RTLM算是用图的
但其实它里面也是一大堆的大算子
然后他只是用图
你可以认为他用图串大致串了一下
这几个算大算子
OK然后还有一个问题是关于说
就是音念
现在用的这个就是连续的这个batch
之后它的这个
每个大batch中间的
这个模型的forward部分
和这个解码采样
它是不是会有一个串形的执行流水线
这样的话
是不是有可能会出现一个
GPU的一个空隙
嗯对
这个啊
这个这个问题其实提的非常好
实际上就是说你编睫毛
睫毛采样这件事情
现在目前其实主要的优化手段
还是把睫毛采样这个时间段
尽量的放短
因为现在主主要的方式
他仍然是头肯依次申请
生成的就是你那个大圆模型
它因为显存的问题
就它显存占用太大了
然后你比较难像以前一样
可能我这个buy跟那个buy可以交换着
然后做一个流水线
现在可能就是你一个BAT尽量打大
打大之后然后显存已经没了
你没有另外一个BAT的显存
来干你现下一步的推理了
所以说现在这一块基本上是串行
然后主要的优化
其实是
想办法把解码采样这一个环节
压缩了
就相当于把它压的更短
不要就那个地方的解码采样
现在很多都最后全部用GPU算子来做
而不是用呃咱们在拍摄层面去写
然后写了来来弄
所以说这这个地方
其实就是都是为了极致的压缩
因为它这整个过程现在就是个串形的
真的比以前的优化的空间小很多
嗯你说他既是串行
且他又不能够说让流水线让下一个
下一个batch的GQ计算能够进来
所以他还是说得等着前一个
那个解码采就是采访解码采样算完
所以说你们希望尽可能的用GPU去算
这个解码采样
对现在基本上是都是
主流的方案都是这样的
因
因为因为你中间就刚才提到这个coding
它是有一个状态的
那就是那个它的KV catch
你这你不可能在这么短的时间内
一个采样的时间
然后把它换出去
然后再换另外一个BAT进来
然后然后再去跑这个你是得不偿失的
嗯嗯嗯
好的好的
呃然后还有一个问题是说
依恋有
有没有做过跟这个tensrtlm的这个对比
并且就是说有没有去测过
说用A800或者说4090做推理
它的这个性价比
嗯
其实我们现在主流测其实主要是测嗯
a A800或者我们A800或者A100的有测
但是其实普通的
因为线上没有这些卡
所以说都没太去测这些
然后跟RTM的对比的话
其实呃呃
但不能不能直接对大家这么这么去说
其实我们最后的呃性能是相当
或者是甚至在看具体场景
其实是有一定的10%-20%的收益嗯
对就大致的情况是这个样子
OK这个问题他或者就这样
其实这个地方为什么会有这样的收益
其实可以看这张图
就说其实我们下面的开源算子封装
有FT的算子
就是fast Transformer的算子
有VRM的算子
有有tensrt的算子
也就是说
我们其实是集成了所有这些开源的
还有就一个是开元的大圆模型
推理框架
以及咱们的呃业务的定制的算子
就是最后是个折优的
就谁跑的快用谁
但这里面其实还有一个问题
有时候不管完全是跑得快的问题
就是因为大家做了大量的融合算子
都而且都用F16在推
这种情况下F16有精度损失
大家都知道
在业务实际应用的时候
他有些业务
可能会有非常强的这个效果
一致性的需求
比如说甚至有时候
我们其实是要把算子做退化
退化到比如说拍唾弃的算子
然后去跟他的训练做对齐
也就说这个地方嗯
所以其实TRTLM呃它大家可以去试的试
其实说它TRTLM它它会
它
其实完全用IP16的性能并不是那么好
它的性能呃好的时候就说它
但是
如果它一旦开启那个Inter 8和IP16的
这种混合
量化的这些机制之后
它的性能才上得来
OK OK
呃然后最后一个小问题
它是问的是说是如果是对于Gr
就是那个深深推理的话
它是呃更适合做一个market screen的并发
还是说也是像Lom那样
也是做一个这样的一个异步打
打败之后
再去做一个这样的一个创新执行
嗯这个地方OK
明白这这个问题我我理解一下
其实就是咱们在推荐的场景
是不是要做玛丽作用
然后多半期的去做流水线
就刚才类似刚才那个问题
为什么刚才已经解释
大语言模型情况下
为什么没有做这件事情
那是因为显存不够
但其实在G2的场景呢
如果咱们的模型大小不是很大
因为咱们现在做的大圆模型
好歹你上个13B的
那就是26G的显存就没了
但你
如果你的模型可能只有g这种级别
那你剩剩下的显寸就是足够大的
你就其实是有空间去做多半起来
对其实做这个现在这个大约模型
这个产品
目前来说
很多问题
都傍着在了减存大小这件事情上
OK OK
好的应该对回答非常清楚
好的然后我看这个评论区
也没有别的问题了
然后这个对我们时间也也也比较晚了
哦对
非常感谢这个袁老师分享
嗯
OK好
谢谢大家带来的非常精彩
