# 一念LLM大语言模型推理加速



概述： 当然就是说这个前部分的可能会比较偏大语言模型的推理
然后最后的话
会结合现在咱们最新的一些
包括推荐的GR模型和大语言模型在推理上面的一些异同点的一些地方
最后包括我们的GR这些模型在推理上面会有些什么样的一些挑战



前请提要：在讲这块的工作之前会先涉及到大语言模型推理。可能我讲的重心不会将一些之前深度学习，系统推理相关的一些优化，比如说像量化，算子的优化这种类型。更多会集中在大语言模型推理所特殊的地方来进行讲解。



### Transformer结构的大语言模型

对于大语言模型，需要先了解一些基本的Transformer的结构。在整个推理的过程中，一个token或者一个字的生成的过程大致上可以认为分成两步：

- Step 1: 根据已有的信息，也就是input的已知信息去估计下一个token的概率分布
- Step 2: 根据采样的策略，从概率分布里面挑出最有可能的下一个token

这个过程有可能是以概率最大的，偏Grady的方式来做，是要考虑到后期生成的token的一些概率，从总体上去做一个采样。这里是是跟传统的深度学习的推理其实不太一样的地方。这两步是一个循环的过程，当我生成了下一个token之后，这个token会进到下一步Step 1里面去再生成再下一个token，这么一个基本的逻辑。



### KVCache的由来

这里会引出一个经常提到的东西，即KVCache。 到底为什么会有KVCache呢？
刚才提到的在step 1的时候，是根据已有的信息，这里的已有信息包含两个意思，一个是原由的原始的输入，另一个是之后的生成的token。如果我们把一个生成的过程拆开，前面部分是最原始的输入，生成第一个token A。第二步从概率分布的逻辑上来说其实是要把前面的部分再加上a去估计下一个token，依次循环。这会导致一个问题，整个的计算量是在不停的增长的，而且就它会直接跟前面已生成的部分和input部分形成一个正比，可以想象到这样的逻辑一定会越跑越慢的。

所以在Transformer结构里面，他有一个计算的特性，当前token的结果只与前面的token有关。就可以把前面token的计算结果进行缓存。形成经常听到的两个阶段。

- Prefilling 阶段： 在Prefilling的阶段，把输入输进去后走一遍全部的过程，这是全量的走模型的过程，走完之后，会产生一些中间结果。这些中间结果被缓存起来，放入到这里标红的下一步的过程中，KVCache在进入attention之前，跟现有的新生成的token的结果做一个concat，然后再做计算。之后又是一个token生成的过程。

- Decoding阶段：通过KVCache的优化，在decoding的整个阶段的时候的计算量和前面的token数就变得无关了。这里其实是一个近似的无关。因为在其他主要的部分都是无关的，但是在attention的计算地方，是被恢复成了一个全长的token，然后进行attention。
  这是其实现在的整个KV cache为什么它会存在，就是为什么会有KV cache这样一个东西，它其实是要让的decoding阶段的计算尽量复用以前已经计算过的结果。这样的话就达到对前部分的数量没有依赖，让我们的整个推理的速度变提高。

其实在这里会有一个问题，第一把进去的时候是有很多的input的，之后每次只会输入一个。比如这里输进去的是一本书，那可能就是成百上千万的一个输入，之后的计算的数全部都是1。所以在推理的时候就会出现这样的一种现象，比如说我们按一张A100的卡，它能够就是并行推理的token数的跟GPU的TFLOPS的关系。可以看到当我的token数增长的时候，GPU会被更充分的利用起来，到达一个阈值之后，可能跟跟算值本身的时限有关，基本上到达了该GPU卡能够提供的最大的一个TFLOPS。



### 显存趋势的趋势

我们发现在prefill的运转区间，是可以把GPU给压满。但是因为后续decoding的并行每一次只只预测了一个token数，也就是的并行度非常的小，在生成的大量过程中，GPU都处于一种不饱和的工作状态。对于不饱和，其实最简单的处理方式就是做batch，把batch加大。这里面存在一个问题，如何才能把batch size加大？由于KVCache的存在，而且由于在大模型的情况下，它的KV cache占用的显存非常厉害。实际上我们会发现batch size是会受到显存的限制。

那我们就需要去看一看，显存到底是被怎么消耗掉的。在一个正常的执行的过程，包括了perfill和decoding两个阶段。正常一个模型加载之后它会占用一部分显存，之后第一把执行input token的时候，显存会有个快速的消耗，之后随着token的逐步的生成，显存的消耗在慢慢的变大，这和常规的深度学习的推理非常不一样。因为它有prefill过程和后面的生成的过程。这个消耗过程其一是跟长度有关系，其二是跟我们加的输入的长度也有关系，当我们batch size扩大的时候，这里的显存就会成倍的往上涨。

从显存角度去看这个事情的时候，可以列个很简单的公式，首先是模型占用了多少参数，然后在模型的推理过程中有很多的中间变量其实也会占用一部分参数，另外有多少token的KVCache缓存也会占一部分，最终是要小于显存的大小。

正常而言，比如一个llama-13B的模型，这种模型一上去，显存基本就固定了。这里排除量等的讨论，因为到时候对应的去改改这个公式就好了那，这不是大语言模型推理特殊的部分。


如果我们想优化的是把batch size做大：

1. 就可以通过优化这个β，将显存使用变小。这里主要是涉及到KVCache的量化技术。

2. 还有一个就是占用了KV cache的这些token的数量，这个东西跟什么东西有关呢？可以这样去列一个公式，实际上是batch内平均的token的数量。比如说现在已经输入了多少token，以及生成了多少token的一个总数乘以batch size。这里还存在一个γ的系数，其实就是batch 之中不同的请求之间它的TOKEN可复用的KVCache。简单来讲大家输入是我都是什么什么这样一个头部，所有的请求都是这个头部所对应的KVcache其实是一样的，那我们就可以把这个内容的KVCache进行复用。



## 一念LLM （KsanaLLM）名字的由来


简单的介绍一下一念LLM框架本身名字的由来。它取名是自一念三千，因为我们现在的大语言模型，因为现在的大模型就是在一个模型转瞬之间生成了很多的各种各样的不同的结果。一念本身在佛教里面是一刹那的意思，正好他对应的这个大概就是我们一个正常的13B的模型，生成一个token的时间。一念三千也代表我们的目标是想大模型在一瞬之间生成世间万象。这就是名字的由来。

因此这里会对latency，throughput这两个方向去重点的去做相应的优化。



### 一念LLM的基本框架
从上往下去介绍：
最上层就是咱们经常实现的如llama，baichuan，qwen的这些模型，与常规的深度学习的模型的推理方案不一样，我们采用的是手写模型。即我们抛弃掉了计算图，计算图以前都是用角度算值，去拼成这个模型，这最大的好处是有很多的灵活性，可以便于算法人员去实现。实际上这会带来另外一个问题，他的深度优化比较难做，以至于最后大家就会走向一个方向，去做融合算子，然后把这个融算子放到图里面去，如果符合了这个图的pattern，用这个融合算子去把它替掉。

对于Transformer结构，因为结构是非常的简单，并且以现在的情况，结构基本都开始完开始收敛，大家不再卷模型结构，更多的是卷模型效果。也就是说，这一块我们需要实现的模型的类型是比较少的，这就让手写模型和手写算子这件事情变得有利可图。

手写算子这件事尤其是像英伟达或者像Facebook等各个大厂，大家会发现，他们最后为大语言模型这个场景写的算子都非常的大。
甚至像attention这种，按现在基本就是一个标准的，用Flash Attention做一个大算子来做。

这个地方我们为什么要彻底去丢掉计算图这个事情呢？因为我们还希望的是去优化整个模型推理过程中的显存，只有已经规定好了模型，就可以盯着这个模型，仔仔细细的把里面的显存的使用全部去调好，使得整个显存使用最小化。省出来的显存，我全部都可以拿去做其他事，去做KVCache相关的事。

然后另外一个其实就是高效的调度去提高吞吐，稍后我会去讲到具体这里面业界采用的一些技术，来做这样的事情。

下面就是算子择优，就说我们的底层的这些算子，其实很多时候会是开源的封装。因为其实当模型结构相对固定之后，硬件厂商会专门针对这些大的算子进行优化，提高性能，最终目的是卖掉他们的卡。这是在大语言模型这一块，现在每个厂商都在干的事情，甚至为了不同的tensor的大小，去写不同的算子。因为这样的话就可以拿到一些收益的，因为这个模型，是大家都现在都在试着去用，所以这个收益的投入产出比还是不错的。

下面其实就是一个多硬件的支持，现在业界的主流的框架，基本上都是英伟达偏多，当然也有支持英特尔的CPU。然而像支持GPU和华为昇腾等国产的这些卡的，整个生态其实是并没有那么的完善的。从国内厂商的角度来看都会面临一个问题是高性能的GPU卡进不来的一个问题。从业务安全的角度，我们也是必须要去支持不同的硬件。

但是为什么不是按不同厂商使用不同框架，比如英伟达用一个框架，华为用华为的框架，两边的都能用到一个好的收益？实际上，当你在做这一层优化的时候，你会发现面临不同的框架，需要重复做的问题。另外各个框架本身对后期的业务的更上层的一些逻辑的适配，本身也会存在一个表达。

我们通过上面统一框架，下面支持多种硬件，这样就相当于做到调度这层一次优化在所有硬件上都可以用，去实现这样一个目标，这样对于整个平台本身的可运营性也是有很大的好处的。



## 调度的部分

### ContinuousBatching 和PageAttention优化BS

第一个问题其实可能是大家或多或少都了解到ContinuousBatching 和PageAttention，从我们最原始的那个公式来看它其实就是在有效的优化batch size。

正常情况下，我们会把不同的请求打成一个batch去做GPU的推理，这个过程往往是一个batch一个batch的进去，要等到这个batch里面的请求全部处理完整个才退出。从GPU的调度层任务调度上来说这是最简单的使用方式，但这种使用方式最大的一个问题在于它的有效batch会越来越低，因为每个请求的输入长度不一样，输出长度不一样。比如有可能第一个请求是一个摘要的任务，会丢入一个1,000字的文章让大模型用一句话总结出来；第二个请求可能是一个扩写的任务，给了一小段话，让模型把它扩写成一边长文。也就说输入输出的不匹配，会导致有些请求很快就结束了，有些请求还要跑很久。这样的话，要等到所有的请求都完成，这个batch才会退出。这会导致有效的batch到后面越来越小。

同时这个过程就会导致另外一个问题，后面很多请求结束了，GPU的计算力就闲置。

很容易想到我们能否当一个请求完了之后，就把另外新的请求给塞进去，这就是ContinuousBatching的一个想法。这个想法其实早在22年就已经被微软提出来，但是这么好的一个想法一直都没爆发，是因为里面存在一个问题，就是它的KVCache的显存的操作成本比较高，比较麻烦。去年由伯克利提出的PageAttention把这个问题给解了，于是整个这个ContinuousBatching+PageAttention的机制就迎来了一个爆发期，现在成为了大语言模型推理框架的一个标配功能。

简单讲因为整块操作成本高，就将它切小，它采用了常规操作系统里面的内存分页的机制来管理显存，把KVCache切成不同的块，用指针去引用的方式，来进行计算。这样会显著的降低了显存操作的粒度。整个的对ContinuousBatching这个调度而言，它的操作效率就上来，整个收益就上来。刚才提到的ContinuousBatching，如果按这张图去看这件事情，其实就是最早的通用方案，一个batch、一个batch的往下走。
可以明显看到的，上面提到的有效Batch在不断的缩小。因此ContinuousBatching解决的问题，其实就是将整个GPU填的更满。



### Prefix Caching 优化γ

这里面还遗留了一个问题，就是input在请求与请求之间的共享问题这在以前的深度学习的推理里面很少关注到。但是在推荐里面实际上以前是比较早的时候，也已经做过类似的事情。比如一个用户的多个item的rank操作，因为用户都是一样的，所以这一部分的推理只需要做一次。然后不同的item的推理多推多次，再把它融合到一起，把它拼到一起再去做后面的推理工作。这在推荐领域前两年就已经干过的一些事情，但在大语言模型推理里面，这个事情一直就这样没有干。
但他有什么特点呢

就是请求的input是有共性的，这些共性的请求，其实可以只算一次，然后把计算结果的缓存，就可以把KVCache存起来，等到下一个请求，只需要处理后面的。这样的话，在以不同的请求之间这部分就只算了一次。还可以有batch和batch之间仍然可以继续复用，这样整个后期请求的推理响应就可以一下提上去。

这里还会扯到另外一个问题，KVCache这个机制看起来非常的美好，但是也是有成本的，因为KVCache会占用显存，而且一旦缓存的大，就会面临cache换入换出和命中问题。比如放两个前序在显存里面，就得占两份的显存。最终在具体的执行节点上面，命中率就决定了这个机制最终的收益。所以prefix catching，更多就相当于上面公式里面的γ。

其实我们在服务节点的前置加了叫prefix-token的一个路由器的。在这里，路由需要平衡两件事情，命中率以及传统路由的这些问题，包括负载平衡，熔灾等问题。例如，如果前序都是一样的，就直接打到一个节点上去，它一定会被命中。但是实际上还是会面临负载平衡和熔灾的事情怎么解决的问题。所以我们是在构建了一个路由表，例如经常看到的一些角色扮演的服务，比如正在跟宋江聊，其实需要给模型的东西包括，第一告诉模型你现在正在扮演的是宋江，宋江是一个什么样的人，之前的生平，有什么样的能力，他的性格等一些重要的事件，就跟用户简历一样的东西。然后再说你跟宋江之前聊过什么，下面请生成你准备作为宋江回复给用户的信息。这个过程里面有大量的信息其实是跟用户profile一样。还有另外一种，比如作为爱因斯坦或者牛顿这样的角色，对不同的角色，在进到模型之前，我们就会把前面的这一段做一个prefix-token。对相同的一段给他指定一个具体的路由表，在有限的机器里面去选中一个机器集合。

另外刚才也提到一个问题，我们需要解决不能有太多份的cache，cache份数多了，显存里就全是cache。没有显存去计算当前要执行的请求了。我们通过另一个维度的管理，对于单一的节点，相当于是一个server-set其实是有限的。从这张图可以感觉到，最后每个节点它只命中了两个set，从而达到就说对单个节点来说，cache份数是相对可控的。然后又能够在set的维度完成负载平衡和熔灾。



### CPU/GPU混合推理 优化M

最后我们再讲一下，CPU跟GPU的混合推理，其实就是优化M。
前面提到过，把模型从FP16变成INT8或者INT4，这种是量化，它是效果有损。从框架层面需要支持，业务是否使用得业务去评估。我们这讲的是无损的一些方式，计算密度的问题。计算密度这个东西，可以看到在大语言模型里面，我们一般讲的大语言模型是计算强密集的，其实往往是讲的是这部分是它的Transformer结构部分。实际上它还有一个token embedding部分，这部分实际上就是查表，类似在推荐里面的一个sparse操作。
但往往这个部分又是放在GPU上做的，这个表一旦大了就会占显存。而且可以看到一般的我们在业务应用的时候，很多时候都会扩token，也就是扩词表。因为原始的模型里面的那些词，跟我们业务里面的词，有时候并不是完全覆盖的。有可能业务里面会有一些特殊的自己业务场景的这些词，最后比较开源的模型，比如标准的llama-13B在v2的时候是3.2万的词表。但实际上业务的时候，就慢慢就扩词表了。
就我们这以在llama-13B来看原生的词表可能只占整个参数的百分之一。
但是如果我们把它扩到
百分三十万的词表的时候
那其实它对整个的模型参数量的占比就会到11.8%，即11.8的显存。
而它又是个sparse操作
那很直接的一个想法
那就是把它丢到CPU上去改
就是我们在自己测试
这种30万的词表的llama-13B干这样一件事情就能够有10%的性能提升。但这个10%并不是一定能拿到的收益，跟词表大小有有直接相关。
因为这牵涉到CPU跟GPU的联合推理的问题，也就意味着CPU执行完的结果要拷贝到GPU去，这相当于多了一个成本。如果节省的显存不足以去cover这个成本，收益就可能是负的。所以这会根据实际的业务所用的模型去调整是否选用这个机制。



## Generative Recommendation

对于现在推荐系统的算法同学来说，大家都在考虑大语言模型的Transformer，生成式的结构，能不能用在推荐系统里面去。其实推荐系统的推理一直有一个问题，就是因为他的量非常的大，耗时要求非常的高推理成本其实比常规的AI推理要高很多。模型规模动不动就是GB、TB级，一个用户来了之后可能要推上千个item，耗时要求还低，业务请求量又大，这都是成本。
那当我们把常规的模型结构，变成一个大语言模型的这种结构，然后用一个长序列的输入去做的时候
这里面可能我们的计算量是以上千近万倍的量来估计的。

那我还能不能以原来的这个成本，去干这件事情就变得很重要。
再回过来看生成式推荐它的基本优化是一个什么样的任务。其实是基于输入一个历史序列的预测，以历史序列去预测候选的item的action。就是我已经知道这个item干过什么事，那个item干过什么事，然后现在我给你新的一个item猜你下面用户会干什么事，是点击还是收藏。

那其实这里面就隐含了一个事情就是单个用户大量的item要预测。因为rank往往是以千这种级别来算。对于同一用户的历史序列，其实是一样的，那就其实是很好的符合了这个prefix catching的场景。可以对这个用户所有的item预测请求，做一次prefix-cache，之后只做item部分的推理。这可以实现原来整个的计算量
是我的prefix-token然后加上一个待预测的item这样一个要推你的token量乘以item的数量。等以每一个token的计算量是乘的关系，所以会导致我们的计算量成千上万倍的这个增长，因为这个token是成千上万的。
通过这个功能，其实就可以实现item变成了item的数量加上TOKEN数量，也就是把乘变成了加。这样的话最后的计算量就是跟item数量线性相关，就跟现在咱们的正常的推荐的推理是相似。
这里只是解决了一个计算量的问题，还有另一个问题，latency的问题。
如果说我是以万的TOKEN去输入，想要最后控制在10毫秒以下也是非常非常的困难
哪怕业务能够接受更长时间的latency，也不是将阈值从10放松到50毫秒这样的一个状态，而是要放松到秒级去。这个地方可以发现一件事情，对于item的预测，这一段的预测和这一段预测是可以分开的。在不知道item的情况下，就已经知道了用户的序列，可以提前计算。也就是说在用户刚刚过来请求的时候，就可以把序列发给用户了，然后等到把item做了召回初排之后，再去执行这一部分。这部分就只有一个item的TOKEN耗时才是我们传统意义上讲的一个rank的请求耗时，这个耗时就可以做到只跟这个token的最后item有关而跟前面的prefix-token的数量无关。这样的话就可以把整个系统跟现有推荐的推理系统的一个基本对齐。

## 未来方向

对那说技术内容可能就这些
未来的方向其实也就是围绕之前的整个架构的几层，
一、对模型的支持：常用的大语言模型；业务的GR的推荐模型的支持。
二、调度层面的优化：计算/显存的流水线，包括现在大家说的比较多的投机解码等业界的先进技术，我们都会随时去跟跟进。
三、就是硬件的支持：这里其实不只是在这个硬件上跑起来，更重要的其实是在那个硬件上的定制化的这些算子的开发。例如华为等国内也有一些其他的公司也在做这种加速类的硬件。这里为什么会单拎一个CPU呢？因为英特尔没赶上这一波，现在最新的英特尔芯片，已经在CPU盒里面，加进了矩阵计算的硬件单元。这个情况下，CPU其实是可以去承担一定程度的高密度的矩阵计算这种类型的事情。不会像以前传统的CPU的时候，一条线走到黑的，它会性能会比这个现有的状况会好很多。





## Q & A
Q： 
因为
其实如果说之前做CTR的推理的话
那可能之前做了很多像显存的分配啊，动态batch啊，多流并行啊，kernel launch等等这样的一些工作
那如果说从未来来看的话
那之前在CTR推理的这样一些能力
或者说经验
哪些是可以借鉴到LM推理上的
就是这两块CTR和LM他们之间的这个区别和优化侧重点有一些什么样的共性和不同，请老师再稍微小结下。

A：其实我就这里没有讲传统的深度学习推理里面的那些优化，准确的说是那些优化全部都有效。
只是在大语言模型这个推理场景下面，因为长序列这件事情，就是他有序列输入，以token方式去做并行的这样一个特殊性，引入了一些特殊的优化方法。包括动态batch，其实很大程度上也会是跟我们刚才的continuous batching这些结构是有一些像，然后但实际上就是那个更细化。就是continuous batching会比那个更细化，多流的并形其实也都可以在我们就是单个请求单个batch，然后去做前向推理的过程。
优化里面也可以用
因为其实我刚才一直没有讲这部分
因为因为这部分
就相当于你已经batch一定了
你要生成一个TOKEN
你就是要过这个图
过这个图
这个过程
所有的以前用的优化都可以用
所有的优化的东西都要继续用下去
那只是说可能有一部分，如果像我们现在这种方案是手写算子。像图优化这件事情，因为没有图了，所以说图优化这件事情没了
但就是如果说是简单来说，目前的GR模型它其实并不是一个连续生成的模型，它不是个连续生成的，其实对KVCache连续生成的这一点上的依赖没那么重。就像这个其实在咱们现有的体系下面，是计算图去实现。
就是这个
然后这个走一次前项，这个再去走一次前项
只不过多了一个输入，就是那个KV cache的输入
然后图有些有些变化，其他的部分，用传统计算图的一些优化方式去实现也都是没问题的。
这个理论，就可能会牵涉到一个极致优化的问题，因为手写算子极致优化这件事情本身就是相当于损失应用性来拿到的
因为这张图其实跟那图上会应用，咱们算法同学搞完然后就上线了。但如果到大语言模型这个场景下面，如果有一个新模型，就又得去支持他，工程层面上来说，得重新把从训练到推理的这一套，全部的给他重新弄一遍，这其实就是取决于这个模型结构本身的稳定性的问题。
因为在大语言模型这个场景，这个结构现在已经非常的稳定，他跟推荐场景比起来简单太多了。
推荐咱们会搞很多很多的那种sparse，小算子来回拼来拼去的那些事情。在大语言模型这种情况下全都没有了，只有大算子，然后就那么几个在那拼。在这个地方
其实呃我理解就是你像kernel launch啊这种类型的
还有显同拷贝这些优化的
这些东西在现有的大语言模型场景下去全部都可以用。现在的主流的大语言模型推理框架基本上都不是用用图。除了tensorRT LLM算是用图的，但其实它里面也是一大堆的大算子，然后他只是用图大致串了一下这几个大算子



Q：一念现在用的这个就是连续的这个batch
之后它的这个
每个大batch中间的
这个模型的forward部分
和这个解码采样
它是不是会有一个串形的执行流水线
这样的话是不是有可能会出现GPU的一个空隙

A：这个问题其实提的非常好
实际上就是说你编解码，解码采样这件事情，现在目前其实主要的优化手段，还是把解码采样这个时间段尽量的放短
因为现在主要的方式仍然是token依次生成。当前大语言模型，因为显存的问题，显存占用太大了，比较难像以前一样。
可能我这个batch跟那个batch可以交换着
然后做一个流水线
现在可能就是你一个BATch尽量打大，打大之后然后显存已经没了，你没有另外一个BAtch的显存来干下一步的推理了
所以说现在这一块基本上是串行，然后主要的优化，其实是想办法把解码采样这一个环节压缩了。
就相当于把它压的更短，不要就那个地方的解码采样，现在很多都最后全部用GPU算子来做
而不是用咱们在python层面去写，写了再用。所以说这这个地方，其实就是都是为了极致的压缩，因为它这整个过程现在就是个串行的，比以前的优化的空间小很多。



Q：就是说他既是串行
且他又不能够说让流水线让下一个
下一个batch的GQ计算能够进来
所以他还是说得等着前一个
那个解码采就是采访解码采样算完
所以说你们希望尽可能的用GPU去算这个解码采样

A：对，现在基本上是都是主流的方案都是这样的

因为你中间就刚才提到这个decoding
它是有一个状态的，那就是那个它的KV cache，你不可能在这么短的时间内（一个采样的时间）把它换出去，再换另外一个batch进来
然后然后再去跑这个你是得不偿失的。

Q：一念有没有做过跟这个tensorrt-llm的这个对比。并且就是说有没有去跟A800或者说4090做推理。它的性价比如何？
A：其实我们现在主流测其实主要是测。A800或者A100的有测，其他普通的
因为线上没有这些卡
所以说都没太去测这些
然后跟tensorrt-llm的对比的话
其实呃呃
但不能不能直接对大家这么这么去说
其实我们最后的呃性能是相当
或者是甚至在看具体场景，其实是有10%-20%的收益。
对就大致的情况是这个样子

其实这个地方为什么会有这样的收益
其实可以看这张图
就说其实我们下面的开源算子封装，有fast Transformer的算子，有vllm的算子，有tensort的算子。也就是说我们集成了所有这些开源的大语言模型推理框架，以及咱们的业务的定制的算子。但这里面其实还有一个问题，有时候不管完全是跑得快的问题，就是因为大家做了大量的融合算子都而且都用F16在推
这种情况下F16有精度损失
大家都知道在业务实际应用的时候，有些业务可能会有非常强的这个效果一致性的需求
比如说甚至有时候
我们其实是要把算子做退化，到比如说pytorch的算子去跟他的训练做对齐
也就说这个地方嗯
所以其实TensortRT-LLM它大家可以去试的试
其实说它TensorLRT-LLM它其实完全用FP16的性能并不是那么好
它的性能呃好的时候就说它
但是
如果它一旦开启那个Inter 8和FP16的
这种混合
量化的这些机制之后
它的性能才上得来





Q 如果是对于GR生成推理的话是更适合做一个multi  stream的并发，还是说也是像llm那样也是做一个异步大batch之后，再去做一个这样的一个串行执行？

A：在大语言模型情况下为什么没有做这件事情，那是因为显存不够。但是在GR的场景，模型大小不是很大，像现在做的大语言模型，13B的规模那就是26G的显存就没了。但如果模型可能只有G这种级别，剩下的显存就是足够大的，你就其实是有空间去做多batch。对于现在这个大语言模型这个场景，很多问题都暴露在了减存大小这件事情上。