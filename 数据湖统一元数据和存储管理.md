大家好我是范佚轮，来自于阿里云。
目前是负责这个，阿里云数据湖构建这款产品的研发
今天给大家分享的主题是数据湖统一元数据与存储管理

主要是介绍一下我们阿里云的数据湖架构，以及我们在元数据的存储分层相关的优化和实践。

首先先谈一下我们数据湖相关的概念和架构
大家还是回到我们数据湖的概念上，就是什么是数据湖。
我们可以看到不同的云产商其实也有不同的定义。
但是基本上从关键词上来看，还是围绕这几个特性和目标：

一个是统一存储，就是说数据湖是一个统一的中心化的数据存储。
然后另外的话它是可以用来放一些原始数据的。
然后它是支持多种格式的，包括结构化的数据和非结构化数据。

首先第一个统一存储，其实是为了解决这个数据孤岛的问题
因为传统的数据库或者是数据仓库在设计上是存算一体的。
也就是说不同的查询引擎之间，数据是需要经过清洗和同步的。
那不管是在存储空间上，还是效率上，其实都有一定的浪费。
那数据湖场景呢，它是怎么解决这个问题的呢？
它其实是用存算分离的查询引擎的，典型的比如hadoop生态的hive和spark。
然后加上开放的存储格式parquet、orc这些，然后让不同的引擎同时可以查询同一个数据。
这样就是我们早期的数据湖的一个架构。
然后另一方面在存储实现上，其实数据湖通常会使用这种扩展性比较高的，廉价的存储。比如HDFS，或者云上我们会用OSS或者S3这种对象存储。
这样的话呢大家可以就把这种更多的原始数据，非结构话数据可以直接放入，避免原始数据的丢失。
那为了能够读取这些原始数据，计算引擎通常也是支持这种schema on read的方式，也就是采取事后建模的高灵活性的解析方式。



那对数据格式就没有很强的约束，因为这种灵活性，其实也带来一些弊端。
就是数据湖他缺少了一些功能，比如说他的高度开放性，他对安全和权限的管理照比数仓是有所差距的。
另外就是包括他的事务性，因为他开放的存储，并发写入的场景尤其是流式写入的场景，事务对ACID的要求会更高。这也是我们这些数据湖格式诞生的一个重要原因。
那有没有一种办法使我们能够利用上数据湖这些优势，也能让数据湖拥有数仓的这些功能特性呢？
因为前两年databrick提出的这个Lakehouse湖仓一体，其实就是要让这个数据湖能够去实现更多数仓的企业级能力。就是说要像使用数仓一样使用这个数据湖。这个lakehouse概念是在数据库的基础之上的，其实就是添加了几层内容。首先在存储上层，他其实是做了一个元数据的统一，就是matedata统一。
对上层提供统一的元数据结构化SQL的这么一个接口，然后让不同的应用，可以用相同的元数据访问数据。
另外的话，为了在性能上，我们也支持了一些cache，来解决数据湖读取性能。
另外的话，很重要的，还是数据湖格式这个事务层。
因为我们
呃目前很火热的这个数据湖格式，delta lake然后hudi还有iceberg
呃我们现在提到数据湖很多场景下
甚至跟这个数据湖格式已经划成等号了。
当然他是有些夸张了，但足以证明他在这个数据湖架构中的一个重要地位。





然后底层在数据湖存储的实现上，相比于HDFS，目前在云上也有一个使用对象存储作为数据湖存储的这么一个趋势。因为云上对象存储它的扩展性相比于你自建HDFS要高很多。然后不管是在成本上，还是在可用性上其实都是会高一些。所以说我们提到数据湖存储目前很多也是在解决云上对象存储的问题。







OK然后前面两页我们提的是数据湖的概念和它的一些架构
与之对应的我们阿里云云上也是做了一些类似的产品和功能帮助用户去使用这个数据湖架构
首先我们云上的这些大数据引擎都是可以进行数据湖分析的
就是常见的可以用EMR的spark和hive，Presto啊StarRocks这些引擎
然后也可以用阿里云自研的引擎，比如maxcompute hologres都是可以进行湖上数据分析的
也就是说我们指的是OSS上的这个数据分析的
我们可以根据合适的场景来选是选择这些引擎
另一方面这些引擎为了能够无缝的对接湖上的结构化数据呢
DLF提供了统一的元数据和湖上的权限管理，作为整个lakehouse架构里的元数据管理层
这一块我会在接下来重点介绍一下我们这个data lake formation就是DLF这个产品
然后在存储层这一方面，因为云上的对象存储OSS是天生适合做数据湖存储的
然后前面也提到了它的成本也不高啊
另外的话，现在OSS也支持了兼容HDFS接口的产品，它叫OSS-HDFS就是完全支持HDFS接口的，那更适合对接一些老版本的大数据引擎之类的







对这个是我们阿里云在云上的一个数据湖架构的一个实践
我们在构造这个数据湖架构的时候可能会遇到一些问题和挑战
尤其是在这个元数据管理和湖上存储管理这几个方面嘛
然后阿里云上的DLF产品
其实就是为了专门解决这个问题的
啊我这里先简单概述一下，后面还会详细介绍。
我们的核心能力是做了一个全托管的统一元数据服务

为什么说这个元数据服务在数据湖里比较重要呢？因为咱们把数据已经放在数据湖上了，那这个元数据其实也需要一个中心化的管理才能实现多个引擎的无缝对接。
这样的话不同引擎它读写同一份数据是围绕统一的schema去做操作的，而不是每个引擎都要建外表之类的。那围绕这个元数据，我们提供对数据的细粒度的权限管控。
另外数据湖上的一些存储管理的功能我们也都提供了，下文会介绍到。






首先先介绍一下我们第一个重要能力就是数据湖上的这个统一元数据
在开源大数据体系里，我们知道从早期的map reduce到类似SQL查询语言的hive诞生出来之后。
其实hive逐渐就成为了开源数仓的一个事实标准
那围绕着hive的元数据hive metastore 也成为了对接开源数仓的这个元数据的一个标准
那从此以后其实各个引擎啊包括spark、presto等等其实都是支持对接hive metastore
围绕着hive metastore做这个元数据的
所以目前来讲，大家自建元数据，也基本上是用这么一套实现。
hive metastore其实是一个常驻的无状态的服务，它可以部署多个实例，可以部署一个也可以部署多个。
大数据引擎通过thrift协议连接hive metastore做元数据的读写的。
然后hive metastore它的元数据本身是需要挂一个数据库，通常大家会用这个mysql作为这个hive metastore的元数据底层存储。   
这样就形成了通常来讲的开源大数据常见的元数据体系
不过我们使用这个hive metastore去管理元数据也存在一些问题和挑战了。
首先在功能层面上他是没有做多版本的，他不能追溯之前的元数据版本
然后包括一些acid的特性和LOC这些接口。
另外他是和hive引擎绑定的，在湖上多引擎的场景下，是没有办法利用到它的一些功能的。
另外它暴露的接口是thrfit的协议，如果你自己有一些服务要对接，或者自研了一个引擎要去对接会相对麻烦一些。
然后有的时候你可能需要直接连mysql去读一些元数据，但这也是一个比较好的一个方法。
最后的问题就是他存在性能瓶颈，存在单点问题和运维成本，尤其是在元数据量比较大的客户，这是一个比较常见的问题，
因为单点的hive metastore，还有metastore后端连接的mysql接口都可能会成为瓶颈，需要一些性能调优的工作。
具体来讲，我这里列了几个日常碰见的一些真实的客户问题。
在hive metastore使用的过程当中，首先第一个是jdbc连接的问题，因为hive metastore需要连接后端的数据库，通过jdbc连的时候可能会遇到一些错误，比如说有的时候我们查询元数据的所有请求都突然变慢了。
这个时候我们第一时间想到要检查一下你mysql的状态，看一下mysql监控是不是有慢SQL，
因为如果分区数总量很大的话，mysql表他可能上千万，就是会查询的比较慢，这个时候，需要做一些数据清理，删一些分区啊来缓解这个问题。
另外的话mysql连接数也可能会带来一些连接上的异常，因为很多外部系统是需要查元数据的。
然后就是自建的一些数据管理系统之类的通常不会用thrift协议去调hive的metastore server，都会直连JDBC，然后连接数多的话，也可能会带来一些额外的压力。
另外的就是内存方面，这个hive metastore server的内存也存在oom的风险。
因为有些操作，比如list partition，它会加载全部分区对象。
如果说有人写了一个糟糕的查询，比如在一个很大的分区表上，他没有加分区查询条件，就可能会拿到上百万的分区。
然后导致整个hive metastore内存full gc或者oom这种情况，一旦hive metastore出问题，整个集群的作业都会受到影响。
然后后面列举几个呢
就是这stackoverflow也是我们见到过的
如果你drop partition的分区数量很多的话，在hive metastore的内部实现是递归的，可能会堆栈溢出报错，直接执行不了。
然后嗯包括可能会超时，因为HMS的客户端是全量返回的，没有一个分页的设计，所以在拉取元数据的时候，可能会出现超时的情况。这也是一个风险点。这些就是我们在使用HMS时候遇到的一些问题。

那在云上，我们的DLF就是data lake formation上也提供了这种全托管的元数据服务，我们采用的是完全不同的架构，这种架构可以解决上面大部分问题和痛点。
首先来看我们作为一个云产品，是通过标准的open api去暴露接口，api提供了兼容hive 2和 hive 3
matestore接口的client。这个client呢它是可以直接替换掉引擎的hive metastore client实现类的
这样我们原本访问hive元数据的地方可以直接替换为我们客户端的实现类，实现了无缝对接。
另外除了开源体系的这种引擎以外，我们也对接了阿里云上的其他大数据引擎，包括这个max compute、hologres、Flink等等。云这上其他大数据引擎也可以利用我们这个统一元数据来进行这个元数据管理。这样的话其实真正做到统一的catalog，就是用一个引擎写入，比如用flink 入湖，然后入完的可以直接用SPARK查，查完之后也可以用Hologres等等做OLAP分析，可以直接采用同一个元数据来进行分析。
我们元数据服务底层实现是用阿里云表格存储，不是像HMS用的mysql，mysql扩展性还是差一些的。底层用的这种表格存储也是阿里云的一种服务，面向海量数据它有非常强的伸缩能力，扩展性很高。所以不用担心分区数过大会带来的一些扩展性问题。
因为我们整个也是一个全托管的服务，对使用方来说可以SLA，保证高可用的问题，前面提到的运维问题也是可以避免的。

总结一下，我们这个统一元数据一方面因为是全托管，会减少很多元数据的运维成本，另一方面，是真正对接云上多引擎的这么一个元数据。



具体再补充一些关于元数据实现本身的细节，
首先我们元数据的客户端是兼容hive metastore 的行为。我们实现了hive metastore的一个接口
可以直接去对接hive生态相关的大数据引擎。然后hive metastore它本身内部的有些行为，比如说在创建partition的时候，可能会统计table size等等这些动作，我们都会保留在这个客户端里，所以不用担心有些接受性问题。
另外的话，客户端会做一些性能优化，包括异常重试，
然后硬化读取啊之类的请求提议？？？？？ 我们也会做一些性能的压缩
在服务内部，除了刚才提到的存储层的高扩展性以外，我们也通过一些自动的分区索引，再做一些分区过滤的性能提升。
总体来讲在元数据的性能上，
我们在一些小表上可能跟RDS有些差距，但是不太明显。
在大分区表上，比如单表有300万分区的场景下，我们的查询的性能会有比较明显的优势。
比如说300万分区表，在分区全部命中的条件下，这个list partition by filter在我们的元数据可以在0.5秒内返回，如果说RDS的话因为他的分区值可能没有索引，是需要花5秒左右时间才能返回的。
在元数据的功能上我再举几个例子，
一个是元数据多版本，我们会记住元数据的所有的每一次更新的前后状态，可以看到什么时间点你加了什么字段，是谁改的。
我们有一个比较好的回溯机制，包括元数据审计（这个Hive也有），包括元数据检索
我们的元数据本身会把内容同步到ES搜索引擎里，对外暴露。你可以通过字段搜表，也可以全局搜索


OK
谈到元数据实现相关的话，我们再看一下权限相关的一些问题。

如果我们要在开源大数据场景，做到用户级别的权限控制，通常有这么几种方案。
hive它其实本身是提供了认证的能力的，它有storage-based authorization还有sql-standard-based authorization。但是hive的实现其实都是跟hive引擎绑定的。我们通常用其他引擎是无法使用到它的功能的，基本上也没有人真正在用吧。
大家常见是用Ranger去做权限管理，ranger是一个通用的多引擎方案，它可以对sql进行这个权限管理，也可以对文件系统做权限管理。它的原理是从ldap同步用户信息，提供UI供用户配置权限。
在大数据引擎这一侧，可以加各种插件，他通过插件来实现权限的拦截和检查。
ranger是目前一个可行的方案，只不过一方面他虽然可以对接开源引擎
但是在公有云上我们自研的大数据引擎，是没法支持对接的。
另一方面虽然它包括Spark sql的这些插件，他官方的支持并不好，更多还是需要自研一些插件，或者找第三方插件，就是说部署起来没有那么简单。
关于权限这一块，DLF统一元数据也提供了鉴权的能力。我们的权限默认是没有开启的，因为不一定所有用户都需要，但是用户可以是按catalog级别进行开关，catalog是基于database之上的一层管理模型。如果我们基于catalog设置了权限之后，管理员就可以在我们的控制台进行具体的授权了。包括database、table、column、function这些粒度都可以进行授权，然后可以设置不同action的权限。可以只给某一个人设某个table的select权限，但是不给他insert的权限，这种是可以进行设置的。然后我们也支持RBAC，我们可以把权限包在role里，统一赋权给一堆用户，这些基本的能力都是具备的。然后在鉴权环节的实现上，我们提供了两个层面的鉴权，第一层面是元数据的API。
就是说我想要查看这个table或者create table，那这种动作我们会在服务端鉴权。 
因为我们的云服务直接会去鉴权，你发送请求的用户角色是不是有相应动作的权限。如果没有的话就会进行拦截，另外因为有些SQL操作在元数据层面感知不到，比如说元数据可能就是查一张表
但是我其实并不知道你是在往里写数据还是在读数据，这个时候和ranger类似，我们也提供了引擎的插件。可以放在spark、hive上做一些拦截器，和ranger类似，他也会在内部去检查代理用户到底有没有select权限，没有的话去做拦截。所以说是个两层的鉴权模型，适用于不同的场景。

再介绍一个额外的功能，就是元数据本身在云上也好，或者说是自建的mysql的元数据也好。如果想要迁移的话，是需要一个迁移的过程。为了方便这个过程，我们在产品上做了元数据迁移，在控制台上就可以做数据迁移。简单来讲就是我们会去连远端的mysql数据库，如果这个数据库在阿里云VPC内，会自动打通网络，通过JDBC直接拉取元数据，转换成我们云上的DLF元数据，这个是直接产品化的。
除了导入需求，可能还有些导出的需求，包括两边元数据对比的需求。这些我们也提供现成的工具直接使用，在元数据迁移方面，不管是导入导出还是其他方面的需求，我们都保持开放性，不需要担心元数据被绑定的一些问题。

除了元数据迁移模式，可能在有些场景下还需要做这种元数据抽取，快速构造出湖上的元数据。
元数据抽取它适合于一个什么样的场景呢？比如说我们数据湖上已经有一些数据文件了，这个数据文件可能是从其他数仓拷贝过来的，或者是一些零散的CSV数据集文件等等。这个时候我们没有这个表的元数据，就需要用ddl语句自己去建表，再做查询，会比较麻烦，也容易出错。尤其是对于像json这种半结构化的嵌套类型，更难去写这个建表语句。这种情况下使用我们这个元数据抽取功能就比较方便，可以直接把元数据给推断出来。用户只需要填一个这个OSS路径，我们会根据他的路径格式自动扫描下面的表，包括分区值，创建好之后，我们就会写入到这个元数据里进行直接查询了。包括各种格式，CSV、json、parquet 、orc，也包括湖格式都是可以识别出来。另一方面因为我们做格式推断，需要扫描所有数据，会比较耗时，于是我们采用快速采样的方式。这些就不详细展开了。



刚才提到元数据相关的一些内容，接下来介绍一下我们在数据湖存储方面分流上方面做一些管理和优化。
首先我先介绍一下元仓，我们除了元数据服务本身之外，也做了一个元数据仓库。元仓是我们在元数据存储之外做了一个在线的元数据的数据仓库。因为我们在线的元数据存储需要做在线服务，它需要比较高的读写事务保障的。有些后台分析，包括一些聚合查询是不适合在这里去做的，于是我们做了一个实时的元数据仓库。元仓底层是基于这个max compute和hologres实现的，它会收集元数据的变更信息，也会收集计算引擎的查询的一些写入的信息，包括我们存储上的信息都会实时收集到。这样的话，我们就形成围绕database的table partition做的指标库，即dataprofile指标。拿到这些指标之后，我们会把这些指标通过标准的API暴露出来。一方面可以在控制台上可以做统计分析，包括对接我们的一些云产品，像dataworks之类的，也可以做一些数据展示和预估。另一方面这些指标可以用来做存储生命周期的优化和管理。

接下来我介绍一下这个data profile指标的几个实现：

举几个例子，首先提到这个表和分区的大小，这是一个比较基础的属性。通常来讲其实表和分区大小是写在元数据，就是Hive元数据的table properly里面。本身他就定义了计算引擎，会在创建表或者分区的时候写入。但是不同引擎写入的标准key会不一样，比如hive是叫total size，spark它有spark开头的一个属性值。另外的话，这些写入也是需要一些参数去开启的，你不开启是不会写入。所以在实际情况中我们会发现元数据本身存储的表大小是不准确的。

在元仓里，因为我们默认了这个大部分数据湖默认是使用OSS的，我们会通过OSS的底层存储来获取表分区的大小，这样可以最大限度保证数据的准确性。因为OSS提供了一个存储清单，是t+1更新的，我们这个元仓有点像LAMBDA架构，会t+1的去更新存储清单的表和分区的存储大小。另外对于实时表和分区的变更，我们会监听到
然后会实时的再从OSS那边拿到最新的大小去做更新。也就是存量加增量的流程去做表分区的大小，然后拿到大小之后，会每天产出一些分析报表。比如表的存储排名，文件大小占比等等。因此我们可以看到哪些表，哪些分区的存储占用比较大，去做相应的优化。这是一个比较完整的湖上的管理视图。

另外再介绍两个关键指标：

第一个是表和分区的访问频次，通过访问频次可以鉴别那些仍然在用的表但是访问不频繁。这些表可以在oss底层置为这种低频存储，低频存储是可以照常读取，也可以节省一些成本。那访问频次是如何获取的呢？在原理上还是使用引擎的hook来实现的。我们会解析SQL的plan，拿到它读取的表和分区，拿到这些信息之后会提交到元数据服务里去做一些记录。然后会把指标统计出来。
第二个指标就是最后访问时间，表和分区的最后访问时间。他可以用来识别这个表和分区是否有人在访问。为了保证指标的准确性，最后访问时间是通过OSS底层的访问日志获取的。因为这样的话只要有人不管通过任何引擎任何途径读这里面的数据，访问时间都会获取到。最后对于没有人使用的表分区，就可以考虑去做归档或者删除了。

那结合这几个指标吧，更有利于我们可以更好的做库表分区的生命周期管理的，湖上生命周期管理也是一个重点。因为数仓是有存储分层的概念的，但在数据湖上其实是没有一个比较完整的管理能力。那我们其实目前就在做这方面的事情。



那这个
那首先OSS对象存储是提供了这个存储分层能力的
就是正常我们使用的是标准型嘛
然后也可以按需设置成这种
低频归档啊
冷归档啊这些这些这些层次
然后呃设置好了归档之后呢
其实会对这个数据房方式有影响
但是呢它的存储成本啊会大幅降低
啊那
用户首先我们可以设置一些规则
那在我们这边可以设置一些规则
包括你基于分区的值啊
包括分区的创建时间啊
包括刚才提到的这个访问频次啊
等等这些指标啊
设置一些预值
就是说我啊
多长时间没人访问或者说30天内访问频次很低小于几次
那其实就是可以作为一个规则，设定一个预值
然后我们后台呢会定期的把这些
符合条件的这个分区呢
整个目录去做归档
或者说或者说置为低频等等
然后这样的话
就是
现在这个表和分区的生命周期管理
啊对于这个
啊对于这个呃
存储成本优化
其实是一个比较好的一个事情
然后另外的话如果说归档和冷归档他
做了之后是不能直接访问的
他是需要做解冻的吗
然后需要一个流程
那如果说用户需要有一天需要访问
这些已经归档的数据呢
也可以在我们这边一键解冻就是说不用像OSS在那边哎呀逐个文件去做操作了
然后整个目录后面就可以直接使用了
那这个这种存储优化
这种存储生命周期管理其实是一个呃
对于存储量比较高的数据湖用户
是一个会是一个比较好的实践
啊最后最后我介绍一下这个
这个在湖格式层面
我们啊做的一些事情吧
对呃胡格式我们提到的就是还是
呃我们
hudi、iceberg这几个湖格式它有几个特点
就是说它为了实现这个ACID
呃他底层存储文件
其实在
他的底层数据文件在更新的时候呢
大部分copy on write的
然后新版本呃他的数据就会读新文件
然后老版本的数据还保留在存储侧
然后就给大家几个问题嘛
就一个问题就是他时间一长
历史版本数据文件是要做清理
然后另一方面他
频繁流式的写入其实会产生很多小文件
那通常来说
大家是可以做一种手动命令做清理的
或者说他可以
结合在我们的streamming任务当中啊
去
配一些参数
比如说多少commit他清理一次
但是呢他对流式
写入的本身的性能会有些影响
然后这种情况
其实我们业内很多公司
都额外部署了这种table service的这种方式啊
就是说用啊不影响流失写入
然后另外起一个批作业去做这种
呃清理和优化
然后呢我们其实呃DLF就是相当于把
这种table service做在了云服务里面
这样的话我使用我们这个DLF的这个
湖格式的这个用户呢
他是可以直接在控制台上配置规则的，比如说基于啊版本号更新多少次就做一次清理这样的这样的事情。这样的话，我们这个第二后台就会跑任务
去做这种呃
vacuum或者optimize这种命令
然后整个过程也是全全托管的
然后用户也不用关心他背后使用的资源
那原理我们也介绍一下，
其实在我们的刚才提到
这个元仓其实是会维护啊
很多这种元数据的变化和引擎消息的
那我们也会感知到就是哪些湖格式表
发生了写入和变化
那每一次表着写入呢
我们就会触发我们规则引擎去啊
去做一个判断是否满足条件
那如果满足条件就会触发我们这个
呃这个动作的执行
然后目前我们其实是delta lake
我们呃已经比较完整的支持了
然后hudi我们还在进行当中
对我们这块也是一个相当于在
呃比较新的这么一个模块

然后具体再介绍一下我们这个湖格式管理的具体的几种策略吧
那首先第一种是最常见的
就是基于版本间隔，清理清理历史文件或者合并小文件
啊就比如说我写入了多少个commit的
我写入了20个commit之后就会自动触发这个整个表的清理
或者是小文件合并
然后这个预值呢
也是可以随这个用户级别
或者作业级别做做配置
的啊然后我们内部会把这些合并的这些任务啊放在一个队列里
就比如说前一个合并任务还没有跑完，我们是不会跑下一个合并的
就是说避免这种并发执行
写冲突这种现象发生
对
然后另外还有一种合并规则呢
我们在这种客户实践
过程当中觉得比较实用的
我们就要基于时间分区
自动合并上一个分区的小文件
这种在流式啊
因为我们在流式写入的场景下
通常我们会按这种
时间顺序去命名分区值吗
然后每写入一个新分区就会代表
其实就代表上一个
分区写入停止了
然后这个时候
一旦我们发现有新分区的创建
我们其实是可以去对上一个分区
做一些啊优化和合并的动作的
这样的话上一个分区他后续的查询的性能就能得到保证
然后这种做法也是能最大程度避免合并任务和写入的这个流任务的写冲突的。
啊
当然我们呃为了实现这个这个方案
我们也是呃
内部自动处理了很多分区的
呃分区值的这种时间格式
这样的话
我们就可以自动识别这些时间分区
就哪一个分区是最新的
然后哪一个分区是它上一个分区啊
像这种策略
我们是内部做了一个
呃时间格式的支持的
对然后
那这两个策略其实比较典型的
然后还有一些其他策略我就不展开了
OK
啊好就是我这边分享的内容就是
主要是这些
然后主要还是介绍我们这个阿里云上
数据湖的产品架构
还有我们这个
元数据和存储优化相关的事情





Q&A

Q1 DLF元数据的管理，跟databricks推的unity catalog有什么区别

DLF元数据管理有点类似于hive metastore的这么一个
升级呃databricks推的unity catalog呢
它其实是跟呃它的那个引擎
它的那个databricks的spark的绑定比较多，它是基于databricks的引擎去做很多事情。
我们对单个引擎的集成没有unity catalog那么的完整，但是更focus在云上的统一元数据，也就是说同一份元数据可以被云上各种各样的引擎，包括自研的、开源的引擎，然后统一的进行读写。
总结：我们对云上统一的数据这个角度做的比较多，针对多引擎的打通，对针对某一个引擎内部去做集成的没有那么深入。



Q2: DLF的OPEN API是开源的吗？
首先我们是一个这个全托管的云产品，内部的实现是是做成云服务。
然后我们会提供标准的api，用户可以通过这种阿里云sdk对api的调用和使用。
然后我们的元数据client，适配hive client。同时client本身也是开源的，内部的元数据服务是在云上实现的。



Q3: DLF针对小文件治理，计算资源等控制

这点是跟湖格式相关的小文件合并的问题。
就是说目前因为我们还是这个产品
还是一个
就是在这个湖格式这个小文件
这个产品还是在
呃一个公测阶段嘛
就是说还没有进行真正的计费
但是我们底层的资源是我们
呃内部内部提供的
就是说不是使用用户的资源的啊
然后我们内部是会做一些单租户的
最大的CEU的一个限制的啊
但是目前在计费策略上还没有明确的推出。这个我们后续可能足够完善之后会去做这个事情



Q4： 现在的hive  hook解析HSQ的SQL，
matestore的listen的监听
这些监听DTR这些吗
就现在我们
实现的还有深刻还有户口能够监听
matestore里面的listener里面的DDL
可以监听到嗯
哦我我我我看一下
我理解一下这个问题啊就是
啊解析SQL
就是说这用户呃是这样的
就是说首先我们DLF
呃元数据本身啊
因为刚才提到了我们也有元仓吗
其实是我们内部是
呃会监听到这个所有的元数据变更的
然后包括我们也会基于这个引擎的hook去
监听到我们表示查询的这些信息的
然后会维护到我们DLF这个元仓里面
然后用户可以在我们这个DLF
的这个data Profile的a p i进行获取
然后因为我们这个实现是没有metastore的
呃如果说呃你想自己
你想自己实现一个这个metastore的listener，
像以前hive metastore一样啊
这个我们是不支持的
但是
你可以基于我们这个云上的这个API去获取到这个元信息。



