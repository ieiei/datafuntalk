大家好我是范佚轮，来自于阿里云。
目前是负责这个，阿里云数据湖构建这款产品的研发
今天给大家分享的主题是数据湖统一元数据与存储管理

主要是介绍一下我们阿里云的数据湖架构，以及我们在元数据的存储分层相关的优化和实践。

首先先谈一下我们数据湖相关的概念和架构
大家还是回到我们数据湖的概念上，就是什么是数据湖。
我们可以看到不同的云产商其实也有不同的定义。
但是基本上从关键词上来看，还是围绕这几个特性和目标：

一个是统一存储，就是说数据湖是一个统一的中心化的数据存储。
然后另外的话它是可以用来放一些原始数据的。
然后它是支持多种格式的，包括结构化的数据和非结构化数据。

首先第一个统一存储，其实是为了解决这个数据孤岛的问题
因为传统的数据库或者是数据仓库在设计上是存算一体的。
也就是说不同的查询引擎之间，数据是需要经过清洗和同步的。
那不管是在存储空间上，还是效率上，其实都有一定的浪费。
那数据湖场景呢，它是怎么解决这个问题的呢？
它其实是用存算分离的查询引擎的，典型的比如hadoop生态的hive和spark。
然后加上开放的存储格式parquet、orc这些，然后让不同的引擎同时可以查询同一个数据。
这样就是我们早期的数据湖的一个架构。
然后另一方面在存储实现上，其实数据湖通常会使用这种扩展性比较高的，廉价的存储。比如HDFS，或者云上我们会用OSS或者S3这种对象存储。
这样的话呢大家可以就把这种更多的原始数据，非结构话数据可以直接放入，避免原始数据的丢失。
那为了能够读取这些原始数据，计算引擎通常也是支持这种schema on read的方式，也就是采取事后建模的高灵活性的解析方式。



那对数据格式就没有很强的约束，因为这种灵活性，其实也带来一些弊端。
就是数据湖他缺少了一些功能，比如说他的高度开放性，他对安全和权限的管理照比数仓是有所差距的。
另外就是包括他的事务性，因为他开放的存储，并发写入的场景尤其是流式写入的场景，事务对ACID的要求会更高。这也是我们这些数据湖格式诞生的一个重要原因。
那有没有一种办法使我们能够利用上数据湖这些优势，也能让数据湖拥有数仓的这些功能特性呢？
因为前两年databrick提出的这个Lakehouse湖仓一体，其实就是要让这个数据湖能够去实现更多数仓的企业级能力。就是说要像使用数仓一样使用这个数据湖。这个lakehouse概念是在数据库的基础之上的，其实就是添加了几层内容。首先在存储上层，他其实是做了一个元数据的统一，就是matedata统一。
对上层提供统一的元数据结构化SQL的这么一个接口，然后让不同的应用，可以用相同的元数据访问数据。
另外的话，为了在性能上，我们也支持了一些cache，来解决数据湖读取性能。
另外的话，很重要的，还是数据湖格式这个事务层。
因为我们
呃目前很很火热的这个数据湖格式，delta lake然后hudi还有iceberg
呃我们现在提到数据湖很多场景下
甚至跟这个数据湖格式已经划成等号了。
当然他是有些夸张了，但足以证明他在这个数据湖架构中的一个重要地位。





然后底层在数据湖存储的实现上，相比于HDFS，目前在云上也有一个使用对象存储作为数据湖存储的这么一个趋势。因为云上对象存储它的扩展性相比于你自建HDFS要高很多。然后不管是在成本上，还是在可用性上其实都是会高一些。所以说我们提到数据湖存储目前很多也是在解决云上对象存储的问题。







OK然后前面两页我们提的是数据湖的概念和它的一些架构
与之对应的我们阿里云云上也是做了一些类似的产品和功能帮助用户去使用这个数据湖架构
首先我们云上的这些大数据引擎都是可以进行数据湖分析的
就是常见的可以用EMR的spark和hive，Presto啊StarRocks这些引擎
然后也可以用阿里云自研的引擎，比如maxcompute hologres都是可以进行湖上数据分析的
也就是说我们指的是OSS上的这个数据分析的
我们可以根据合适的场景来选是选择这些引擎
另一方面这些引擎为了能够无缝的对接湖上的结构化数据呢
DLF提供了统一的元数据和湖上的权限管理，作为整个lakehouse架构里的元数据管理层
这一块我会在接下来重点介绍一下我们这个data lake formation就是DLF这个产品
然后在存储层这一方面，因为云上的对象存储OSS是天生适合做数据湖存储的
然后前面也提到了它的成本也不高啊
另外的话，现在OSS也支持了兼容HDFS接口的产品，它叫OSS-HDFS就是完全支持HDFS接口的，那更适合对接一些老版本的大数据引擎之类的







对这个是我们阿里云在云上的一个数据湖架构的一个实践
我们在构造这个数据湖架构的时候可能会遇到一些问题和挑战
尤其是在这个元数据管理和湖上存储管理这几个方面嘛
然后阿里云上的DLF产品
其实就是为了专门解决这个问题的
啊我这里先简单概述一下，后面还会详细介绍。
我们的核心能力是做了一个全托管的统一元数据服务

为什么说这个元数据服务在数据湖里比较重要呢？因为咱们把数据已经放在数据湖上了，那这个元数据其实也需要一个中心化的管理才能实现多个引擎的无缝对接。
这样的话不同引擎它读写同一份数据是围绕统一的schema去做操作的，而不是每个引擎都要建外表之类的。那围绕这个元数据，我们提供对数据的细粒度的权限管控。
另外数据湖上的一些存储管理的功能我们也都提供了，下文会介绍到。






首先先介绍一下我们第一个重要能力就是数据湖上的这个统一元数据
在开源大数据体系里，我们知道从早期的map reduce到类似SQL查询语言的hive诞生出来之后。
其实hive逐渐就成为了开源数仓的一个事实标准
那围绕着hive的元数据hive metastore 也成为了对接开源数仓的这个元数据的一个标准
那从此以后其实各个引擎啊包括spark、presto等等其实都是支持对接hive metastore
围绕着hive metastore做这个元数据的
所以目前来讲，大家自建元数据，也基本上是用这么一套实现。
hive metastore其实是一个常驻的无状态的服务，它可以部署多个实例，可以部署一个也可以部署多个。
大数据引擎通过thrift协议连接hive metastore做元数据的读写的。
然后hive metastore它的元数据本身是需要挂一个数据库，通常大家会用这个mysql作为这个hive metastore的元数据底层存储。   
这样就形成了通常来讲的开源大数据常见的元数据体系
不过我们使用这个hive metastore去管理元数据也存在一些问题和挑战了。
首先在功能层面上他是没有做多版本的，他不能追溯之前的元数据版本
然后包括一些acid的特性和LOC这些接口。
另外他是和hive引擎绑定的，在湖上多引擎的场景下，是没有办法利用到它的一些功能的。
另外它暴露的接口是thrfit的协议，如果你自己有一些服务要对接，或者自研了一个引擎要去对接会相对麻烦一些。
然后有的时候你可能需要直接连mysql去读一些元数据，但这也是一个比较好的一个方法。
最后的问题就是他存在性能瓶颈，存在单点问题和运维成本，尤其是在元数据量比较大的客户，这是一个比较常见的问题，
因为单点的hive metastore，还有metastore后端连接的mysql接口都可能会成为瓶颈，需要一些性能调优的工作。
具体来讲，我这里列了几个日常碰见的一些真实的客户问题。
在hive metastore使用的过程当中，首先第一个是jdbc连接的问题，因为hive metastore需要连接后端的数据库，通过jdbc连的时候可能会遇到一些错误，比如说有的时候我们查询元数据的所有请求都突然变慢了。
这个时候我们第一时间想到要检查一下你mysql的状态，看一下mysql监控是不是有慢SQL，
因为如果分区数总量很大的话，mysql表他可能上千万，就是会查询的比较慢，这个时候，需要做一些数据清理，删一些分区啊来缓解这个问题。
另外的话mysql连接数也可能会带来一些连接上的异常，因为很多外部系统是需要查元数据的。
然后就是自建的一些数据管理系统之类的通常不会用thrift协议去调hive的metastore server，都会直连JDBC，然后连接数多的话，也可能会带来一些额外的压力。
另外的就是内存方面，这个hive metastore server的内存也存在oom的风险。
因为有些操作，比如list partition，它会加载全部分区对象。
如果说有人写了一个糟糕的查询，比如在一个很大的分区表上，他没有加分区查询条件，就可能会拿到上百万的分区。
然后导致整个hive metastore内存full gc或者oom这种情况，一旦hive metastore出问题，整个集群的作业都会受到影响。
然后后面列举几个呢
就是这stackoverflow也是我们见到过的
如果你drop partition的分区数量很多的话，在hive metastore的内部实现是递归的，可能会堆栈溢出报错，直接执行不了。
然后嗯包括可能会超时，因为HMS的客户端是全量返回的，没有一个分页的设计，所以在拉取元数据的时候，可能会出现超时的情况。这也是一个风险点。这些就是我们在使用HMS时候遇到的一些问题。

那在云上，我们的DLF就是data lake formation上也提供了这种全托管的元数据服务，我们采用的是完全不同的架构，这种架构可以解决上面大部分问题和痛点。
首先来看我们作为一个云产品，是通过标准的open api去暴露接口，api提供了兼容hive 2和 hive 3
matestore接口的client。这个client呢它是可以直接替换掉引擎的hive metastore client实现类的
这样我们原本访问hive元数据的地方可以直接替换为我们客户端的实现类，实现了无缝对接。
另外除了开源体系的这种引擎以外，我们也对接了阿里云上的其他大数据引擎，包括这个max compute、hologres、Flink等等。云这上其他大数据引擎也可以利用我们这个统一元数据来进行这个元数据管理。这样的话其实真正做到统一的catalog，就是用一个引擎写入，比如用flink 入湖，然后入完的可以直接用SPARK查，查完之后也可以用Hologres等等做OLAP分析，可以直接采用同一个元数据来进行分析。
我们元数据服务底层实现是用阿里云表格存储，不是像HMS用的mysql，mysql扩展性还是差一些的。底层用的这种表格存储也是阿里云的一种服务，面向海量数据它有非常强的伸缩能力，扩展性很高。所以不用担心分区数过大会带来的一些扩展性问题。
因为我们整个也是一个全托管的服务，对使用方来说可以SLA，保证高可用的问题，前面提到的运维问题也是可以避免的。

总结一下，我们这个统一元数据一方面因为是全托管，会减少很多元数据的运维成本，另一方面，是真正对接云上多引擎的这么一个元数据。



具体再补充一些关于元数据实现本身的细节，
首先我们元数据的客户端是兼容hive metastore 的行为。我们实现了hive metastore的一个接口
可以直接去对接hive生态相关的大数据引擎。然后hive metastore它本身内部的有些行为，比如说在创建partition的时候，可能会统计table size等等这些动作，我们都会保留在这个客户端里，所以不用担心有些接受性问题。
另外的话，客户端会做一些性能优化，包括异常重试，
然后硬化读取啊之类的请求提议？？？？？ 我们也会做一些性能的压缩
在服务内部，除了刚才提到的存储层的高扩展性以外，我们也通过一些自动的分区索引，再做一些分区过滤的性能提升。
总体来讲在元数据的性能上，
我们在一些小表上可能跟RDS有些差距，但是不太明显。
在大分区表上，比如单表有300万分区的场景下，我们的查询的性能会有比较明显的优势。
比如说300万分区表，在分区全部命中的条件下，这个list partition by filter在我们的元数据可以在0.5秒内返回，如果说RDS的话因为他的分区值可能没有索引，是需要花5秒左右时间才能返回的。
在元数据的功能上我再举几个例子，
一个是元数据多版本，我们会记住元数据的所有的每一次更新的前后状态，可以看到什么时间点你加了什么字段，是谁改的。
我们有一个比较好的回溯机制，包括元数据审计（这个Hive也有），包括元数据检索
我们的元数据本身会把内容同步到ES搜索引擎里，对外暴露。你可以通过字段搜表，也可以全局搜索


OK
谈到元数据实现相关的话，我们再看一下权限相关的一些问题。

如果我们要在开源大数据场景，做到用户级别的权限控制，通常有这么几种方案。
hive它其实本身是提供了认证的能力的，它有storage-based authorization还有sql-standard-based authorization。但是hive的实现其实都是跟hive引擎绑定的。我们通常用其他引擎是无法使用到它的功能的，基本上也没有人真正在用吧。
然后呃大家常见的其实是这个
用这个Ranger去做权限管理  那ranger其实它是一个呃
多引擎通用的一个方案
那它可以对sql进行这个权限管理
也可以对文件系统做权限管理
然后呢它原理呢就是说它
他会从ldap同步用户信息嘛
然后他提供UI用户可以配置权限
然后大数据引擎这一侧呢
他是可以加各种插件的
然后他通过这个插件来去实现这个权限的拦截和检查啊
那ranger其实目前也是一个可行的方案
然后只不过一方面呢他虽然说可以对接开源引擎
但是可能在比如说在公有云上我们一些其他的大数据引擎
呃
我们自研的大数据引擎
是没法知识对接的
然后另一方面其实
呃
它包括Sparks sql的这些插件啊
他官方的支持并不好
然后更多的大家还是要自研一些插件
或者说找一些找一些第三方插件嘛
就是说部署起来没有那么简单
那关于权限这一块呢
我们这个统一元数据其实也提供了
这个DLF统一元数据也提供了这个鉴权的能力
那这样啊我们的呃权限其实是呃
默认是没有开启的啊
因为不一定所有用户都需要
但是用户可以是按catalog级别进行
这个全开关
就是catalog
是基于database之上的那么一层啊管理管理模型
然后如果我们基于catalog
设置了这个权限之后
呃
管理员就可以在我们的控制台进行
这种呃
具体的授权了
包括这个带database、table、column、function这些粒度都可以进行授权
然后可以设置不同action的权限
你可以
我只给一个人设某个table的select权限
但是不给他insert的权限
就是这种是可以可以进行设置的
然后我们也支持这个RBAC啊
就是说我们可以把权限包在roll里
然后统一付权给一堆呃一堆用户
像这些基本的能力都是
具备的呃
然后在鉴权环节呢
其实在实现上
我们是提供了两个层面的鉴权
就是呃第一第一层面是元数据的API
就是说我想要看这个table
或者说我要create table
那这种动作呢我们会在服务端鉴权
因为我们是这个啊
云服务吗
我们云服
我们服务会啊
直接会去鉴权
你这个请求发来的
这个用户角色
是不是有这个相应动作的权限
然后如果没有的话就会进行拦截
然后另外的话呢
因为有些SQL操作
在元数据层面层感知不到
就比如说元数据可能就是查一张表
但是我其实并不知道
你是在往里写数据
还是在读数据
那这个时候其实和ranger类似
我们也提供了引擎的插件
就是说我们可以
把放在这个呃spark、hive上做一些
做做一些拦截器
然后呢和ranger类似
然后呃他会去呃
在内部去检查这个用户啊
这个代理用户到底有没有啊
select权限
然后没有的话去做一个这个呃拦截
对所以说这个我们是
呃两层的鉴权模型啊
适用于不同的场景
那然后
介绍一下就是额外的功能
就是元数据本身
其实呃在云上也好
或者说是
自建的这个mysql的元数据也好
呃如果说想要迁移的话
其实是需要一个迁移的过程的
那为了方便这个过程
我们产品上其实是做了一些
这个元数据迁移
功能比如说控制台上
可以做这么一个元数据迁移啊
简单来讲就是说我们会去连远端
的一个mysql数据库啊
如果这个数据库在阿里云上VPC内
我们就会自动做网络打通
然后呢就是说通过
JDBC直接把这个元数据可以啊拉取
然后转换成我们
我们这个云上的这个DLF原数据
这个是我们直接产品化掉的
然后另外的话呢
呃除了导入的话
可能还有些导出的需求
然后包括两边元数据对比的这个需求
然后这些我们也都有这个
现成的工具
然后可以直接使用
也就是说在元数据迁移方面
就是说不管是导入导出还是其他的
我们其实都有都有
都保持了我们的开放性啊
就是说是不需要担心这个
元数据被绑定的这么一些问题
OK然后嗯
除了元数据呃迁移这种模式啊
可能还啊有些场景下
还需要做这种元数据抽取
快速构造出这个湖上的元数据
那元数据抽取
它适合于一个什么样的场景呢
就比如说我们数据湖上
已经有一些数据文件了
这个数据文件可能是从其他数仓
同步过来的呀
呃拷贝过来的
或者说是一些零散
的CSV数据集文件啊
等等
那这个时候我们没有这个
呃这个表的元数据
就要用ddl语句自己去建表
然后做查询
是比较麻烦的
然后也容易出错
尤其是对于这种
像json这种半结构化的嵌套类型的
更是很难去写这个建表语句啊
这种情况下
就是用我们这个元数据抽取功能就
比较方便
可以直接把元数据给
他呃推断出来
然后用户只需要填一个这个OSS路径
那填一个OSS路径
然后我们会根据他的路径格式
自动扫描下面的表
然后包括他那个分区的
呃分区分区值
那创建好了之后呢
我们就会写入到这个元数据里
进进行直接查询了
对啊包括我们这种各种格式啊
CSV啊json啊parquet 、orc
包括湖格式啊
啊其实都是可以可以识别出来的
然后呃另一方面呢
因为我们做格式推断
是需要扫描所有数据的吗
那所有数据扫描一遍
呃会比较耗时
我们也只是一种快速采样的这种方式
啊这个这些我们就不详细展开了

刚才提到元数据相关的一些内容
然后接下来这块
呃介绍一下我们在呃数据湖存储方面
呃存储分流上方面做一些管理和优化
呃那首先我们先介绍一下这个元仓
呃就是说我们除了这个元数据
呃服务本身之外呢
也做了一个元数据仓库啊
那那什么是元仓呢
元仓其实就相当于我们
在线服务的这个
元数据存储
之外呢我们
做了一个在线服务
在线元数据的这个数据
元数据的数据仓库
那因为我们这个在线的元数据存储
它是需要做serving嘛
需要做在线服务的
它需要比较高的这个读写
读写事务保障的
然后有些后台分析啊
啊包括一些聚合查询呢
其实是不适合在这里去做的
于是我们呃做了一个啊
实时的元数据仓库
它底层我们是底层我们是基于这个
max compute和hologres去做实现的
然后它会收集哪些信息呢
它会收集我们元数据的变更信息啊
也会收集我们计算引擎的查询的一些
写入的信息
然后包括我们存储上的信息
就这些信息我们都会都会收集到
都会实时收集到
这样的话
我们在这里就会形成围绕着元数据
就是database的table partition
做的一个指标库
那么我们把这个指标库叫做dataprofile指标
拿到这些指标之后呢
我们啊就是具体有这些指标
等会我可以介绍一下
就是拿到这些指标之后
我们会把这些指标通过标准的API暴露出来啊
一方面我们在控制台上可以做一些统计分析啊
包括我们对接的一些云产品，像dataworks之类的也可以做一些
啊数据展示和预估
然后另一方面我们
呃这些指标可以用来做存储生命周期的一些优化和管理
那接下来我介绍一下这个data profile指标的几个实现

举几个例子啊
首先呃首先提到这个表和分区的大小，这是一个比较基础的一个属性
那通常来讲其实表和分区大小啊
写在元数据
就是Hive元数据的table properly里面
这个是呃呃本身他就定义的了
本身他就定义了计算引擎
他会在创建
呃表或者创建分区的时候呢去写入
但是呢呃首先它们不同引擎它
写入的key标准不一样啊
像hive它是叫total size然后spark它有spark开头的一个属性值
然后另外的话呢
呃这些写入也是需要一些参数去开启的
你不开启他不会写入
所以说实际情况我们会发现
呃元数据上的这个表大小
元数据本身上存储这个表大小
他不一定是准确
啊
所以说我们呃在于我们的元仓里呢
我们是通过OSS底层存储啊
就是因为因为我们默认了这个
大部分这个数据湖是使用OSS的嘛
然后我们会通过OSS的底层存储来获取你这个表分区的大小
这样可以最大限度保证他这个数据的准确性
那因为OSS他提供了一个存储清单，他是t+1更新的
然后这个我们这个有点像LAMBDA架构了
就是我们会t+1的去更新这个
存储清单的这个表和分区的存储大小
然后另外的话呢
对于实时一些表和分区的变更呢
我们有会监听到
然后呢也会实时的
再去从OSS那边拿到最新的大小去做更新啊
也就是说存量加增量的这么一个流程去做这个表分区的这种大小
然后啊拿到这个大小之后呢
啊我们也会每天产出一些分析报表
这里就大概列了一下
就比如说我们表的存储排名啊
大小文件占比啊之类的
就是说我们也可以看哪些表
哪些分区的存储占用比较大可以做一些优化
这是一个比较完整的湖上的一个管理识图
另外再介绍两个关键指标：

第一个是表和分区的访问频次啊
就是
啊表的访问频次
那为什么我们要统计表的访问频次呢
呃这个访问频次其实是可以鉴别那些访问不频繁，但是仍然在使用的这些表。
这些表可以在oss底层制为这种低频存储
低频存储它其实是可以照常读取的
但是它可以节省一些成本
对
呃那访问频次的获取呢
我们原理上
还是用这个引擎的hook来去实现的
就是说我们
会解析这个SQL的plan
然后拿到它读取的表和分区
然后拿到这些信息之后呢
会提交到我们的元数据啊
服务里去做一些记录
然后呢
呃会把这个指标啊统计统计出来
然后另外一个指标就是最后访问时间
表和分区的最后访问时间
这个指标其实也也很有用
就是说因为
他可以用来识别这个表和分区
是否有人在访问
呃就是说为了保证这个指标的准确性
其实我们最后访问时间是通过OSS底层的这个访问日志去去拿的。
这样因为这样的话就是只要有人
就是不管说通过任何引擎任何途径
他在读这里面的数据其实都会都会获取到
那对于没有人使用的啊这些表分区呢
就是可以考虑去做这种归档或者删除了

那结合这几个指标吧
其实是有利于我们可以更好的来做库表分区的生命周期管理的啊
因为湖上这个生命周期管理啊其实也是一
个嗯也是一个我我觉得也是一个重点
就是因为数仓
是有这个存储分层的概念的
但在数据湖上呢
其实是没有一个比较
完整的管理能力的
那我们其实
啊目前就在做这方面的事情
那这个
那首先OSS对象存储是提供了这个存储分层能力的
就是正常我们使用的是标准型嘛
然后也可以按需设置成这种
低频归档啊
冷归档啊这些这些这些层次
然后呃设置好了归档之后呢
其实会对这个数据房方式有影响
但是呢它的存储成本啊会大幅降低
啊那
用户首先我们可以设置一些规则
那在我们这边可以设置一些规则
包括你基于分区的值啊
包括分区的创建时间啊
包括刚才提到的这个访问频次啊
等等这些指标啊
设置一些预值
就是说我啊
多长时间没人访问或者说30天内访问频次很低小于几次
那其实就是可以作为一个规则，设定一个预值
然后我们后台呢会定期的把这些
符合条件的这个分区呢
整个目录去做归档
或者说或者说置为低频等等
然后这样的话
就是
现在这个表和分区的生命周期管理
啊对于这个
啊对于这个呃
存储成本优化
其实是一个比较好的一个事情
然后另外的话如果说归档和冷归档他
做了之后是不能直接访问的
他是需要做解冻的吗
然后需要一个流程
那如果说用户需要有一天需要访问
这些已经归档的数据呢
也可以在我们这边一键解冻就是说不用像OSS在那边哎呀逐个文件去做操作了
然后整个目录后面就可以直接使用了
那这个这种存储优化
这种存储生命周期管理其实是一个呃
对于存储量比较高的数据湖用户
是一个会是一个比较好的实践
啊最后最后我介绍一下这个
这个在湖格式层面
我们啊做的一些事情吧
对呃胡格式我们提到的就是还是
呃我们
hudi、iceberg这几个湖格式它有几个特点
就是说它为了实现这个ACID
呃他底层存储文件
其实在
他的底层数据文件在更新的时候呢
大部分copy on write的
然后新版本呃他的数据就会读新文件
然后老版本的数据还保留在存储侧
然后就给大家几个问题嘛
就一个问题就是他时间一长
历史版本数据文件是要做清理
然后另一方面他
频繁流式的写入其实会产生很多小文件
那通常来说
大家是可以做一种手动命令做清理的
或者说他可以
结合在我们的streamming任务当中啊
去
配一些参数
比如说多少commit他清理一次
但是呢他对流式
写入的本身的性能会有些影响
然后这种情况
其实我们业内很多公司
都额外部署了这种table service的这种方式啊
就是说用啊不影响流失写入
然后另外起一个批作业去做这种
呃清理和优化
然后呢我们其实呃DLF就是相当于把
这种table service做在了云服务里面
这样的话我使用我们这个DLF的这个
湖格式的这个用户呢
他是可以直接在控制台上配置规则的，比如说基于啊版本号更新多少次就做一次清理这样的这样的事情。这样的话，我们这个第二后台就会跑任务
去做这种呃
vacuum或者optimize这种命令
然后整个过程也是全全托管的
然后用户也不用关心他背后使用的资源
那原理我们也介绍一下，
其实在我们的刚才提到
这个元仓其实是会维护啊
很多这种元数据的变化和引擎消息的
那我们也会感知到就是哪些湖格式表
发生了写入和变化
那每一次表着写入呢
我们就会触发我们规则引擎去啊
去做一个判断是否满足条件
那如果满足条件就会触发我们这个
呃这个动作的执行
然后目前我们其实是delta lake
我们呃已经比较完整的支持了
然后hudi我们还在进行当中
对我们这块也是一个相当于在
呃比较新的这么一个模块

然后具体再介绍一下我们这个湖格式管理的具体的几种策略吧
那首先第一种是最常见的
就是基于版本间隔，清理清理历史文件或者合并小文件
啊就比如说我写入了多少个commit的
我写入了20个commit之后就会自动触发这个整个表的清理
或者是小文件合并
然后这个预值呢
也是可以随这个用户级别
或者作业级别做做配置
的啊然后我们内部会把这些合并的这些任务啊放在一个队列里
就比如说前一个合并任务还没有跑完，我们是不会跑下一个合并的
就是说避免这种并发执行
写冲突这种现象发生
对
然后另外还有一种合并规则呢
我们在这种客户实践
过程当中觉得比较实用的
我们就要基于时间分区
自动合并上一个分区的小文件
这种在流式啊
因为我们在流式写入的场景下
通常我们会按这种
时间顺序去命名分区值吗
然后每写入一个新分区就会代表
其实就代表上一个
分区写入停止了
然后这个时候
一旦我们发现有新分区的创建
我们其实是可以去对上一个分区
做一些啊优化和合并的动作的
这样的话上一个分区他后续的查询的性能就能得到保证
然后这种做法也是能最大程度避免合并任务和写入的这个流任务的写冲突的。
啊
当然我们呃为了实现这个这个方案
我们也是呃
内部自动处理了很多分区的
呃分区值的这种时间格式
这样的话
我们就可以自动识别这些时间分区
就哪一个分区是最新的
然后哪一个分区是它上一个分区啊
像这种策略
我们是内部做了一个
呃时间格式的支持的
对然后
那这两个策略其实比较典型的
然后还有一些其他策略我就不展开了
OK
啊好就是我这边分享的内容就是
主要是这些
然后主要还是介绍我们这个阿里云上
数据湖的产品架构
还有我们这个
元数据和存储优化相关的事情





Q&A

Q1 DLF元数据的管理
跟databricks所推的
呃unity的catalog有什么区别
啊呃
我们这个元数据管理
其实呃
有点类似于就是
呃对hive metastore的这么一个
升级呃databricks推的unity catalog呢
它其实是跟呃它的那个引擎
它的那个databricks的spark
它它绑定比较多嘛
它其实是可以基于他的那个
databricks的引擎去做很多事情的啊
我们跟呃单个引擎的集成
其实没有做到像他那么的完整
但是呢我们
更多的是focus
在云上的这个统一元数据
也就是说我同一份元数据可以被云上
呃各种各样的引擎
包括自研的包括开源的
然后统一的进行读写啊
对就是还是有有一定的区别的
我们其实呃在云上统一的数据
这个角度做的比较多
就是说多引擎的打通
对针对某一个引擎内部去做集成的没有那么深入



Q2: 第二个就是DLF的OPEN API是开源的吗
呃就是说首先我们是一个这个全托管的云产品啊
就是说内部的实现是是我们这个
云服务嘛
啊然后我们会提供标准的api
标准api
就是用户是可以通过这种阿里云sdk
去做这个api的调用和使用的
然后我们的元数据client就是说
呃包括适配hive client这个
这个client也是是开源的
对
但是我们内部是就是我们元数据服务
内部实现是我们在云上的



Q3: 接下来一个问题是dLf针对小文件
治理，计算资源等控制
啊OK
就是说呃
这一块就是是提到我们这个最后那个
湖格式相关的一个
小文件合并这个这个问题
就是说目前因为我们还是这个产品
还是一个
就是在这个湖格式这个小文件
这个产品还是在
呃一个公测阶段嘛
就是说还没有进行真正的计费
但是我们底层的资源是我们
呃内部内部提供的
就是说不是使用用户的资源的啊
然后我们内部是会做一些单租户的
最大的CEU的一个限制的啊
但是目前在计费策略上还没有明确的推出。这个我们后续可能足够完善之后会去做这个事情



Q4： 现在的hive  hook解析HSQ的SQL，
matestore的listen的监听
这些监听DTR这些吗
就现在我们
实现的还有深刻还有户口能够监听
matestore里面的listener里面的DDL
可以监听到嗯
哦我我我我看一下
我理解一下这个问题啊就是
啊解析SQL
就是说这用户呃是这样的
就是说首先我们DLF
呃元数据本身啊
因为刚才提到了我们也有元仓吗
其实是我们内部是
呃会监听到这个所有的元数据变更的
然后包括我们也会基于这个引擎的hook去
监听到我们表示查询的这些信息的
然后会维护到我们DLF这个元仓里面
然后用户可以在我们这个DLF
的这个data Profile的a p i进行获取
然后因为我们这个实现是没有metastore的
呃如果说呃你想自己
你想自己实现一个这个metastore的listener，
像以前hive metastore一样啊
这个我们是不支持的
但是
你可以基于我们这个云上的这个API去获取到这个元信息。



